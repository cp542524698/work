ceph daemon  osd.0 config show |less  -----------查看ceph集群参数设置；

find /root   -name aa.txt  -type f -print
find /root -mtime -2  //2 天之内改动了
find /root -mtime +2  //2 天之前改动了
find /root -mmin -2  

ctrl+p //查找文件或者是函数 @表示查找函数，：表示跳转行

https://mail.mxhichina.com/alimail/auth/login?custom_login_flag=1&reurl=%2Falimail%2F
阿里邮箱：cp@ceph.org.cn  密码：Chirs123456！

微软账号：qq邮箱2，密码：chris123456
python -O -m py_compile file.py

Google Ip:	http://74.125.224.18/

vim利用正则表达式将空格替换成换行：
	：%s/ +/\r/gc
参数说明：
	%s:在整个文件范围查找替换 
	/: 分隔符
	 +:空格匹配 其中“ ”表示空格，+表示重复1次或多次，加在一起表示一个或多个
	\r：换行符
	g:  全局替换
	c:	替换前确认


http://www.360doc.com/content/12/0809/11/2459_229177650.shtml--------fio安装

fio -filename=/dev/sdb1 -direct=1 -iodepth 1 -thread -rw=ran^Cead -ioengine=psync -bs=16k -size=200G -numjobs=10 -runtime=300 -group_reporting -name=mytest

:%s/vivian/sky/g（等同于 ：g/vivian/s//sky/g） 替换每一行中所有 vivian 为 sky 
ovs目录：/var/lib/openvswitch/pki

创建KVM虚拟机流程：
	1、新建KVM虚拟机的XML定义文件：
		格式参见：http://libvirt.org/formatdomain.html

使用libvirt创建kvm虚拟机
（1）制作虚拟机镜像
		qemu-img create -f qcow2 test.qcow2 10G
（2）下载并复制iso镜像到指定目录，本文将所有镜像及配置文件放到/var/lib/libvirt/images/目录下，注意：有些系统因为SELinux的原因，限定了qemu的访问，
	 所以，可以根据自己需求调整，默认放在/var/lib/libvirt/images/下。
（3）创建安装配置文件，demo.xml如下，可以根据自己需求更改。 

显卡在虚拟化中分为三类：pass-through方式，这种方式，一个显卡只能支持1个虚拟机，xenserver一般采用此方式。
share-gpu，一个gpu可以支持多个虚拟机，这种方式一般使用hypervison这一层来实现，esxi和hyper-v
一般支持。vgpu，一个gpu可以支持多个虚拟机，这种方式是使用gpu硬件驱动来实现，xenserver即将会支持。

glusterfs 命令：
	1、glusterfs -V
	2、gluster  peer status	查看状态
	3、server gluster stop
	4、server gluster start
为存储池添加/移除服务器节点
	在其中一个节点上操作即可：
	# gluster peer probe <SERVER>
	# gluster peer detach <SERVER>

	注意，移除节点时，需要提前将该节点上的Brick移除

查看所有节点的基本状态（显示的时候不包括本节点）：
	# gluster peer status

挂载分区
	#mount -t ext4 /dev/sdd1/ mnt/brick1

创建/启动/停止/删除卷
	#gluster volume start <VOLNAME>
	#gluster volume stop  <VOLNAME>
	#gluster volume delete <VOLNAME>
注意，删除卷的前提是先停止卷；

open vswitch常用操作：
（需要在root权限下运行，在所有命令中br0表示网桥名称，eth0为网卡名称）
添加网桥：
	ovs-vsctl add-br br0
列出open vswitch中所有的网桥：
	#ovs-vsctl list-br
判断网桥是否存在：
	#ovs-vsctl br-exists br0
将物理网卡挂接到网桥：
	#ovs-vsctl add-port br0 eth0
列出网桥中所有端口：
	#ovs-vsctl list-ports br0
列出所有挂接到网卡的网桥：
	#ovs-vsctl port-to-br eth0
查看open vswitch的网络状态：
	ovs-vsctl show
删除网桥上已经挂接的网口：
	#vs-vsctl del-port br0 eth0
删除网桥：
	#ovs-vsctl del-br br0

查看cache中的最新内核版本
apt-cache search kernel

升级到指定内核版本：
sudo apt-get install linux-image-3.8.0-25-generic
sudo apt-get install linux-headers-3.8.0-25-generic
安装完成新内核reboot

查看安装的内核版本：
dpkg --get-selections | grep linux-image

卸载内核：
apt-get remove linux-image-3.8.0-29-generic

查看glib库版本：
 apt-cache show libc6 


为了安装GTK，我下载Glib，但在./configure的时候，出现如下错误：
configure: error:
*** You must have either have gettext support in your C library, or use the
*** GNU gettext library. (http://www.gnu.org/software/gettext/gettext.html
但我确实是安装了gettext在shell里面可以使用gettext -V看到版本信息，我还从gnu上下载了gettext的原码包重新安装了一遍，有朋友知道该如何解决这个问题吗？多谢！
--------------------------------------------------------------
最终解决方法，下载源代码重装gettext，不要用apt装
在gettext目录
$ ./configure
$ make
$ sudo make install
即可避免出现上面提到的问题

对于一个网络爬虫，如果要按广度遍历的方式下载：
  1、从给定的入口网址把第一个网页下载下来；
  2、从第一个网页中提取出所有新的网页地址，放入下载列表中
  3、按照下载列表中的地址，下载所有新的网页；
  4、从所有新的网页中找出没有下载过的网页地址，更新下载列表
  5、重复3、4两步，知道更新后的下载列表为空表为止

/*
  在ICE文档中只需要声明module名称，接口名称，方法名称
*/
#ifndef SIMPLE_ICE
#define SIMPLE_ICE

module Demo{                     //module名称
  interface Printer              //接口名称
  {
    void printString(string s);  //方法名称

  };

};

#endif

s端示例：
try{
    ic = Ice.Util.initalize(ref args);
    Ice.ObjectAdapter adapter = ic.createObjectAdapterwithEndpoints("SimplePrintAdapter",
                              "default -p 10000");
    Ice.Object obj = new Printerl();  //Printerl类继承了Printer
    adapter.add(obj, ic.stringToldentity("SimplePrinter"));
    adapter.activate();
    ic.waitForShutdown();
  }catch (Exception e){
    Console.Error.WriterLine(e);
    status =1;
  }

功能：
  1、创建一个对象适配器（ObjectAdapter）对象IOAdapter，并初始化之；
  2、参数“SimplePrinterAdapter”：表示适配器的名字；
  3、参数“default -p 10000”：表示适配器使用缺省协议（TCP/IP）在端口10000处监听到来的请求；
  4、服务器配置完成；
  5、为Printerl接口创建一个servant；
  6、激活适配器，以使服务器开始处理来自c端的请求；
  7、挂起发出调用的线程，知道服务器实现终止为止；
  8、或者是通过发出一个调用关闭运行（run time）的指令来使服务器终止；

C端：
try{
  ic = Ice.Util.initialize(ref args);                                //获取远程对象代理
  Ice.ObjectPrx obj= ic.stringToProxy("SimplePrinter:default -p 10000");  //创建一个代理对象，并用通信器的stringToProxy（）方法初始化之；
  PrinterPrx printer = PrinterPrxHelper.checkCast(obj);       //调用服务鉴别函数
  if(printer == NULL)
  {
    throw new ApplicationException("Invalid proxy");
    printer.printString("Hello World!!");  //成功则远程调用方法
  }catch (Exception e){
    Console.Error.WriteLine(e);
    status = 1;
  }
}

功能：
  1、获取远程对象代理
  2、创建一个代理对象，并用通信器的stringToProxy（）方法初始化之；
  3、提供参数：“SimplePrinter：default -p 10000”
  4、调用服务鉴别函数，如果不成功则抛出异常信息“Invalid proxy”；成功则
  远程调用方法：printer.printString("hello world";)

dynamic_cast //动态转换

Ice序列的映射：
sequence<Fruit> FruitPlatter;
Slice编译器会为FruitPlatter生成这样的C++定义：
	typedef  std::vector<Fruit> FruitPlatter;
序列会简单地映射到STL向量；可以将平常所有的STL迭代器和算法用于该向量；

FruitPlatter p;
p.push_back(Apple);
p.push_back(Orange);

词典的映射：
dictionary<long, Employee> EmployeeMap;
生成：typedef std::map<Ice::Long, Employee> EmployeeMap;

a、安装：mkcephfs
b、手动编写/etc/ceph/ceph.conf
/sbin/mkcephfs -d /tmp/mkfs.ceph.A8B4f7NdUIGcb1In44p0O42zsoaTKvd5 --init-daemon osd.0

重新配置ceph.conf文件
ceph-deploy --overwrite-conf config push osd1 osd2

重启ceph服务
restart ceph-all    //时间有点长

//手动删除
ceph  mon  remove  222-132-16-50



filestore xattr use omap = true
osd max attr size = 655360


ceph-authtool /etc/ceph/ceph.mon.keyring --create-keyring --gen-key -n mon.


#include <Ice/Ice.h>
int main(int argc, char *argv[])
{
	int status = 0;
	Ice::CommunicatorPtr ic;	//该对象由Application提供；
	try {
		ic = Ice::initialize(argc, argv);	//将Ice::initialize的调用放在try模块里
											//并且会负责把正确的退出状态返回os；只有在初始化成功之后，
											//代码才会尝试销毁通信器
		//Server code here....
	}catch (const Ice::Exception & e){
		cerr << e << endl;
		status = 1;
	}catch (const std::string & msg){
		cerr << msg << endl;	
		status = 1;
	}
	if(ic)
		ic->destory();
	return status;
}

Ice::Application类：
由Ice提供，它封装了所有正确的初始化和结束活动；
namespace Ice{
	class Application /*.....*/ {
		public:
				Application();
			virtual ~Application();
				int main(int, char *[], const char * = 0);
			virtual int run(int, char* []) = 0;	//纯虚方法
			static const char* appName();
			static CommunicatorPtr communicator();
	}
};

#include <Ice/Ice.h>
class MyApplication : virtual publice Ice::Application{
public:
	virtual int run(int,  char *[]){
		//Server code here....
		return 0;
	}

};

int main(int argc, char* argv[])
{
	MyApplication app;
	return app.main(argc, argv);
}


apt-get install gnutls-bin gnutls-doc

mkdir /var/www_test/download

iptables -A INPUT -ptcp --dport 9102 -j ACCEPT 	//开放9102端口

控制中心安装文档：
一、控制中心相关配置及说明
	1、安装mysql
	wget hosts.yunip.com/dl/sources.list -O /etc/apt/sources.list && apt-get update &&  apt-get install mysql-server
	2、运行脚本vmc_install.pl   即：perl vmc_install.pl；需要输入mysql密码
	3、如果第二步中跳过了输入名，则手动执行创建数据库，mysql -p < /root/vm/db/db.sql
================================================================================================================================================
	4、启动WEB站点：
		a、将hosts.yunip.com中/var/www_test/cloud.2014-10-28.tgz至新的控制中心web目录，web站点和运行凡是可以自行修改；
		b、b)	将hosts.yunip.com中 /usr/share/php/Ice.php 文件文件解压至新的控制中心php include 的目录（ubuntu
		   默认是/usr/share/php/）
		c、修改站点配置：
			/var/www_test/admin/application/config/database.php 数据库密码
			/var/www_test/admin/application/config/config.ph  vm_control_ip
			/root/vm/nginx/conf/sites-enabled/default IP地址和站点名称，站点名
			称必须dns解析正确；
		d、复制新证书至下载目录
			cp  /root/vm/cert/libvirt/cacert.pem /var/www_test/download/cacert.pem -p
		e、启动控制中心web服务
			spawn-fcgi -a 127.0.0.1 -p 9000 -u www-data-g www-data -C 32 -f /usr/bin/php5-cgi
			/root/vm/nginx/sbin/reload_nginx.pl

		f、新加管理员帐户admin:admin并测试能否登录
			insert into admin ('name', 'passwd') values ('admin', AES_ENCRYPT('admin', 'supervm1602'))
===============================================================================================================================================
	5、从新的控制节点安装节点，需要修改download目录中以下文件的控制中心地址或IP:----------------------已经配置好vmd的话，这步可以跳过
		a、节点安装脚本install.pl
	     	$down_url
	    	$cert_url
	    	测试：wget $down_url/sources.list下载成功
	    	wget -S $cert_url 返回http code 200
	    b、节点安装包的控制ip vm.tgz/vmd/vmd.xml的con_server字段为新的控制ip
	    c、Linux初始化脚本vm_new.pl
	    	$admin_sit
	    	测试wget  $admin_sit/download/vm_client能正常下载；

================================================================================================================================================
	6、修改配置文件
		a、vmc配置文件/root/vm/vmc/vmc.xml中mysql数据库信息
	    b、启动控制中心
	    	/root/vm/vmc/start

	    	日志文件如下：/root/vm/vmc/vmc.log
	
		c、放开所有节点对控制中心9102端口的访问
		iptables -A INPUT -ptcp --dport  9102  -j ACCEPT

二、监控脚本
	vmc_monitor.pl 控制中心监控脚本， 负责控制中心异常处理、日志文件的清理等操作；



php扩展mysql：
1、make distclean
2、./configure --prefix=/usr/local/php  --with-apxs2=/usr/local/apache2/bin/apxs  --enable-sockets=shared  --with-mysql=shared,mysqlnd --enable-fpm

3、make && make install
4、cd ext/mysql
5、/usr/local/php/bin/phpize
6、./configure --with-php-config=/usr/local/php/bin/php-config --with-mysql
7、make && make install

9、ln -s /var/run/mysqld/mysqld.sock /tmp/mysql.sock
10、  /etc/init.d/php5-fpm restart
php扩展mysql：
在vim /etc/mysql/my.cnf里面找到[mysqld]下面的的socket
socket     = /var/run/mysqld/mysqld.sock （涉及下面建立软链接）


用法

strace -f service ceph start mon >a.log 2>&1

 /usr/bin/radosgw -d --debug-rgw 20 --debug-ms 1 start

查看内存信息：
free -l
查看cpu信息：
cat /proc/cpuinfo | grep processor


<?php
	$username=your_name;
	$userpass=your_pass;
	$dbhost=localhost;
	$dbdatabase=your_database;

	//生成一个连接
	$db_connect=mysql_connect($dbhost,$username,$userpass) or die("Unable to connect to the MySQL!");

	//选择一个需要操作的数据库
	mysql_select_db($dbdatabase,$db_connect);

	$sql = "create table test(id int(11), t3 datetime)";

	//执行MySQL语句
	$result=mysql_query($sql);

	
	while(1)
	{
		$tt  = now();
		$sql = "insert into test values(1, $tt)";
		$result=mysql_query($sql);
	}

?>

设置连接ceph集群句柄：
	1、创建一个集群句柄用于连接存储集群；
	2、使用句柄进行连接，App must supply a monitor address, a username and an authentication key 
once you have a cluster handle, you can:
	1、Get cluster statistic
	2、Use Pool Operation(exists, create , list , delete)
	3、Get  and set the configuration

CREATING AN I/O CONTEXT
	I/O Context functionality includes:
		write/read data and extended attributes;
		List and iterate ove objects and extended attributes
		Snapshot pools, list snapshots, etc
asychronous IO
	rados_aio_write()
	rados_aio_append()
	rados_aio_write_full()
	rados_aio_read()


namespace IceUtil{
	template <class T>
	class Monitor {
		public:
			void lock() const;
			void unlock() const;
			bool tryLock() const;
			
			void wait()   const;
			bool timeWait(const Time&) const;
			void notify();
			void notifyAll();

			typedef LockT<Monitor<T> > Lock;
			typedef TryLockT<Monitor<T> > TryLock;
	}

}
cookie包含啥信息


不重启修改hostname
1、修改/etc/host/
	127.0.0.1  new-hostname
2、hostname new-hostname
3、退出shell。再重新登录


nslookup www.baidu.com 8.8.8.8

route -n  查看网关

linux内网ip设置
/etc/network/interfaces

加上：
auto eth1
iface eth1 inet static
	address 10.0.0.50
	netmask 255.255.255.0
/etc/init.d/networking restart


[root@localhost ~] vi /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0(设置网卡设备名)
BOOTPROTO=static(设置网卡以static或dhcp方式运行)
BROADCAST=192.168.1.255(广播地址，可以不设)
HWADDR=00:40:D0:13:C3:46(mac地址，默认不需要设置)
IPADDR=192.168.1.100(本机IP地址)
NETMASK=255.255.255.0(子网掩码)
NETWORK=192.168.1.0(网络地址，可以不设)
GATEWAY=192.168.1.1(默认网关，可以不设)
ONBOOT=yes(设置是否开机启动，yes为自动启动)
MTU=500(1设置最大传输单元的值，一般很少用到)
以上就是ifcfg-eth0的设置值了。
一般来说，如果设置静态IP的话，只需要设置以下几个值:
DEVICE、ONBOOT、BOOTPROTO、IPADDR、NETMASK
如果设置动态IP，只需设置:
DEVICE、ONBOOT、BOOTPROTO


//删除配置文件
 apt-get remove  ceph ceph-common ceph-fs-common ceph-mds  --purge


删除dpkg数据库保存信息的脚本：
#!/bin/bash
set -e

# Clean out /var/cache/apt/archives
apt-get clean
# Fill it with all the .debs we need
apt-get --reinstall -dy install $(dpkg --get-selections | grep '[[:space:]]install' | cut -f1)

DIR=$(mktemp -d -t info-XXXXXX)
for deb in /var/cache/apt/archives/*.deb
do
    # Move to working directory
    cd "$DIR"
    # Create DEBIAN directory
    mkdir -p DEBIAN
    # Extract control files
    dpkg-deb -e "$deb"
    # Extract file list, fixing up the leading ./ and turning / into /.
    dpkg-deb -c "$deb" | awk '{print $NF}' | cut -c2- | sed -e 's/^\/$/\/./' > DEBIAN/list
    # Figure out binary package name
    DEB=$(basename "$deb" | cut -d_ -f1)
    # Copy each control file into place
    cd DEBIAN
    for file in *
    do
        cp -a "$file" /var/lib/dpkg/info/"$DEB"."$file"
    done
    # Clean up
    cd ..
    rm -rf DEBIAN
done
rmdir "$DIR"
执行完毕后也许提示类似于x11-common有空文件列表
解决办法是编辑/var/lib/dbkg/status文件
找到开头为 Package: x11-common的那一段，然后删除，再重新安装
终端输入

gedit /var/lib/dbkg/status
然后删除x11-common那一段
继续输入
apt-get --reinstall install x11-common




vim中删除到行首/尾：d^/d$


1、首先停止osd进程后，清空相应目录中的内容
/etc/ceph/
/var/lib/ceph/osd
/var/lib/ceph/bootstrap-osd
/mnt/ceph/osd1
2、在mon节点执行
ceph-deploy admin osd-hostname
3、从crush map中移除osd的信息
ceph osd out osd.x
out出去，待重新平衡之后，删除就可以
ceph osd crush remove osd.x
ceph auth del osd.x
ceph osd rm osd.x
//osd挂掉之后，最好不要直接out掉该osd点；



ceph存储区别：
块存储例如iscsi，对象存储例如fastdfs、文件系统就是本地存储或者mfs


ceph ceph-common ceph-fs-common ceph-mds librados2 librbd1 python-ceph   // 卸载这些
ceph-deploy install --release firefly  admin node1 node2 node3

apt-get install librbd-dev  //单独安装

Ceph的底层是RADOS(可靠、自动、分布式对象存储)，可以通过LIBRADOS直接访问到RADOS的对象存储系统。RBD(块设备接口)、
RADOS Gateway(对象存储接口)、Ceph File System(POSIX接口)都是基于RADOS的。


grep -n rbd_create * -r

rbd -p libvirt-pool ls   //查找池里文件


int ret = -1;
char *name = "my_image";
uint64_t  size = 4 * 1024**3 ;
int obj_order = 1;
rbd_image_t* image = NULL;
char *snap_name = "snap_name";

ret = rbd_create(io_ctx,  name, bytes,  &obj_order );
if(ret == 0){
	printf("success \n");
}else{
	printf("failure \n");
}

ret = rbd_open(io_ctx, name, image, snap_name);
if(ret == 0){
	printf("rbd_open image success \n");
}else{
	printf("rbd_open image failure \n");
}


rados_ioctx_destroy(io_ctx);
rados_shutdown(cluster);





#!/usr/bin/python
#coding=utf-8

import rbd 
import rados

cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
try:
	cluster.connect()
	ioctx = cluster.open_ioctx('libvirt-pool')
	try:
		rbd_inst = rbd.RBD()
		size = 4 * 1024**3  # 4 GiB
		rbd_inst.create(ioctx, 'myimage', size)
		image = rbd.Image(ioctx, 'myimage')
		try:
			data = 'foo' * 200 
			image.write(data, 0)
		finally:
			image.close()
	finally:
		ioctx.close()
finally:
	cluster.shutdown()

g++ test2.cpp -lrados  -lrbd -o client++


qemu-img create -f rbd rbd:libvirt-pool/new-image 2G


size   			obj_order 
2147483648 B	22

rbd命令集合：
rbd -p pool-name ls


vim 显示tab键  set list
1、显示 TAB 键

文件中有 TAB 键的时候，你是看不见的。要把它显示出来：

:set list

现在 TAB 键显示为 ^I，而 $显示在每行的结尾，以便你能找到可能会被你忽略的空白字符在哪里。 
这样做的一个缺点是在有很多 TAB 的时候看起来很丑。如果你使用一个有颜色的终端，或者使用 GUI 模式，Vim 可以用高亮显示空格和TAB。 
使用 ‘listchars’ 选项：

:set listchars=tab:>-,trail:-

现在，TAB会被显示成 ">—" 而行尾多余的空白字符显示成 "-"。看起来好多了，是吧？
100000000000000000000000000000000


create table base_info(
	id int IDENTITY(1, 1) PRIMARY KEY,
	domain_name char(50)  default NULL,
	domain_admin char(50) default NULL,
	domain_tel   char(50) default NULL,
	domain_mail  char(50) default NULL,
	domain_addr  varchar(50) default NULL,
)


修改权限：
	 vm/vmd/vmd_monitor.pl 
	/root/vm/vmd/flashpolicyd/flashpolicyd.py flashpolicyd.init  flashpolicyd.py  install.sh  nohup.out  run


参数：poolname,  image_name（单位是B）, bytes(镜像大小)， 分块默认值：22
写入的内容，内容的大小，偏移量ofs, 

创建kvm虚拟机：
使用命令行：
	#qemu-img create -f qcow2 /var/lib/libvirt/images/centos6.6-x86_64.qcow2 2G
	#virt-install --virt-type kvm --name centos6.6-x86_64 --ram 1024 	\
	--disk /var/lib/libvirt/images/centos6.6-x86_64.qcow2,  format=qcow2	\
	--network network=default	\
	--graphics vnc, listen=0.0.0.0 --noautoconsole	\
	--os-type=linux --os-variant=rhel6	\
    --cdrom=/opt/iso/CentOS-6.6-x86_64-minimal.iso
或者使用图形化界面：
打开 virt-manager
远程登录到装的 kvm 平台的服务器上，打开 virt-manager（Xshell 在会话属性-连接-SSH-隧道-X11 转移-勾选转发X11连接到 Xmanager）



stresslinux
ceph稳定性测试工具：iozone



查看ceph相关文档：
ceph保存数据一致性的方法：
	Ceph的OSD可以比较放置在一个组对象元数据存储与在其他的OSD布置组及其副本。清理（通常每天执行）捕获OSD的错误或文件系统错误。
	OSD还可以通过比较对象中的数据比特位进行更深层次的清理。深度清理（通常每周执行）可以发现在平时清理中不会被发现的磁盘上的坏扇区。

ceph状况的详细状态：ceph health detail

配置文件直接下发
	ceph-deploy admin      cs-node4（配置点）	//先cd到/ceph目录下
修复pg点：
	 ceph pg repair 3.7b


crush算法通过计算数据的存储位置确定了如何存储和检索数据，crush授权ceph的客户端直接与osd沟通，而不是通过
一个通过集中的服务器或代理。拥有一个决定存储和检索数据方法的一个算法；ceph避免了单点故障，性能瓶颈，它的
规模的物理极限；
	crush需要集群的一个映射，并使用crush有映射在整个集群的数据是均匀分布在osd上防伪随机的存储和检索数据；
crush映射包含一系列的osd，一系列聚集设备至物理位置的buckets和一系列的告诉crush如何在ceph的存储池上复制数据
的规则。通过底层物理组织安装的反馈。crush可以模拟相关


ceph基本操作：
	/etc/init.d/ceph -a start/stop  //即在所有节点上执行

	单个操作：
		start/stop ceph-osd id={id}
		start/stop ceph-mon  id={hostname}
		start/stop ceph-mds  id={hostname}

	集群监控：监控osd状态、monitorstatus、placement组（pg）、元数据服务器状态；
	#ceph        //进入ceph会话模式
	ceph> health
	ceph> status
	ceph> quorum_status		//？？
	ceph> mon-status	

	实时监控集群：
	#ceph -w

	#ceph osd stat        //检测osd的状态
	#ceph osd  dump		  //检测osd的详细信息
	#ceph osd tree		  //根据Crush Map查看osd的状态

	monitor监控：
	#ceph mon stat 		
	#ceph mon dump
	#ceph quorum_status

hash(object1) = key1;
.....
hash(object4) = key4;


	mds监控：
	#ceph mds stat
	#ceph mds dump

	使用admin socket
	ceph管理员通过一个Socket接口查询一个守护进程，默认情况下，在/var/run/ceph下
		#ceph --admin-daemon /var/run/ceph/{socket-name}

		#ceph --admin-daemon /var/run/ceph/{socket-name} help

	监控守护进程OSD的4中状态：
		In----Out	（是否在集群里面）	
		Up----Down	（是否是开启状态）

	pg集群：（placement  groups）
		查看pg列表
		#ceph pg dump
		#ceph pg stat
		#ceph pg dump -o {filename} --format=json  #输出json格式，并保持文件



		查看pg map 及 参数
		#ceph pg map {pg-num}
 
 		#获取池子副本个数：
 		#ceph osd pool get POOLNAME size
		#查看osd池子个数
		ceph osd lspools
		#池子名字必须是两次
		ceph osd pool delete {pool-name} {pool-name} --yes-i-really-really-mean-it



du -sh  /var/local //查看目录下所有文件大小


从crush map中移除osd的信息
ceph osd out osd.x
ceph osd crush remove osd.x
ceph auth del osd.x
ceph osd down osd.x
stop ceph-osd id=x
ceph osd rm osd.x




      <auth username='libvirt'>
        <secret type='ceph' uuid='0f1c4968-0400-4205-805c-8b5b8861ee9b'/>
      </auth>

secret-list




sudo passwd root

首先得说明一下，没有0/24是什么意思的说法。192.168.1.0/24表示网段是192.168.1.0，子网掩码是24位，子网掩码为：255.255.255.0，用二进制表示为：11111111 11111111 11111111 00000000 ，这里为什么是24呢，就是因为子网掩码里面的前面连续的“1”的个数为24个，一定要连续的才行。

再给你举个例子，192.168.1.0/28表示的意思是网段是192.168.1.0，子网掩码为：255.255.255.240，用二进制表示为：11111111 11111111 11111111 11110000。

这时候你也许就疑惑了，就是24和28两个字不一样，为什么网段是一样的呢？




命令待分析：
	1、ip route
	2、nohap ./run &
	3、 ip route add  222.132.16.48/28 dev br0  proto kernel  scope link  src 
		 222.132.16.50
	4、ip route del 222.132.16.48/28
	5、ip rule
	6、arp -n
	7、netstat -tlunp   #查看存在的连接
	8、lsblk


many objecs map to one PG
each object maps to exactly one PG
One PG maps to a single list of OSDs, 
Many PGs can map to one OSD

A PG represents nothing but a grouping of objects; you configure the number of PGs you want
(http://ceph.com/wiki/Changing_the_number_of_PGs )

//伪随机算法
locator = object_name
obj_hash = hash(locator)
pg = obj_hash%num_pg
OSDs_for_pg = crush(pg)
primary = osds_for_pg[0]
replicas = osds_for_pg[1:]


def crush(pg):
	all_osds =['osd.0', 'osd.1', 'osd.2', ....]
	return = []
	while len(result) < size:
		r = hash(pg)
		chosen = all_osds[r%len(all_osds)]
		if chosen in result:
			#OSD  can be picked only once
			continue
		return.append(chosen)

	return result

ceph计算pg数量：
	每个存储池的PG = （OSD*100）/replica_size（副本数）
	若PG  - 2^(X) < 0.25* PG， 那么设置PG数为2^(X)；否则设置为2^(X+1);
	例如：
		3个osd，存储池的size是2，那么
		PG = 3 * 100 / 2 = 150
		0.75 * 150 < 2^7,所以PG=128，X=7

开源监控利器grafana
ceph命令：
	ceph osd pool set rbd pgp_num 100
Important：Ceph doesn’t support QCOW2 for hosting a virtual machine disk. Thus 	 if you want to boot virtual machines in Ceph (ephemeral backend or 			boot from volume), the Glance image format must be RAW.
Note that image must be RAW format. You can use qemu-img to convert from one format to another：
	qemu-img convert -f {source-format} -O {output-format} {source-filename} {output-filename}
	qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw

 rbd是一个实用程序，用于操纵rados块设备（RBD）的镜像，QEMU/KVM就是使用的Linux rbd驱动和rbd存储驱动
 打印池子详细信息：rbd -p libvirt-pool ls  -l

ceph log路径：/var/log/ceph/ceph

详细信息查看osd0
ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | less

ceph设置池子pg_num:
ceph osd pool set {pool-name} pg_num

PG:Placement Group，用途是对object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object（数千上万），
但一个object只能被映射到一个PG咋红，即：PG和object之间是“一对多”的映射关系，
object id;即：oid



导入池子：
rbd import ./ceph_test.qcow2  --dest-pool libvirt-pool


比如你现在有3个osd。rbd pool size是3，min_size是2。集群健康的情况下，每个object有3个副本。如果一个osd挂掉，object就只剩两个副本了。这时虽然集群不健康了，但仍可以正常访问pool里的数据。然后要是再挂掉一个osd，object就只剩一个副本了，这时就无法正常访问pool里的数据了。

dhcplog：/var/log/messages  或者/var/log/syslog




增加sodu权限：
chmod u+w /etc/sudoers
vim  /etc/sudoers
xxx ALL=(ALL) ALL -----------xxx用户名


NetType==0情况：
select sql_no_cache a.id, a.control_ip, a.cluster_id, b.id, b.pub_gate_way, b.pub_mask, b.br_name, a.bakup_node_id from `node` a, `node_eth` b, `cluster`  c
NetType==1情况：
select sql_no_cache a.id, a.control_ip, a.cluster_id, b.id, b.virt_gate_way, b.virt_mask, b.network_name, a.bakup_node_id from `node` a, `node_eth` b, `cluster` c 


	不带vlan、不指定集群:
		where a.status=1 and a.vm_count < a.max_vm_count and a.service_type=1 and b.status=1 and b.idc_route_id="<<IdcRouteId<<" and c.status=1 and c.type=1  and c.vlan_con_node_id=0 and a.id=b.node_id and a.cluster_id=c.id；
	不带vlan，指定集群：
		where a.cluster_id="<<ClusterID<<" and a.status=1 and a.vm_count < a.max_vm_count and a.service_type=1 and b.status=1 and b.idc_route_id="<<IdcRouteId<<" and c.status=1 and c.type=1 and c.vlan_con_node_id=0 and a.id=b.node_id and a.cluster_id=c.id；

	指定vlan情况：（集群id必须有）
		where a.cluster_id="<<ClusterID<<" and a.status=1 and a.vm_count < a.max_vm_count and a.service_type=1 and b.status=1 and b.idc_route_id="<<IdcRouteId<<"   and c.status=1 and c.type=1 and c.vlan_con_node_id>0 and a.id=b.node_id and a.cluster_id=c.id；



服务器启动跳过检查硬盘
shell大于：greater  than  //gt
	小于：less  than      //lt
	等于：equal    		  //eq 	







openstack 命令：
nova-manage service list


root@222-132-16-53:~# ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx
> pool=volumes, allow rx pool=images'
[client.cinder]
	key = AQDIfthUuBylJhAA++HQpt7a2uAPSsPcfJW6CA==

root@222-132-16-53:~# ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx
> pool=images'
[client.glance]
	key = AQByf9hUuBKaNhAAlNbLzVRny7DLGaOvymJ6/Q==


libvirt问题；看/var/log/libvirt/qemu日志
openstack的log路径：find


ceph认证加入：libvirt：
1、ceph auth get-key client.cinder | tee client.cinder.key
2、virsh secret-define --file secret.xml
3、virsh secret-set-value --secret bfe573d1-0a91-42e8-941a-49d5730c4150  --base64 $(cat client.cinder.key) 


bash注释：
:<<EOF
注释的代码
EOF

openstack 命令：
nova-manage service list

source  admin-openrc.sh
cinder list


cinder create --display_name cin02 2

killall screen;  killall python

ntpdate asia.pool.ntp.org  //同步时间

nova-manage --version

keystone user-list	//查看openstack用户

2661


 for i in {1..10}; do echo "$i"; done;
 for((i=0;i<=10;i++));do echo "$i"; done
find    /opt   -mmin   -1  //查找1分钟之内修改过的文件



libvirt + qemu + kvm + 内核：

shell多行注释：
:'
echo "ni"
echo "ni"
echo "ni"
echo "ni"
'
1、增加monitors
ceph-deploy mon add ceph-node2 
ceph-deploy mon add ceph-node3
ceph-deploy mon create ceph-node2 ceph-node3

ceph-deploy gatherkeys ceph-node2
ceph-deploy gatherkeys ceph-node3


调试网络
ping www.baidu.com
arp -n 

tcpdump -n arp
tcpdump -en arp -i eth0


下载模板：（确认依赖关系：qemu-img info ）
wget ftp://vmread:hqcloud@203.156.198.3/kvm/win2003-32.img

libvirt  serverkey.pem
virsh /api  clientkey.pem

mtr ip
dig ip

证书验证：放在window下，linux下的.pem文件改成.crt文件，打开即可查看；


ubuntu下强制更新dhcp模式下获取的ip
$ sudo dhclient -r //release ip 释放IP
$ sudo dhclient //获取IP
win：(卸载网卡，之后在安装，方法之一)
ipconfig /release
ipconfig /renew


# /etc/init.d/networking restart 


 tcpdump -en -i vnet5 port 67 or port 68



vim多行操作
<ESC>之后按v进入visual模式。 
<ESC>之后按CTRL+v进入visual block模式（列编辑）。
按下I之后插入操作，按下x删除操作
再按下ctrl+[ 退出编辑即可

substitutil-g 显示目前编辑的文件名、是否经过修改及目前光标所在之位置。
:f 文件名改变编辑中的文件名。(file)
:r 文件名在光标所在处插入一个文件的内容。(read)
:35 r 文件名将文件插入至35 行之后

on 替换

:f 或Ctr

cw:删除一个单词并进入插入模式,cc:删除一行并进入插入模式。
r:然后输入的字母将替换当前字母并保持命令模式,R 则是不停的替换(一个挨着一个)。

yfa 表示拷贝从当前光标到光标后面的第一个a 字符之间的内容.
dfa 表示删除从当前光标到光标后面的第一个a 字符之间的内容.





wget 114.80.119.146/change_xml.sh  && chmod 777 change_xml.sh


ls -l /dev/disk/by-uuid/          


win
route print

tcpdump -en -i eth0 imcp and host 128.0.10.1


eptables -L

172.22.0.254
172.22.0.219

172.22.0.198
172.22.0.130
172.22.0.219

172.22.0.55
172.22.0.2

/etc/iproute2/



qemu-img check -f qcow2 -r all copy.img  //检测并修复磁盘
ifconfig   br0
ovs-ofctl show br0

ip route add default via  203.156.196.1 src 203.156.196.3

ip route change default via  203.156.196.1 src 203.156.196.3


mysqladmin -u root password "newpwd"

证书验证：放在window下，linux下的.pem文件改成.crt文件，打开即可查看；





那vmc最近发现好几个要更新的功能。 底层机器不存在时的删除接口；这个控制节点连接失败重试报警；还有个是克隆线程失败的时候连接控制中心失败也要重试报警。

ifconfig |grep  HWaddr |grep  -v br0: |grep br0  |awk '{print $5}'


CentOS 6.6下yum快速升级内核
作者：大D 分类：实验室 发表于：2015年1月5日 09:59 评论：0 条
1、导入public key


1
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
2、安装ELRepo到CentOS 6.6中


1
rpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm
3、安装长期支持版本


1
yum --enablerepo=elrepo-kernel install kernel-lt -y
4、编辑grub.conf文件，修改Grub引导顺序


1
vim /etc/grub.conf
确认刚刚安装的内核的位置，然后将default修改一下。

# Note that you do not have to rerun grub after making changes to this file
# NOTICE:  You have a /boot partition.  This means that
#          all kernel and initrd paths are relative to /boot/, eg.
#          root (hd0,0)
#          kernel /vmlinuz-version ro root=/dev/mapper/vg_have-lv_root
#          initrd /initrd-[generic-]version.img
#boot=/dev/xvda
default=0
timeout=5
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title CentOS (3.10.63-1.el6.elrepo.x86_64)
        root (hd0,0)
        kernel /vmlinuz-3.10.63-1.el6.elrepo.x86_64 ro root=/dev/mapper/vg_have-lv_root rd_LVM_LV=vg_have/lv_root rd_LVM_LV=vg_have/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYTABLE=us console=hvc0 crashkernel=auto rhgb quiet
        initrd /initramfs-3.10.63-1.el6.elrepo.x86_64.img
title CentOS (2.6.32-504.3.3.el6.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.32-504.3.3.el6.x86_64 ro root=/dev/mapper/vg_have-lv_root rd_LVM_LV=vg_have/lv_root rd_LVM_LV=vg_have/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYTABLE=us console=hvc0 crashkernel=auto rhgb quiet
        initrd /initramfs-2.6.32-504.3.3.el6.x86_64.img
title CentOS (2.6.32-358.6.1.el6.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.32-358.6.1.el6.x86_64 ro root=/dev/mapper/vg_have-lv_root rd_LVM_LV=vg_have/lv_root rd_LVM_LV=vg_have/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYTABLE=us console=hvc0 crashkernel=auto rhgb quiet
        initrd /initramfs-2.6.32-358.6.1.el6.x86_64.img
title CentOS (2.6.32-220.el6.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.32-220.el6.x86_64 ro root=/dev/mapper/vg_have-lv_root rd_LVM_LV=vg_have/lv_root rd_LVM_LV=vg_have/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYTABLE=us console=hvc0 crashkernel=auto rhgb quiet
        initrd /initramfs-2.6.32-220.el6.x86_64.img
title CentOS (2.6.32-131.0.15.el6.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.32-131.0.15.el6.x86_64 ro root=/dev/mapper/vg_have-lv_root rd_LVM_LV=vg_have/lv_root rd_LVM_LV=vg_have/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYTABLE=us console=hvc0 crashkernel=auto rhgb quiet
        initrd /initramfs-2.6.32-131.0.15.el6.x86_64.img
新装的内核位置为0，所以讲default修改为0，保存退出重启。

5、检查内核


1
2
[root@MyServer ~]# uname -r
3.10.63-1.el6.elrepo.x86_64



 	-c:并发数目		-t：
webbench -c 1000 -t 3000 http://localhost:8088/test


openstck mysql密码：openstack



--overwrite-conf



--image-format 2
 port='6003' autoport='no' listen='222.132.16.50' keymap='en-us' passwd='698dcf3c71b8311f'


[osd.4]
   host = osd4
   deves = /dev/sda3


698dcf3c71b8311f

克隆所得的镜像大小为：
root@222-132-16-50:~# rbd info  libvirt-pool/clone_ubuntu_snap
rbd image 'clone_ubuntu_snap':
  size 2574 MB in 644 objects
  order 22 (4096 kB objects)
  block_name_prefix: rbd_data.276c2ae8944a
  format: 2
  features: layering
  parent: libvirt-pool/Ubuntu12.04_x64.img@ubuntu_snap
虚机里面大小为19G（df -h），（具体如何得来未知）



rollback测试：
测试机器：ee1f6a40-c4a2-4e34-a931-9cb9742e6e8b
测试系统：Ubuntu12.04_x64.img
快照：clone_ubuntu_snap
克隆生成镜像名称：

关机之后，rollback；
rbd snap rollback libvirt-pool/clone_ubuntu_snap@rollback5
开机时间大概：11：09----（含dhcp配置网络时间）
  根目录下，文件系统只读了；





1、rbd snap create  libvirt-pool/clone_snap_image@snap_image
2、rbd snap protect libvirt-pool/clone_snap_image@snap_image
3、rbd clone libvirt-pool/clone_snap_image@snap_image   libvirt-pool/clone_image
4、rbd   flatten  libvirt-pool/clone_image
5、rbd snap unprotect libvirt-pool/clone_snap_image@snap_image



端口开放情况：
	netstat -antp
	 iptables -F    //端口不通也有可能是防火墙问题；

c++字符串分割：
	std::string str_buf = "hello/world";
	std::vector<std::string> str_arr;
	split(str_buf, "\n", str_arr);

	for(std::vector<std::string>::iterator it=str_arr.begin();
		it !=str_arr.end();it++){
		std::cout << "std::string is" << *it << std::endl;
		}
	std::string tmp = str_arr[str_arr.size() -1 ]
string s("12345asdf");
string a=s.substr(0,5);       //获得字符串s中 从第0位开始的长度为5的字符串//默认时的长度为从开始位置到尾


去掉字符串最后的换行符：
 if(my_string[my_string.Length()]=='\n')     
  my_string.Delete(my_string.Length(),1);


zt  		"当前行置于屏幕顶端
ctrl+f      "向前滚一整屏


移除mon：
	1、service ceph -a stop mon.{mon-id};  //mon-id可由ceph  mon dump获得
	2、ceph mon remove {mon-id}
增加monitor：

下载vmd更新：
wget  http://114.80.119.146/vm-dl/curl.tgz

rbd -p kvm ls  |xargs -n 1 -i rbd info kvm/{} |more
-i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给{}，可以用{}代替。
-n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。

去掉最开头二行，最末尾一行
virsh list --all | awk 'NR>3{print p}{p=$0}' |awk '{print "virsh dumpxml "$2" > "$2".xml"}'|sh
awk 'NR>2{print p}{p=$0}' urfile
$0:表示行号的内容，切记
第一行时, NR=1, 不执行print, p=第一行的内容
第二行时, NR=2, 不执行print, p=第二行的内容
第三行时, NR=3, 执行print p,此时p=第二行的内容, 即打印第二行, 然后p=第三行
............
最后一行时, 执行print p, 打印倒数第二行, 然后p=最后一行

也就是去除了第一行和最后一行... 



auth_key

libvirt 迁移命令：
virsh  migrate --live 53fc497f-72be-80c4-a177-bb20c356929c qemu+tcp://192.168.2.17/system


VIR_MIGRATE_UNDEFINE_SOURCE, VIR_MIGRATE_PEER2PEER

qemu 迁移使用方法
 
迁移是qemu中支持的，libvirt只是封装好命令并传递给qemu的监控模块。
1、qemu中使用方法：
在server端，在启动qemu的命令行中加入-incoming tcp:0:4444(4444为端口）参数，启动后可以应用netstat -apn 命令查看，4444端口是不是在监听。
在client端，启动qemu后（sdl模式下）， 使用ctrl+alt+2切换到监视端口，输入命令:migrate -d tcp:10.10.10.1:4444。（可以使用info migrate 查看migrate状态）

2、virsh中使用实例：
migrate 待迁移域名 qemu://10.10.10.1/system （tls 模式）
migrate 待迁移域名 qmeu+ssh://10.10.10.1/system （ssh 模式）
migrate 待迁移域名 qmeu+tcp://10.10.10.1/system （tcp 模式）

上面是静态迁移，其中tls模式需要加密和鉴权文件，详细操作见下面网址：

http://wiki.libvirt.org/page/TLSSetup#Full_list_of_steps

migrate --live 待迁移域名 qemu+ssh://10.10.10.1/system（动态迁移）

3、在virt-manager中，在虚拟机名字上右击鼠标右键，弹出的对话框中，有迁移一项，选上offline为静态迁移，不选为动态迁移。

迁移flag：VIR_MIGRATE_LIVE	：动态迁移 
		  VIR_MIGRATE_PERSIST_DEST ：
		  VIR_MIGRATE_PEER2PEER	   ：
		  VIR_MIGRATE_NON_SHARED_INC：增量形式
		  VIR_MIGRATE_NON_SHARED_DISK：完全拷贝

migrate command:
virsh migrate 53fc497f-72be-80c4-a177-bb20c356929c --live qemu+tls://192.168.2.17/system --xml  /etc/libvirt/qemu/53fc497f-72be-80c4-a177-bb20c356929c.xml 


1、增量迁移；完全迁移；
2、磁盘信息已使用/容量区别


select b.storage_type, b.type, b.vlan_con_node_id, b.volume_name,b.port, b.control_ip, b.auth_key from node a, cluster b where a.control_ip='192.168.2.18' and a.cluster_id=b.id

egraded状态是pg的副本数不够，看下是不是存储池的副本数大于crush的分组数

undersized是up集合和acting集合不一致



ceph存储集群API:
	apt-get install librados-dev (ubuntu)
	apt-get install librados2-devel (centos)


windowserver 2012链接:
9600.16384.WINBLUE_RTM.130821-1623_X64FRE_SERVER_SOLUTION_ZH-CN-IRM_SSSO_X64FRE_ZH-CN_DV5.ISO 

模板密码:yunvm.com789

p/src# rbd snap create template/centos_6.0_x86.img@snapname_01
root@u18:~/cp/src# rbd snap protect template/centos_6.0_x86.img@snapname_01
root@u18:~/cp/src# rbd clone template/centos_6.0_x86.img@snapname_01 kvm/centos_test_01
root@u18:~/cp/src# rbd -p kvm ls |grep centos

ceph auth caps client.libvirt mon 'allow r'  osd 'allow class-read object_prefix rbd_children, allow rwx pool=kvm, allow rx pool=template'

rbd import --image-format 2 ./ubuntu_12.04_x86.img   --dest-pool template


valgrind


先tell ，然后写ceph.conf，然后push

ceph编辑ceph map:
导出并编辑：
	ceph osd getcrushmap -o map
	crushtool -d map -o map.txt
	vim map.txt
导入：
	crushtool -c map.txt -o map
	ceph osd setcrushmap -i map

参照：http://ceph.com/docs/master/rados/operations/crush-map/


osd配置：
  ssh {new-osd-host}
  mkfs -t {fstype}  /dev/{disk}
  mount -o user_xattr /dev/{hdd}  /var/lib/ceph/osd/ceph-{osd-number}

下面命令列出Ceph作业和例程：
  initctl list |grep ceph


1. apt-get update; apt-get install pptpd
2. vim /etc/pptpd.conf    # 修改客户端 ip 范围
3. vim /etc/ppp/options  # 修改 ms-dns
4. vim /etc/ppp/chap-secrets  # 添加用户名密码
5. sysctl -w net.ipv4.ip_forward=1
    vim /etc/sysctl.conf   # 开启 ip_forward
6. iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j MASQUERADE    # 地址伪装




L爷 2015/5/4 17:43:54
xuzhiquan	*	Xj0Eh2iK0IidIp3B		192.168.0.101
xulei		*	J2oKuJj6m7a6fMJC		192.168.0.102
daiwei		*	uM1sb7JRcR7kGc7W		192.168.0.103
chenxingguo	*	rOWj33tGnvYM09Ep		192.168.0.104
louting		*	0e8qAGdP6OZjfi8M		192.168.0.105
yanxiaoming	*	1WJt7oGlOVWrh3h6		192.168.0.106
jingqiang	*	7K1hRLznHd1d8bWG		192.168.0.107
xiangliang	*	nP1f4zTKGvhNw3M4		192.168.0.108
shenzhiwei	*	Uo9WQ9kmFHH8rk6s		192.168.0.109
genghanghang	*	QtoU80iQlKj3QB2j		192.168.0.110
liyang		*	3SeMqCn7ILu4uG5r		192.168.0.111

L爷 2015/5/4 17:45:02
mkpasswd.pl -l 16 -d 4 -C 6 -c 6 -s 0
密码生成规

less than 5 OSDS set pg_num  to 128
Between 5 and 10 OSDS set pg_num to 512
Between 10 and 50 OSDS set pg_num to 4096


crushmap 简单使用
获取crush map
ceph osd getcrushmap -o /tmp/map  或者  ceph osd  crush  dump ，关于crush的命令集，可以通过　ceph osd crush help 查看

反编译map
crushtool -d /tmp/map -o /tmp/map.txt

 编译map
crushtool -c /tmp/map.txt -o /tmp/map.new

 设置map
ceph osd setcrushmap -i  /tmp/map.new

crushmap设置参看  http://ceph.com/docs/master/rados/operations/crush-map/

设置pool 的crush rule

ceph osd pool set <poolname> crush_rule set 4   #此处 4 是指在rule 里 ruleset 设置的值




ceph tell osd.0 injectargs "--rbd_cache true" 

ceph-deploy disk zap --fs-type xfs osd0:/dev/sd


"osd_scrub_load_threshold": "0.5",

load average 低于0.5 才会srcub 

命令行模式下输入  sp 另外一个文件 就可以水平分割继续打开第二个文件，如果想纵向分割，可以使用vsp 文件名


#define random(x) (rand()%x)

python字典：
	for key, value in data.iteritems()
		print key, value

	for key in data:
		print data[key]

	data.get('key', 'N/A')		//字典的get函数，当key不存在时候，返回第二个参数，
	默认是None
	data[key]   //key不存在时，返回错误，不安全

	data.setdefault('key', 'chengpeng')	//key不存在字典里将该键值对加入字典；

	data['key'] ="cp"		//当字典里含有该key时，会覆盖value，

列表和字典dict是应用程序使用最频繁的数据结构；
元组和字典主要用在函数调用之间交换参数和返回值；

for line in open('./aa.txt'):
	print line


data=[123, 'abc'. 'ddf']
for i, value in enumerate(data):	//使用计数函数enumerate()
	print i, value
0 123
1 abc
2 ddf

使用raise函数抛出异常：
Django创建项目：(提供一个django-admin.py的命令)---（Django内置web服务器）
	django-admin.py startproject mysite
生成mysite目录，里面包含__init__.py ; manage.py  ; setting.py ;  urls.py
manage.py文件：一个同Django项目一起工作的工具；
setting.py文件：包含项目的默认设置，包括数据库信息，调试标志以及其他一些重要的变量；
urls.py文件：在Django里叫urlconf，它是一个url模式映射到你应用程序上的配置文件；
	启动服务：./manage.py  runserver
有了项目之后，我们就可以在项目下面创建应用（“app”）：
	./manage.py  startapp blog      #创建一个blog应用，blog应用是项目里面的一部分；
创建应用之后，我们的项目目录下有一个blog目录，
	 里面包含：__init__.py,   models.py,    views.py

要让Django知道blog 应用是项目的一部分，你应该去编辑setting.py（配置一下）
	打开setting.py在文件尾部找到INSTARLLED_APPS元组，把blog应用以模块的形式加入元组，
		‘mysite.blog’;

设置应用中的model；
	blog应用的核心部分：model.py文件，这是定义blog数据结构的地方；

from django.db  import models
class BlogPost(models.Model):
	title = models.CharField(max_length=150)
	body = models.TextField()
	timestamp = models.DataTimeField()


创建表：
	./manage.py  syncdb

通常做法是，将nginx作为服务器最前端，它将接收WEB的所有请求，统一管理请求。nginx把所有静态请求自己来处理（这是NGINX的强项）。
然后，nginx将所有非静态请求通过uwsgi传递给Django，由Django来进行处理，从而完成一次WEB请求。

http://www.2cto.com/os/201412/359411.html

/home/site/june/june
./templates/node/create.html

c++随机
#include <iostream>
#include <stdlib.h>
#include <time.h>

#define MAX 100

int main()
{
    srand((unsigned)time(NULL));//srand()函数产生一个以当前时间开始的随机种子
    for(int i=0;i<10;i++){
        std::cout <<"rand value:"<<rand()%MAX << std::endl;
    }   
    return 0;
}

flask初始化：
所有的flask程序都必须创建一个程序实例，web 服务器使用一种名为web服务器网关接口（web server gateway interface； WSGI）协议，把接收客户端的所有请求都转交给这个对象处理。程序实例是flask类的对象；
	from flask import Flask
	app = Flask(__name__)
Flask类的构造函数只有一个必须指定的参数，即程序主模块或包的名字；

路由和视图函数：
	客户端（web浏览器）把请求发送给web server， web server再把请求发送给Flask程序实例；
程序实例需要知道对每个url请求运行哪些代码，所以保存了一个url到python函数的映射关系。
处理url和函数之间关系的程序成为路由。

在Flask程序中定义路由的最简便方式，是使用程序实例提供的app.route修饰器。把修饰的函数注册为路由；
例如：
	@app.route('/')
	def index():			//该函数称为视图函数
		return '<h1>Hello World!</h1>'

ssh u62 "cat /root/.ssh/id_rsa.pub_u60>> /root/.ssh/authorized_keys"

 518   for (i = 0; i < num_snaps; i++) {
 519     printf("snap: %s\n", snaps[i].name);
 520   }
 521 


openstack架构分析：
http://blog.csdn.net/gaoxingnengjisuan/article/category/1461395
1、nova-scheduler分析：
	代码路径：/usr/lib/python2.7/dist-packages/nova/scheduler


网络术语：
birdge：网桥
	Bridge（桥）是Linux上用来做TCP/IP二层协议交换的设备，与现实世界中的交换机功能相似。Bridge设备实例可以和Linux上其他网络设备实例连接，
既attach一个从设备，类似于在现实世界中的交换机和一个用户终端之间连接一根网线。当有数据到达时，Bridge会根据报文中的MAC信息进行广播、转发、丢弃处理。
	Bridge的功能主要在内核里实现，当一个从设备被attach到Bridge上时，相当于现实世界里交换机的端口被插入了一根连有终端的网线；
什么是open vSwitch：
	Open vSwitch的目标，是做一个具有产品级质量的多层虚拟交换机。通过可编程扩展，可以实现大规模网络的自动化（配置、管理、维护）。
	它支持现有标准管理接口和协议（比如netFlow，sFlow，SPAN，RSPAN，CLI，LACP，802.1ag等，熟悉物理网络维护的管理员可以毫不费力地通
	过Open vSwitch转向虚拟网络管理）。总的来说，它被设计为支持分布在多个物理服务器，例如VMware的vNetwork分布式vSwitch或思科的Nexu
	s1000V。
那么什么是虚拟交换？虚拟交换就是利用虚拟平台，通过软件的方式形成交换机部件。跟传统的物理交换机相比，虚拟交换机同样具备众多优点，
一是配置更加灵活。一台普通的服务器可以配置出数十台甚至上百台虚拟交换机，且端口数目可以灵活选择。例如，VMware的ESX一台服务器可以仿
真出248台虚拟交换机，且每台交换机预设虚拟端口即可达56个；二是成本更加低廉，通过虚拟交换往往可以获得昂贵的普通交换机才能达到的性能
，例如微软的Hyper-V平台，虚拟机与虚拟交换机之间的联机速度轻易可达10Gbps


WebSocketProxy token_plugin
websockify/websocket.py  start_server


websockify/websocketproxy.py   L103
一个python字典结构

websockify/token_plugins.py  L13

self._targets

class TokenFile

我们先看看什么是CGI (Common Gateway Interface)。CGI是服务器和应用脚本之间的一套接口标准，目的是让服务器程序运行脚本程序，将程序的输出作为response发送给客户。通常来说，支持CGI的服务器程在接收到客户的request之后，根据request中的URL，运行对应的脚本文件。服务器会将HTTP request信息以及socket信息输入给脚本文件，也负责收集脚本的输出，并组装成为合法的HTTP response。利用CGI，我们可以充分发挥服务器的可编程性，动态的生成response，而不必局限于静态文件。
服务器和CGI脚本之间通过CGI标准作为接口。这样就可以让服务器与不同语言写的CGI脚本相配合，比如说使用Apache服务器与Perl写的CGI脚本，或者Python服务器与shell写的CGI脚本




离线恢复RBD的工具(麒麟云)
https://github.com/ceph/ceph/tree/master/src/tools/rbd_recover_tool


























































arp -s 设置绑定网关的mac地址


云主机存在的bug：自动备份的bug

 cmake -G "Unix Makefiles"  ~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp -DEXTERNAL_LIBCLANG_PATH=~/ycm_temp/llvmsrc/lib/libclang.so

 v是可以选定一行任意个字符的,V是行选定的,一次一整行，然后通过向下或向上移动光标而选定多行

按v选定后按=就是自动格式化代码,自动缩进,
 ctrl+r可以恢复



ceph内部
在这章中，将包含以下几点：
	1、ceph 对象
	2、crush算法
	3、归置组
	4、ceph存储池
	5、ceph数据管理

ceph底层：
	现在很清楚ceph的结构以及其核心组件；下一步，我们将着力于ceph在后台是如何工作的；组成一个ceph集群，有很多不为人知的基本元素；下面就让我们来详细了解他们吧；

对象：
	一个对象通常由数据和被绑定在一个唯一标识符的元数据组件构成；这些唯一标识符确保在同一集群中不存在相同标识符的其他对象，这样就保证了对象的唯一性；

	不同于以文件为基础的存储，由于文件大小被限制了大小；随着元数据的变化，对象可以是巨大的；一个对象中，数据存储了元数据信息，这些信息包括数据上下文以及数据内容；对象存储的元数据允许用户妥善管理和使用非结构化的数据；
	参照以下将病人病历作为对象存储的例子：

	一个对象是不会被任何类型和数量的元数据所限制的，这就使得可以给对象弹性的增加自定义类型的元数据，并保证了用户对数据的所有权。它没有使用一个目录层次结构或者存储树结构，相反，它是存储在有序存放了数10亿对象的地址空间中，对象可以存放在本地，但他们也可以存放在地理位置分开的地址空间；在一个连续的存储空间。这类存储机制有助于对象唯一的存储在整个集群中；任何应用程序可以通过RESTful API调用基于对象的唯一标识来从对象中检索数据；
	URL通过相同的方式，一个对象标识符作为对象的唯一指针。这些对象被存储在OSD(基于对象存储的设备)中通过多副本方式；这就使得数据具有高可用性。当ceph存储集群接受到来自客户端写数据的请求，它把数据作为对象存储起来，osd守护进程随后在osd文件系统中把数据写入一个文件；

	定位对象：
		在ceph中，每组数据都以对象的形式存储在内置存储池中，ceph pool是用来存储对象的逻辑分区，它提供了一种有序存储方式；我们将会本章的稍后部分进行详细的理解；现在让我们来深入理解对象，这个ceph存储的中的最小的数据存储单元；当一个ceph集群部署好之后，该集群创建了一些默认的存储池像data，metadata， rbd池；在元数据服务器部署在集群中其中一个节点上之后，元数据服务器在元数据池中创建对象。cephFS只有在元数据服务器部署之后才能正常工作；由于我们在本书讲解之前部署了ceph集群，下面让我们来测试这个对象：
			注意：在ceph Firefly发行版之后的Ceph Giant发行版中，元数据池和data 池只有在配置元数据服务器之后才会被创建，唯一默认的池只有rbd 池

		1、使用以下命令来检测ceph集群状态，你会发现三个池和一些对象：
			#ceph -s

		2、使用以下命令列出ceph集群中池名字，该命令将会显示默认的池，由于没有创建任何池，下面将列出三个池
			#rados lspools

		3、最后，列出从元数据池中列出对象的名，你将发现在这个池中由系统生成的对象
			#rados -p metadata ls


		CRUSH
			在过去的三年里，存储机制涉及到存储数据和存储元数据；所谓的元数据就是关于数据的数据，存储信息等，例如数据实际存储在那些存储节点以及哪些磁盘阵列上；每一次的新数据增加到存储系统中，它的元数据首先更新数据将要存储的物理位置，随后实际数据才被存储。这种工作流程在几兆大小存储规模到几百万兆字节的存储规模下证明是有效的。但是对于存储PB级或百亿字节数据呢？这一机制将可以肯定不适于上述存储，而且，它会存储单点的存储系统故障问题，不幸的是，如果你丢失了你的元数据，那么你就失去了你所有的数据。所以说，保证元数据的安全以及各种形式的灾备是至关重要的，通过在一个节点上多副本拷贝或者对这个数据以及元数据做多副本用以保证高程度的容错。元数据的管理始终是影响存储可扩展性、高可用性、以及整体性能的瓶颈;

			ceph是数据存储和管理的一次改革，它使用可控的扩展散列复制哈希算法；（crush Replication Unber Scalable Hashing）以及智能数据分发机制；crush算法是ceph皇冠上的一颗宝石（crush算法的重要性），它是ceph整个数据存储机制的核心；

			有别于此传统的存储系统依赖于存储和管理元数据和索引表，ceph使用crush算法来确定数据写入或读取的位置，不用于存储元数据，crush算法只有在需要的时候才计算元数据，这样就消除了用传统方式存储元数据对元数据的限制；
		crush 查找
			crush机制工作在元数据的计算工作只有在有需要时候才被分配并执行这一机制下；元数据的计算过程也可以认为是一个crush 查找，如今的计算机硬件可以快速并高效执行crush查找操作；

			关于crush查找算法的特点之一是它不是系统依赖；ceph为客户端提供了良好的扩展执行触发式元数据计算，即利用自身的系统资源执行crush 查找，从而消除了中心查找；

			对于一个有读写操作的ceph集群，客户端首先是要联络上ceph monitor和简索出一个集群map的副本；集群map使得客户端了解ceph集群的状态和配置信息，数据被通过结合池名字以及id转换为对象；为了得到在需要的ceph池中生成一个归置组，对象将被哈希成多个规则组，经计算得到的归置组通过crush 查找来确定存储和检索数据的主osd位置；在计算出精确的osd id号之后，客户端直接连接上该osd并开始存储数据，所有这些操作都是客户端进行的，因此它不影响集群性能；

			一旦数据被写入主OSD，同一节点上执行一次crush查找操作和计算两次规则组和OSD定位,
			由此，数据利用集群的高可用被副本存储到集群中；

			下面看看一个crush查找算法和osd中规则组的例子：
				首先，对象名称和集群规则组号是应用了哈希函数和基于池id，归置组id（PGID）来生成的，其次，crush查找基于PGID来查找到主osd和次osd来写入数据；

			crush压缩
				crush算法是一种让完全让用户了解和配置的设施；它为所有组件维护这一个嵌套层次结构；crush算法设置列表包括磁盘，节点、排、开关、电源电路、机房、数据中心等等，这些组件被称为破坏区和crush桶；crush map列出了可用桶设置的物理位置；包括了一系列的规则来告之crush如何为不同的池中复制数据；下来的图将展示crush如何在基本物理设备进行查找；

	iftop：查看网络问题

	10字符任意病毒
		210.14.78.77


与迁移相关的最主要的是 .vmx 文件和 .vmdk 文件。
arp -n 
tcpdump -i eth1 icmp -en


vmware 转到kvm，转vmx到xml：
	http://blog.csdn.net/epugv/article/details/15504029

http://www.linuxyunwei.com/2013/02/%E8%BF%81%E7%A7%BBvmware%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%88%B0kvm/
qemu-img convert -f vmdk   -O qcow2 Win2008R2.vmdk Win2008R2.img 
Vmware2libvirt –f centos-H.vmx > libvirt.xml

搭建openvpn：
https://docs.ucloud.cn/software/vpn/OpenVPN4Ubuntu.html


openvpn：openssl openssl-dev gcc
翻墙：
http://www.xiaohui.com/dev/server/20070514-install-openvpn.htm


openvpn故障：
http://www.metsky.com/archives/646.html

[i for  i in range(10) if i%2 ==0]
[0, 2, 4, 6, 8]

seq = ["one", "two", "three"]
for  i, element  in enmerate(seq):
	seq[i] = '%d : %s' %(i, seq[i])

seq
['0: one', '1:two', '2: three']

python修饰器：

def  login_required(fn):
    def ff():
        #if user.is_login():
        if True:
            print "0000"
            fn()
        else:
            pass
    return ff

@login_required
def A():
    print "AAAAA"

@login_required
def B():
    print "BBBB"
    #pass

if __name__ == "__main__":
    A()   #执行过程，将函数A作为参数传递给修饰器login_required()，修饰器必须定义一个函数，并返回该函数；否则报错
    #B()

在方法A上边写一个@修饰符，调用方法A的时候会调用修饰符后边的方法B，方法B以A方法为参数，而且需要返回一个可调用的对象，这个可调用的对象会使用A方法提供的参数执行。看这个例子：

#!/usr/bin/env python  
  
def a(fn):  
    print 'a'  
    def d(st):  
        print st+'d'  
    return d  
  
def b(fn):  
    print 'b'  
    return fn  
 
@a  
@b  
def c(st):  
    print st  
      
c('c')  

输出结果：bacd
我们调用c('c')的时候会先调用b(c)，b(c)打印字符"b"然后返回c，然后再调用a(c)，a(c)打印字符"a"，然后返回方法d，然后再执行d('c')，打印cd。

python生成器：
生成器是这样一个函数，它记住了上一次返回时在函数体中的位置，对生成器函数的第二次（第
n次）跳转至该函数的中间，而上次调用的所有局部变量都保持不变。

生成器不仅“记住”了它的数据状态；还“记住”了它在流控制构造在（命令式编程中，这种构造不只是数据值）中的位置。
特点：
	生成器是一个函数，而且函数的参数都会保留。
	迭代到下一次的调用时，所使用的参数都是第一次所保留下的，即是说，在整个所有函数调用的参数都是第一次所调用时保留的，而不是新创建的；
在python中，yield就是这样一个生成器；

yield 生成器的运行机制：
当你问生成器要一个数时，生成器会执行，直至出现 yield 语句，生成器把 
     yield 的参数给你，之后生成器就不会往下继续运行。 当你问他要下一个数时，他会从上次的状态。开始运行，直至出现yield语句，把参数给你，之后停下。如此反复
     直到退出函数；

yield的使用：
	在python中，当你定义一个函数，是哟嗬那个了yield关键字时，这个函数就是一个生成器。它的执行会和其他普通的函数有很多不同，函数返回的是一个对象，而不是你平常所用return语句那样，能得到结果值。如果想取得值，那得调用next()函数，如：






radosgw-admin user info --uid=<uid>
radosgw-admin bucket list  --uid=37


python 
split（）函数字符串分割函数
join（） 字符串连接函数
iptables-restore < /etc/iptables 

ceph osd把所有的数据都看做对象来存储在一个平面空间（就是说没有目录结构层次），
一个对象拥有集群范围唯一标识符，二进制数据以及由一对键值对构成的元数据；
ceph在其客户端定义了数据分布，the Ceph block device将一个块设备镜像映射到集群里存储的一系列对象上


包含唯一的ID，数据和名称/值配对元数据的对象可以代表
结构化和非结构化数据，以及传统和领先的数据存储接口。

一个Ceph的存储集群可容纳大量的Ceph节点的有效无限
可扩展性，高可用性和性能。每个节点利用商用硬件和
智能Ceph的守护进程，相互沟通（node之间）：
1、存储和检索数据；
2、复制数据；
3、监控和报告集群健康状态（心跳）
4、动态地重新分发数据（回填）
5、确保数据的完整性（洗涤过程）
6、从故障中恢复；

从ceph客户端读写数据的接口看，ceph存储集群看起来像一个简单的存储数据的池子；
然而，存储集群通过一种对客户端接口看来是完全透明的方式执行很多复杂的操作；
ceph 客户端和ceph OSD均使用了crush算法，以下各节就如何使CRUSH的Ceph无缝地执行
这些操作细节。

池：
ceph存储集群把数据对象存储在一种叫做pool的逻辑分区上。您可以创建池
对于特定类型的数据，如用于块设备，对象网关，或只是简单地以分离用户。

从Ceph的客户端的角度来看，存储群集是很简单的。当Ceph的客户端读取
或向其写入数据（即，称为I / O上下文）时，它总是连接到ceph存储集群中的
其中一个存储池，客户端指定的池的名称，用户和秘密密钥，所以池就好象充当
与访问控制数据对象的逻辑分区；

事实上，一个CEPH池不仅用于存储数据对象的一个逻辑分区，它还扮演了一个关键角色
即如何在Ceph的存储集群分布和存储数据，然而这些复杂的操作对ceph客户端来说都是
可见的。
ceph 池定义如下：
	池类型：在早期的ceph版本中，池只是简单地为对象维护多、深拷贝副本，如今
Ceph的可以维护对象的多个副本，以及使用擦除编码。由于保证数据持久性的方式不同于深拷贝以及编码，Ceph的支持池类型。池类型是完全透明的客户端。

缓存层：
在Ceph的早期版本，客户端只能数据直接写入到OSD。如今，
CEPH还可以支持高速缓存层和后备存储层。缓存层是由一池
较高的成本/性能更高的硬件，如固态硬盘。后备存储层是
主存储池。高速缓存层次加快读取和写入的高性能运算
硬件。缓存层（及其池名称）是完全透明的客户端。

规则组：在艾字节规模的存储集群，一个Ceph的池可以存储数百万数据对象或更多。
由于ceph必须处理数据持久性（副本或删除代码块），擦洗，复制，再平衡和恢复，
每个对象的基础上管理数据呈现可伸缩性和性能瓶颈。 
Ceph通过规则组来屏蔽池解决这一瓶颈， crush算法分配每个对象一个规则组，
每个规则组对应一组OSD;

CRUSH规则设置：高可用性，耐用性和性能都在Ceph的极为重要的。
crush算法计算出存储对象的归置组和OSD中的活跃组（acting set即[0, 5, 6]）
（0，是组primary osd id， 5，6是副本OSD ID）；
CRUSH也扮演其他重要作用：即，crush可以识别失败域和性能域
（即，类型的存储介质以及节点的，机架，行等包含其中）。 CRUSH使客户能够跨越
失败域写入数据（房间，机架，行等），这样，如果一个簇的大粒度部分失败（例如，齿条），该
集群仍然可以在降级状态下运行，直至其恢复。 CRUSH使客户能够写入数据
到特定类型的硬件（性能域），如固态硬盘，固态盘硬盘
期刊，或者与同一驱动器上的数据刊物硬盘。crush规则集
确定出现故障域和域表现为池。 
高可用性：
CEPH提供高耐用性的数据在两个方面：第一，副本池将多个存储
对象的使用CRUSH故障域的深层副本以物理方式分离一个数据对象
从另一个副本（即副本获得分配到单独的硬件）。这增加了耐用性
在硬件故障。其次，擦除编码池存储每个对象KM块，其中，
K代表数据块，M表示编码块。总和表示的数
的OSD用于存储对象​​和M个值表示的OSD可以失败的数目
仍然应该恢复的的OSD的M号失败的数据。

从客户的角度来看，Ceph的是优雅而简洁。客户端简单的读取和写入
池。然而，池在维护数据耐久性，性能和高可用性起到了重要作用；

认证：
为了识别用户，并防止人在这方面的中间人攻击，Ceph的提供其cephx
认证系统进行身份验证的用户和守护进程；

cephx协议并没有解决数据加密传输（例如，SSL / TLS）或
在其他形式的加密

Cephx使用共享秘密密钥进行认证，这意味着在客户端和监视器群集
有客户端的秘密密钥的副本。认证协议是这样的，双方都能够
证明给对方，他们有钥匙的副本，而无需实际透露出它。这提供了相互
认证，这意味着该集群是确保用户拥有的秘密密钥，并且用户
确保集群拥有密钥的副本。


PLACEMENT GROUPS (PGS);归置组
在整个集群中，ceph 将池通过伪随机的均匀分布到每个pgs。
（即每个池都被分成一块一块的；）
crush算法将每个对象指定到一个安置组，并且赋予每
安置组到OSD set，（在ceph client与存储了对象副本的osd之间创建的间接层）
如果ceph client知道ceph osd中存在的对象，在将在osd与client之间存在紧耦合；
反之，crush算法动态的分配每个对象到一个归置组，再将归置组分配到一组osd；
当新的osd加入或集群中的osd失效时候，这个间接层使得ceph存储集群开始动态重平衡；
通过管理在成百上千个归置组上下文中的数以万计的对象，ceph集群可以增长、缩小、
从失败中有效地恢复；


一个pool中只有很少的pg，这个“少”是相对于整个集群的大小而言的，ceph将很多数据
存放在一个pg里面，这样使得ceph性能不是那么良好；而当一个pool中有太多的pg时；
这个“多”也是想对于整个集群而言的，ceph osd将消耗太多的内存和cpu资源导致ceph
整体性能不好；因此设置在每个pool中设置一个合适的pg值以及在osd中限制pg值的上限，
对ceph的整体性能是十分关键的；

crush算法：
ceph为每个pool都分配了一个crush 规则组；当客户端在pool中存储或检索数据时；
ceph在一个规则下指定crush规则组、规则、以及顶层bucket来存储或检索数据；

由于ceph执行crush规则，ceph标识包含pg主osd成一个对象；
这就使得client直接连接osd并开始读写数据；

为了映射pgs到osds上，一个crush map定义了bucket type的分级列表；
（这些类型是在crush map中定义的产生的），
创建这些bucket层级结构的目的是；将故障域从性能域中分离，例如驱动型偏析的叶节点，
主机，机箱，机柜，配电单元，豆荚，行，室和数据中心。

除了叶节点代表的OSD，层次结构的其余部分是任意的，并且
您可以根据自己的需要确定，如果默认类型不适合您的要求。
CRUSH支持有向无环图，模拟的Ceph的OSD节点，通常在一
层次。所以，你可以支持多个层次结构，在一个CRUSH映射多个根节点。
例如，您可以创建固态硬盘层次的缓存层，与带有SSD日志的硬盘驱动器的层次结构等；

I/O操作；
CEPH客户端从CEPHmonitor检索crushmap，以及执行I/O在pool中的对象。
pool crush 规则组和pgs的数量是决定
CEPH将如何放置数据的主要因素。最新版本的cluster map中，客户端知道所有的
monitor以及集群中所有 的osds。然而，客户端不知道对象的位置。

客户端唯一需要的输入是对象ID和池名称。这很简单：CEPH存储
在某名的pool中存储数据（例如，“利物浦”）。当客户想要存储命名对象（例如，“约翰”
“保罗”，“乔治”，“林檎”等）中在一个pool中，利用对象名、散列码，该pool中的PG值和
池名称作为输入 通过crush计算为pg得到pg组ID以及主osd；

Ceph clients use the following steps to compute PG IDs.
1、1. The client inputs the pool ID and the object ID. (e.g., pool = "liverpool" and object-id = "john")

2、CRUSH takes the object ID and hashes it.

3、CRUSH calculates the hash modulo of the number of PGs. (e.g., 58) to get a PG ID

4、CRUSH calculates the primary OSD corresponding（对应的） to the PG ID.

5、The client gets the pool ID given the pool name (e.g., "liverpool" = 4)

6、The client prepends（预先设置） the pool ID to the PG ID (e.g., 4. 58).

7、The client performs an object operation (e.g., write, read, delete, etc.) by communicating
directly with the Primary OSD in the Acting Set.（活跃组）

ceph pg  dump  //获取ceph集群的所有活跃组；
ceph pg map  0.3f   //查看活跃组

rados -p data ls (这里会列举pool中的对象名称)
//通过object name查看活跃组：
//ceph osd map {pool-name} {object-name}

ceph osd map data test.txt

apt-get install libzero-ice34-dev
apt-get install libmysqlclient-dev

old_storage_type;   
storage_cluster;

dest_storage_cluster
dest_storage_type


叶子节点：没有子节点的节点；

Twisted学习：
多线程异步：
在异步编程模型与多线程模型之间还有一个不同：在多线程程序中，对于停止某个线程启动另外一个线程，其决定权并不在程序员手里而在操作系统那里，因此，程序员在编写程序过程中必须要假设在任何时候一个线程都有可能被停止而启动另外一个线程。相反，在异步模型中，一个任务要想运行必须显式放弃当前运行的任务的控制权。这也是相比多线程模型来说，最简洁的地方。
值得注意的是：将异步编程模型与同步模型混合在同一个系统中是可以的。但在介绍中的绝大多数时候，我们只研究在单个线程中的异步编程模型。

为什么要阻塞一个任务呢？最直接的原因就是等待I/O的完成：传输数据或来自某个外部设备。一个典型的CPU处理数据的能力是硬盘或网络的几个数量级的倍数。因此，一个需要进行大I/O操作的同步程序需要花费大量的时间等待硬盘或网络将数据准备好。正是由于这个原因，同步程序也被称作为阻塞程序。

mstsc /v:ip /admin

mstsc /v:210.14.78.113:9530  /admin

VIR_MIGRATE_LIVE:Do not pause the VM during migration
VIR_MIGRATE_PEER2PEER：Direct connection between source & destionation hosts
VIR_MIGRATE_TUNNELLED:Tunnel migration data over the libvirt PRC channel


ceph osd reweight OSDID NEW-WEIGHT


基础篇
引言
第一章：ceph的前世今生
1.1 ceph的诞生
1.2 ceph的市场前景
第二章：ceph的根基-RADOS
2.1 ceph与分布式文件系统
2.2 RADOS的组成
2.2.1 MON简介
2.2.1 OSD简介
2.2.1 MDS简介
2.3 快速搭建RADOS环境
2.4 LIBRADOS介绍
2.5LIBRADOS的C语言demo
2.6LIBRADOS的PYTHON语言demo
2.7LIBRADOS的JAVA语言demo
第三章：ceph的灵魂-CRUSH
3.1 CRUSH解决了什么问题
3.2 CRUSH基本原理
3.2.1 object->PG原理
3.2.1 PG->OSD原理
3.2.1 PG与POOL的关系
3.3 CRUSH原理验证（新建pool，上传object，搞清楚RADOS里面，object与pool、PG、OSD的映射关系）

第四章：ceph的图形化管理
4.1 Calamari介绍
4.2 Calamari快速安装
4.2 Calamari基本操作
第五章：ceph的性能与测试
5.1 需求模型与设计
5.2 硬件选型
5.3 性能调优
5.3.1 硬件层面
5.3.2 操作系统
5.3.3 网络配置
5.3.4 ceph配置
5.4 ceph的测试
5.4.1 cephfs的测试(iozone)
5.4.2 rbd的测试(fio)
5.4.3 RGW的测试(cosbench)
5.4.4 RADOS的测试(rados-bench)
第六章：ceph的运维与排错
6.1 ceph日常运维经验分享
6.2 ceph常见错误分享
6.1 cephfs应用案例
6.2 rbd应用案例
6.3 rgw应用案例


中级篇
第七章：ceph的三种存储形式
7.1 CEPHFS文件存储介绍
7.1.1 MDS介绍
7.1.2 快速搭建CEPHFS环境
7.1.3 CEPHFS的应用场景
7.2 RBD块存储介绍
7.2.1 RBD介绍
1)LIBRBD介绍
2)KRBD介绍
7.2.2 常见RBD CLI操作demo
7.2.3 RBD的应用场景
7.3 RADOS GATEWAY对象存储介绍
7.3.1 RGW介绍
7.3.2 快速搭建RGW环境
1)RGW与S3和swift接口兼容情况
2)S3 GUI调用demo
3)S3 CLI调用demo
4)S3 python调用demo
7.3.3 RGW的应用场景
第八章：cephfs在大数据中的应用例：RBD在虚拟化中的应用
8.1 用CEPHFS替代HDFS
8.1 ceph与KVM的整合
8.2 ceph与XEN的整合
8.3 ceph与openstack的整合
8.4 ceph与cloudstack的整合
8.5 基于rbd的iscsi搭建
第十章：RGW在互联网中的应用
9.1 网盘方案：RGW与owncloud的整合
9.2 备份方案：ceph与zmanda的整合
9.3 RGW的异地同步方案
9.4 RGW的多媒体转换网关设计


高级篇
第十章：CRUSH MAP的设计
10.0 一致性hash算法以及crush算法深入
10.1 CRUSH MAP基本组成
10.2 高可靠CRUSH MAP设计实例讲解
10.3 SSD与SATA混合下的实例讲解
10.3.1 SATA和SSD zone的划分
10.3.2 主OSD副本在SSD，其他在SATA的策略实现
第十一章：CACHE POOL和EC
11.1 CACHE POOL原理与应用场景
11.2 CACHE POOL搭建
11.3 EC原理与应用场景
11.4 EC搭建
第十二章：ceph-deploy的二次开发
12.1 ceph-deploy的构架介绍
12.2 扩展ceph-deploy模块实现对iscsi服务的安装
第十三章：calamari的二次开发
13.1 calamari的构架介绍
13.2 saltstack模块开发
13.2 django-resful接口的封装


<disk type='network' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <auth username='libvirt'>
        <secret type='ceph' uuid='cb936eba-e328-4fce-932e-7117a0dbbdbf'/>
      </auth>
      <source protocol='rbd' name='kvm/3439hda.img'>
        <host name='192.168.8.18' port='6789'/>
      </source>
      <backingStore/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>



宽惠迁移：

210.14.78.77
210.14.78.76     //迁移机器
210.14.78.75     //迁移机器
210.14.78.74
这四台机器从114.80.119.160中上去；
连接113：210.14.78.113:9530，密码：khadmin@20130618
存储节点：192.168.99.115;    密码：khadmin
操作流程：

1、copy  XXX.vmx,  xxx-flat.vmdk  到我们的节点;
2、mv   xxx-flat.vmdk  xxx.vmdk
3、vmware2libvirt -f XXXX.vmx > libvirt.xml
4、qemu-img convert -f raw -O  qcow2 xxx.vmdk  xxx.img

/vmfs/volumes/VMstore1/iso/SQL2005_dvd_X86_X64.iso
/vmfs/volumes/VM2store-1/wjw-win03_32-0108-78-188/wjw-win03_32-0108-78-188-flat.vmdk

ceph-deploy disk zap osd0:/dev/sdb
ceph-deploy osd create osd0:/dev/sdb


ceph-deploy --overwrite-conf config push mon2 mon3

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/data/kvm/51817hda.img'/>
      <target dev='hdb' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='1'/>
    </disk>



package one
type ReadWrite interface{
	Read(buf [] byte) (n int, err  error)
	Write(buf [] byte) (n int, err error)
}

package two
type IStream interface{
	Write(buf [] byte) (n int,  err error)
	Read(buf []  byte) (n int,  err error)
}

go语言最主要的特性：
1、自动垃圾回收
2、更丰富的内置类型
3、函数多返回值
4、错误处理
5、匿名函数和闭包
6、类型和接口
7、并发编程
8、反射
9、语言交互性

内存泄露的最佳解决方案是在语言级别引入自动垃圾回收算法（ Garbage
Collection，简称GC）。所谓垃圾回收，即所有的内存分配动作都会被在运行时记录，同时任何对
该内存的使用也都会被记录，然后垃圾回收器会对所有已经分配的内存进行跟踪监测，一旦发现
有些内存已经不再被任何人使用，就阶段性地回收这些没人用的内存。当然，因为需要尽量最小
化垃圾回收的性能损耗，以及降低对正常程序执行过程的影响，现实中的垃圾回收算法要比这个
复杂得多，比如为对象增加年龄属性等，但基本原理都是如此。

特殊的数据类型：
1、字典类型（map）
2、数据切片：一种可动态增长的数组；类似c++中的vector；

多返回值：
func getName()(firstName, middleName, lastName, nickName string){
	return "may", "M", "chen", "Babe"
}

并不是每一个返回值都必须赋值，没有被明确赋值的返回值将默认的空值。
如果开发者只对该函数其中的某几个返回值感兴趣的话，也可以直接用下划线作为占位符来
忽略其他不关心的返回值。下面的调用表示调用者只希望接收lastName的值，这样可以避免声
明完全没用的变量：
_, _, lastName, _ := getName()


错误处理：
关键字：defer、panic、和recover

匿名函数和闭包：
在Go语言中，所有的函数也是值类型，可以作为参数传递。 Go语言支持常规的匿名函数和
闭包，比如下列代码就定义了一个名为f的匿名函数，开发者可以随意对该匿名函数变量进行传
递和调用：
f := func(x, y int) int {
	return x+y
}


c++中的变量相与：
if(info.flag&DATA_HOST)


负载均衡：
	1、以集群为单位，即同一个集群内节点的负载是均衡的；
	2、负载的指标：
		a、以cpu利用率为指标；
		b、以内存使用比例为指标；
		c、以load  average为指标；
	3、触发重平衡条件：
		 a、各指标触发值；
		 b、触发时间；

ceph：
	journal的内容，在内存里面，在一定周期，提交到filestore
	副本池就是数据有多个复制版本，纠删池就是数据被采用纠删编码的方式分块存放
	按照min_size，写入journal后，就返回client
	例如，一般情况是size 3, min_size 2，主需要写入两个osd的journal，就返回


自我管理操作：p15/24
ceph集群自动执行了大量自我监控和管理操作；例如:ceph osd检测集群的健康状态和
报告给ceph  monitor；通过crush算法将object发配到pgs以及pgs到一组osd；
ceph osd利用crush算法重平衡或者动态恢复故障域；
下面介绍一些ceph操作：
心跳机制：
	ceph osd加入集群并将自己的状态报告给ceph monitor；在最低水平下，
ceph osd的状态是up或者down反应了他们是否在运行以及是否能为ceph client
提供服务；如果一个ceph osd在集群中是down状态，则表明该osd是失效的
如果一个osd没有运行（比如它崩溃了）,ceph osd不能通知到ceph monitor自己是
down的状态；ceph monitor可以ping ceph 守护进程来确定它是否在运行；
然而，ceph 也授权给ceph osd，让ceph osd去确定他们相邻的osd是否down状态；
以及更新cluster map，并上报给ceph monitor。这 就意味这ceph monitor
可以保持轻量级进程；

对等：
Ceph的OSD守护程序执行对等，这是使所有存储的屏上显示的过程中
放置集团（PG）到有关的所有对象的状态协议（及其元数据）的
该PG。对等的问题，通常自行解决。

注意：	
当Ceph monitor认可存放了pgs的OSD状态，这不
并不意味着pgs总是拥有最新内容

当Ceph要将pg存放到osd的活跃组中时，它们被分为初级，次级，等等。
按照惯例，活跃组中的第一个osd被认为主osd，并且是
负责协调对等过程的每个pg在那里充当主，
并且是唯一的OSD，这将接受客户端发起的写入对象到一个给定的pg；

一组osd负责一个pg时，对于这组osd，我们称之为an Acting set（活跃组）；
一个活跃组指的是对pg负责的ceph osd守护进程；ceph osd守护进程
是那些不总是up状态的活跃组中的一部分；当在活跃组中的eosd是up状态时；

这些up状态的组是很有重要作用的，因为ceph可以重新remap pgs到其他fail
的osd；

例如：
在这些包含osd.25, osd.32， osd.61 的活跃组中，第一个osd是osd.25, 它是
主osd，如果这osd 失效了，次级osd(osd.32) 将变为主osd；然后osd.25将被移出up
状态的活跃组中；

重平衡和恢复：


.执行如下命令查看后端网卡所属ovs。
ovs-dpctl show
发现统计到该ovs上lost值为0，表明ovs没有丢包，继续查看连接在该ovs上的物理网卡聚合为bond28。

ovs-ofctl show br0

   查看虚拟交换机ovsbr0的信息：
$ovs-ofctl  show ovsbr0
2.    查看ovsbr0上各交换机端口的状态
$ovs-ofctl  dump-ports ovsbr0
3.    查看ovsbr0上的所有流规则
$ovs-ofctl  dump-flows ovsbr0
4.    丢弃从2号端口发来的所有数据包
$ovs-ofctl  add-flow ovsbr0
idle_timeout=0,in_port=2,actions=drop
注意：此处的in_port是指虚拟网卡(vif,tap)的号码，并非传输层的端口号(如www:80,ftp:21,22等)，通过ovs-ofctl  show ovsbr0可查得端口号，传输层的端口号有tp_src/tp_dst指定。
5.    删除条件字段中包含in_port=2的所有流规则
$ovs-ofctl  del-flows ovsbr0 in_port=2
6.    丢弃所有收到的数据包
$ovs-ofctl  add-flow ovsbr0
dl_type=*,nw_src=ANY,action=drop

网络虚拟化或者软件定义网络（Software Defined Network；  sdn
实现数据交换和OpenFlow流表功能，是OVS的核心


ovs：
在虚拟化平台下，ovs可以为动态变化的端点提供2层交换功能；很好的控制虚拟网络中的
访问策略、网络隔离、流量监控等等；
ovs提供了对OpenFlow协议的支持，用户可以使用任何支持OpenFlow协议的控制器对ovs进行
远程管理控制；在ovs中的非常重要的概念：
1、Bridge:Bridge代表一个从以太网交换机（Switch），一个主机中可以创建一个或者多个
Bridge设备；
2、Port：端口与物理交换机的端口概念类似，每个Port都隶属于一个 Bridge；
3、Interface:连接到Port的网络接口设备（在主机端），在通常情况下，Port和Interface是
一对一的关系,(相当于一根网线连接的两端)；只有在配置Port为bond模式后，Port和Interface
才是一对多的关系；注：bond模式即级联模式；
4、Controller： OpenFlow控制器，OVS可以同时接受一个或多个OpenFlow控制器的管理；
5、datapath：在ovs中，datapath负责执行数据交换，也就是把从接收端口收到的数据包在表中
进行匹配，并执行匹配到的动作；
6、Flow table：每个datapath都和一个“flow table”关联，当datapath接收到数据之后，
ovs会在flow table中查询可以匹配的flow，执行对应的操作，例如转发数据到另外的端口；

在OVS中，给一个交换机，或者说一个桥，用了一个专业的名词，叫做DataPath！

查看datapath的信息：
ovs-dpctl show
查看交换机中所有的table：
ovs-ofctl dump-tables br0
查看交换机中所有的流表项：
ovs-ofctl dump-flows  br0;
查看交换机上端口信息：
ovs-ofctl show br0;

除此之外还可以通过Floodlight 管理 OVS


ovs流规则管理
每条流规则由一系列字段组成，分为基本字段、条件字段和动作字段三部分。
基本字段包括:

    生效时间 duration_sec
    所属表项 table_id
    优先级 priority、
    处理的数据包数 n_packets
    空闲超时时间 idle_timeout 等空闲超时时间 idle_timeout 以秒为单位，超过设置的空闲超时时间后该流规则将被自动删除，空闲超时时间设置为 0 表示该流规则永不过期，idle_timeout 将不包含于 ovs-ofctl dump-flows brname 的输出中。

条件字段包括:

    输入端口号 in_port
    源目的 mac 地址 dl_src/dl_dst
    源目的 ip 地址 nw_src/nw_dst
    数据包类型 dl_type
    网络层协议类型 nw_proto

这些字段可以任意组合，但在网络分层结构中底层的字段未给出确定值时上层的字段不允许给确定值，即一 条流规则中允许底层协议字段指定为确定值，高层协议字段指定为通配符(不指定即为匹配任何值)，而不允许高层协议字段指定为确定值， 而底层协议字段却为通配符(不指定即为匹配任何值)，否则，ovs-vswitchd 中的流规则将全部丢失，网络无法连接。
动作字段包括正常转发 normal、定向到某交换机端口 output：port、丢弃 drop、更改源目 的 mac 地址 mod_dl_src/mod_dl_dst 等，一条流规则可有多个动作，动作执行按指定的先后顺序依次完成。



3.2 流规则：

每条流规则由一系列字段组成，分为基本字段、条件字段和动作字段三部分：

基本字段包括生效时间duration_sec、所属表项table_id、优先级priority、处理的数据包数n_packets，空闲超时时间idle_timeout等，空闲超时时间idle_timeout以秒为单位，超过设置的空闲超时时间后该流规则将被自动删除，空闲超时时间设置为0表示该流规则永不过期，idle_timeout将不包含于ovs-ofctl dump-flows brname的输出中。

条件字段包括输入端口号in_port、源目的mac地址dl_src/dl_dst、源目的ip地址nw_src/nw_dst、数据包类型dl_type、网络层协议类型nw_proto等，可以为这些字段的任意组合，但在网络分层结构中底层的字段未给出确定值时上层的字段不允许给确定值，即一条流规则中允许底层协议字段指定为确定值，高层协议字段指定为通配符(不指定即为匹配任何值)，而不允许高层协议字段指定为确定值，而底层协议字段却为通配符(不指定即为匹配任何值)，否则，ovs-vswitchd 中的流规则将全部丢失，网络无法连接。其中dl是datalink的缩写，nw是network的缩写，tp是transport的缩写。

动作字段包括正常转发normal、定向到某交换机端口output：port、丢弃drop、更改源目的mac地址mod_dl_src/mod_dl_dst等，一条流规则可有多个动作，动作执行按指定的先后顺序依次完成。

什么是Tap/Tun、网桥
在计算机网络中，TUN与TAP是操作系统内核中的虚拟网络设备。不同于普通靠硬件网路板卡实现的设备，这些虚拟的网络设备全部用软件实现，并向运行于操作系统上的软件提供与硬件的网络设备完全相同的功能。
TAP 等同于一个以太网设备，它操作第二层数据包如以太网数据帧。TUN模拟了网络层设备，操作第三层数据包比如IP数据封包。
操作系统通过TUN/TAP设备向绑定该设备的用户空间的程序发送数据，反之，用户空间的程序也可以像操作硬件网络设备那样，通过TUN/TAP设备发送数据。在后种情况下，TUN/TAP设备向操作系统的网络栈投递（或“注入”）数据包，从而模拟从外部接受数据的过程。
服务器如果拥有TUN/TAP模块，就可以开启VPN代理功能。
虚拟网卡TUN/TAP 驱动程序设计原理：

tc qdisc del dev eth0 root

也就是说每次只能同时使用localtime()函数一次，要不就会被重写！
因此localtime()不是可重入的。同时libc里提供了一个可重入版的函数localtime_r()；


//开启osd
service ceph-osd start id=0；
//关闭osd
service ceph-osd stop id=0；
//关闭一个节点上的多osd
service ceph-osd-all stop


vm_mon.rt_node表

grep的--exclude-dir=参数就是为了排除某个目录的，即不包含等号后面的目录，所以我们可以利用此参数去掉.svn的隐藏目录。
grep -nr  --exclude="tags"  --exclude-dir=".svn"  "VmOffNetwork"  * 
export GREP_OPTIONS="--exclude=tags --exclude-dir=\.svn"

#ifndef __LOAD_BALANCE_H__
  2 #define __LOAD_BALANCE_H__
  3 
  4 #include <IceUtil/Thread.h>
  5 #include <pthread.h>
  6 
  7 class LoadBalance: public IceUtil::Thread
  8 {
  9 
 10 };
 11 
 12 
 13 #endif
 14 


2、undefined reference to `vtable for ...'

    产生问题的原因是::

            基类中声明了virual 方法（不是纯虚方法），但是没有实现。在子类中实现了，当子类创建对象时，就出现这个问题。

     class   Base

     {

                 public:

                      virtual  int run();

     };

     class  Test:public Base              //必须实现run才可以

     {

                public:

                      Test()

                      {

                      }

     };

     在Test中必须实现run，否则Test不能创建对象，创建对象，编译时会报 undefined reference to `vtable for ...' 这种错误。

     如果是“纯虚函数”，即 virtual int run() = 0;编译时会报" because the following virtual functions are pure within 'Test' "

     如果在基类中实现了虚函数，或者在子类中实现纯虚函数或虚函数，就不会报错。

     包含有虚函数的类，是不能创建对象的。

解决 令人生厌的 multiple definition of

慢慢的自己写的代码 ，有很多了，自己总是加入一些新的东西，并一点点地完善着它，后来想编译一下，看看能否运行了，却出现了几十行的“multiple   definition   of ”

我把所有的全局变量写在一个global.h里，然后其他文件都include 了它 ，于是出现了 multiple   definition   of  .....

（编译器 gcc ) 

后来在网上搜到了很多类似的错误，大家各有各的烦心事。

我的代码结构

main.cpp
#include "global.h"

WinMain(....)
{
...
}

file_1.cpp
#include "global.h"
....

file_2.cpp
#include "global.h"

...

由于工程中的每个文件都是独立的解释的，
（即使头文件有
#ifndef _x_h 
....
#enfif   )

在其他文件中只要包含了global.h 就会独立的解释,然后生成每个文件生成独立的标示符。在编译器连接时，就会将工程中所有的符号整合在一起，由于，文件中有重名变量，于是就出现了重复定义的错误。


下面是解决方法：

在global.c（或.cpp)  中声明变量，然后建一个头文件global.h 在所有的变量声明前加上extern ...
如 extern HANDLE ghEvent;
注意这儿不要有变量的初始化语句。

然后在其他需要使用全局变量的 cpp文件中包含.h 文件而不要包含 .cpp 文件。编译器会为global.cpp 生成目标文件，然后连接时，在使用全局变量的文件中就会连接到此文件 。

select  a.type from `idc_route` a, `vhost_eth` b, `node_eth` c  where  a.id = c.idc_route_id and  b.node_eth_id = c.id  and b.id= VhostEthId;

企业培训(cp部分)：产品对比，部分文档资料，API的demo演示、ceph架构
产品对比：ceph与gluster
		对比项：架构方法、数据分布能力、IO测试、扩展性能、cache、抗逆能力、安装配置和维护、故障处理、
		gluster中遇到的问题：gluster 3.6.1莫名的cpu占用超高，最后卡机；
API的demo演示：
	1、块的新建、读、写、删除等基本操作；
	2、获取块信息、克隆、快照信息、快照创建、回滚等；
	3、异地灾备的原理说明；
ceph架构：
	1、ceph系统的层次结构及其pg状态名称解释：
	2、CEPH的工作原理及流程
	3、浅析RADOS组件以及分发策略–CRUSH算法并对比一致性hash算法
	4、FileJournal的意义；


今天任务：
	1、负载均衡，备注：vmc端收集同集群节点由负载、带宽情况以及同一节点上虚拟机的负载、带宽信息收集；





	一、归置组状态
1. Creating

创建存储池时,它会创建指定数量的归置组。ceph 在创建一或多个归置组时会显示 creating;创建完后,在其归置组的 Acting Set 里的 OSD 将建立互联;一旦互联完成,归置组状态应该变为 active+clean,意思是ceph 客户端可以向归置组写入数据了。

2. peering

ceph 为归置组建立互联时,会让存储归置组副本的 OSD 之间就其中的对象和元数据状态达成一致。ceph 完成了互联,也就意味着存储着归置组的 OSD 就其当前状态达成了一致。然而,互联过程的完成并不能表明各副本都有了数据的最新版本。

3. active

ceph 完成互联进程后,一归置组就可变为 active。active 状态通常意味着在主归置组和副本中的数据都可以读写。

4. clean

某一归置组处于 clean 状态时,主 OSD 和副本 OSD 已成功互联,并且没有偏离的归置组。ceph 已把归置组中的对象复制了规定次数。

5. degraded

当客户端向主 OSD 写入数据时,由主 OSD 负责把副本写入其余复制 OSD。主 OSD 把对象写入复制 OSD 后,在没收到成功完成的确认前,主 OSD 会一直停留在 degraded 状态。
归置组状态可以是 active+degraded 状态,原因在于一 OSD 即使没所有对象也可以处于 active 状态。如果一OSD 挂了,ceph 会把相关的归置组都标记为 degraded;那个 OSD 重生后,它们必须重新互联。然而,如果归置组仍处于 active 状态,即便它处于 degraded 状态,客户端还可以向其写入新对象。
如果一 OSD 挂了,且 degraded 状态持续,ceph 会把 down 的 OSD 标记为在集群外(out)、并把那些 down 掉的 OSD 上的数据重映射到其它 OSD。从标记为 down 到 out 的时间间隔由 mon osd down out interval 控制,默认是 300 秒。
归置组也会被降级(degraded),因为归置组找不到本应存在于归置组中的一或多个对象,这时,你不能读或写找不到的对象,但仍能访问其它位于降级归置组中的对象。

6. recovering

ceph 被设计为可容错,可抵御一定规模的软、硬件问题。当某 OSD 挂了(down)时,其内容版本会落后于归置组内的其它副本;它重生(up)时,归置组内容必须更新,以反映当前状态;在此期间,OSD 在recovering 状态。
恢复并非总是这些小事,因为一次硬件失败可能牵连多个 OSD。比如一个机柜的网络交换机失败了,这会导致多个主机落后于集群的当前状态,问题解决后每一个 OSD 都必须恢复。
ceph 提供了很多选项来均衡资源竞争,如新服务请求、恢复数据对象和恢复归置组到当前状态。osd recovery delay start 选项允许一 OSD 在开始恢复进程前,先重启、重建互联、甚至处理一些重放请求;osd recovery threads 选项限制恢复进程的线程数,默认为 1 线程;osd recovery thread timeout 设置线程超时,因为多个OSD 可能交替失败、重启和重建互联;osd recovery max active 选项限制一 OSD 最多同时接受多少请求,以防它压力过大而不能正常服务;osd recovery max chunk 选项限制恢复数据块尺寸,以防网络拥塞。

7. back filling

有新 OSD 加入集群时,CRUSH 会把现有集群内的归置组重分配给它。强制新 OSD 立即接受重分配的归置组会使之过载,用归置组回填可使这个过程在后台开始。回填完成后,新 OSD 准备好时就可以对外服务了。

8. remapped

某一归置组的 Acting Set 变更时,数据要从旧集合迁移到新的。主 OSD 要花费一些时间才能提供服务,所以它可以让老的主 OSD 持续服务、直到归置组迁移完。数据迁移完后,主 OSD 会映射到新 acting set。

9. stale

虽然 ceph 用心跳来保证主机和守护进程在运行,但是 ceph-osd 仍有可能进入 stuck 状态,它们没有按时报告其状态(如网络瞬断)。默认,OSD 守护进程每半秒(0.5)会一次报告其归置组、出流量、引导和失败统计
状态,此频率高于心跳阀值。如果一归置组的主 OSD 所在的 acting set 没能向监视器报告、或者其它监视器已经报告了那个主 OSD 已 down,监视器们就会把此归置组标记为 stale。启动集群时,会经常看到 stale 状态,直到互联完成。集群运行一阵后,如果还能看到有归置组位于 stale 状态,就说明那些归置组的主 OSD 挂了(down)、或没在向监视器报告统计信息。

二、找出故障归置组

一般来说,归置组卡住时 ceph 的自修复功能往往无能为力,卡住的状态细分为:

1. unclean

不干净:归置组里有些对象的复制数未达到期望次数,它们应该在恢复中。

2. inactive

不活跃:归置组不能处理读写,因为它们在等着一个持有最新数据的 OSD 再次进入 up 状态。

3. stale

发蔫:归置组们处于一种未知状态,因为存储它们的 OSD 有一阵子没向监视器报告了(由 mon osdreport timeout 配置)。

为找出卡住的归置组,执行:

?
1
ceph pg dump_stuck [unclean|inactive|stale]

获取pg分布：
ceph pg dump | awk ' /^pg_stat/ { col=1; while($col!="up") {col++}; col++ } /^[0-9a-f]+\.[0-9a-f]+/ { match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0; up=$col; i=0; RSTART=0; RLENGTH=0; while(match(up,/[0-9]+/)>0) { osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) } for(i in osds) {array[osds[i],pool]++; osdlist[osds[i]];} } END { printf("\n"); printf("pool :\t"); for (i in poollist) printf("%s\t",i); printf("| SUM \n"); for (i in poollist) printf("--------"); printf("----------------\n"); for (i in osdlist) { printf("osd.%i\t", i); sum=0; for (j in poollist) { printf("%i\t", array[i,j]); sum+=array[i,j]; poollist[j]+=array[i,j] }; printf("| %i\n",sum) } for (i in poollist) printf("--------"); printf("----------------\n"); printf("SUM :\t"); for (i in poollist) printf("%s\t",poollist[i]); printf("|\n"); }'

insert(std::map<int, NodeInfo>::value_type(1, tmp));


ceph中的4个map：osdmap、mdsmap、monmap、pg map、还有auth信息保存在mon端；


select vhost_eth.id, node_eth.idc_route_id 


"SELECT i.`id`, i.`name`, i.`pub_ip`, ii.`id` AS routeid, ii.`name` AS routename 
		FROM `node` n 
		LEFT JOIN `node_eth` i ON i.`node_id`=n.`id` 
		LEFT JOIN `idc_route` ii ON ii.`id`=i.`idc_route_id` 
		WHERE n.`control_ip`='$nodeip'";

//选择目标节点的线路
 id 	name 	pub_ip 	routeid 	routename
56 	eth0 	192.168.2.30 	1 	本地公网线路
57 	eth1 	192.168.8.30 	9 	本地内网线路

cluster_id=目标节点所在集群; route_id=目标线路ID user_id=当前用户


//获取指定节点上所以网卡的idc_route_id;
select b.id,  b.idc_route_id from  `node` a,  `node_eth` b where  b.node_id = a.id and  b.statue=1  and a.control_ip = "192.168.2.30"

select a.id,  from `vhost_eth` a, `vlan` b， `vhost` c where  c.id = a.vhost_id and a.vlan_id = b.id and b.idc_route_id =2 and c.uuid="a90bd1d2-da13-bb24-5192-b77931cb2700" 


SELECT i.`id` , i.`name` , i.`pub_ip` , ii.`id` AS routeid, ii.`name` AS routename
FROM `node` n
LEFT JOIN `node_eth` i ON i.`node_id` = n.`id`
LEFT JOIN `idc_route` ii ON ii.`id` = i.`idc_route_id`
WHERE n.`control_ip` = '192.168.2.30'

等待线程返回 int pthread_join(pthread_t thread, void **value_ptr); value_ptr存放线程返回值，线程返回值和线程函数返回值类型一样，也是void*


SELECT i.`id` , ii.`id` FROM `node` n  LEFT JOIN `node_eth` i ON i.`node_id` = n.`id` LEFT JOIN `idc_route` ii ON ii.`id` = i.`idc_route_id`
WHERE n.`control_ip` = '192.168.2.30'


SELECT a.id FROM `vhost_eth` a, `vlan` b, `vhost` c WHERE c.id = a.vhost_id AND a.vlan_id = b.id AND b.idc_route_id =1 AND c.uuid = "a90bd1d2-da13-bb24-5192-b77931cb2700"

node数据库密码：openstack

keystone tenant-create --name admin
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |                                  |
|   enabled   |               True               |
|      id     | 1f14a11f61be4520a7f8ef86670fbaa5 |
|     name    |              admin               |
|  parent_id  |                                  |
+-------------+----------------------------------+

root@node1:~# keystone tenant-create --name service
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |                                  |
|   enabled   |               True               |
|      id     | 0acfd48af0a64639a0ff679c5e1b850b |
|     name    |             service              |
|  parent_id  |                                  |
+-------------+----------------------------------+

openstack：
1、控制节点安装所有，计算节点只有nova-compute；
2、网络选择：
	nova-network还是neutron；
	nova-network比较简单，
	neutron功能强大


openstack所有数据库密码：openstack
rabbit guest密码： openstack
keystone ADMIN_TOKEN:openstack
keystone  admin user 密码：openstack
keystone  demo  user 密码：openstack
glance  密码：openstack


	
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
  IDENTIFIED BY 'openstack';
(crontab -l -u keystone 2>&1 | grep -q token_flush) || \
  echo '10 /usr/bin/keystone-manage token_flush >/var/log/keystone/keystone-tokenflush.log 2>&1' \
  >> /var/spool/cron/crontabs/keystone

keystone endpoint-create   --service-id  2503a30d9928474188f65e7707a0d6f9  --publicurl http://node1:5000/v2.0   --internalurl http://node1:5000/v2.0  --adminurl http://node1:35357/v2.0  --region regionOne




 






1.0.6版本：
192.74.235.97  //ok

122.136.32.5    //ok
198.2.192.193   //ok

211.101.15.196  //ok
59.188.252.102  //ok
59.188.252.101   //ｏｋ
192.74.245.161   //ok
198.200.46.193	 //ok
220.167.104.169		//ok
122.112.81.2    //ok
122.112.81.3  //ok

1.2.12版本
211.101.15.195
114.80.119.121 //

apt-get update

./start && tailf vmd.log
md5sum vmd
7684e1b37c7050dee0bf9e823ba8ed59  vmd

apt-get install libboost-thread-dev libboost1.46-dev 

 apt-get install  libcrypto++-dev
rm  /usr/lib/libgfapi.so.0 && ln -s  /usr/lib/libgfapi.so.0.0.0  /usr/lib/libgfapi.so.0


tar -C / -zxf   glusterfs-3.6.2.tgz  && rm  /usr/lib/libgfapi.so.0 && ln -s  /usr/lib/libgfapi.so.0.0.0  /usr/lib/libgfapi.so.0 && tar -C / -zxf curl.tgz

virsh list --all


CB1788E64B609BA74902优惠券

1、win2003R2-32Bit纯净版-----------偶现问题；
2、切换模板无法进入系统-----------主机xml被删除了系统盘驱动配置，

        if(virDomainUndefineFlags(dom,VIR_DOMAIN_UNDEFINE_SNAPSHOTS_METADATA)){
            if(virDomainUndefine(dom)){
                rt_con.errNum = -2;  
                rt_con.errMsg = "Failed to undefine domain:" + UUID;
                pErr = virConnGetLastError(conn);
                if(pErr){
                    rt_con.errMsg += pErr->message;
                }    
                virDomainFree(dom);
                virConnectClose(conn);  
                post_err(args_str("Failed to undefine domain:%s", UUID.c_str()), 20, 2, 1);
                cdn_log(LOG_VIRT).error(rt_con.errMsg);   
                return rt_con; 
            }    
        } 

      
valgrind --tool=memcheck --leak-check=full --show-reachable=yes  --track-origins=yes ./demo

librados用于在客户端用来访问 rados对象存储设备
ceph rados对象映射：
ceph osd map kvm  hw
api文档：
http://ceph.com/docs/master/rados/api/librados/

对象存储
Ceph文件系统中的数据和元数据都保存在对象中。 对于对象存储，通常的定义是：一个Object，由三部分组成（id，metadata，data），id是对象的标识，这个不必多说。所谓的metadata，就是key/value的键值存储，至于用来保存什么信息，由文件系统的语义定义。data就是实际存储的数据。

Ceph的对象，包括四个部分（id，metadata，attribute，data），在Ceph里，一个Object，实际就对应本地文件系统的一个文件，一个对象的attribute，也是key/value的键值对，其保存在本地文件系统的文件的扩展属性中。对象的metadata就是key/value的键值对，目前Ceph保存在google开源的一个key/value存储系统leveldb中，或者自己写的一个key/value 存储系统中。数据就保存在对象的文件中。对于一个对象的更新，都需要写日志中来保持一个Object数据的一致性（consistence），日志有一个单独的设备或者文件来保存。

文件扩展属性：lsattr 查看文件的扩展属性,
扩展属性（xattrs）提供了一个机制用来将《键/值》对永久地关联到文件，让现有的文件系统得以支持在原始设计中未提供的功能；

Ceph最大的特点是分布式的元数据服务器  通过CRUSH，一种拟算法来分配文件的locaiton，其核心是 RADOS（resilient automatic distributed object storage)，一个对象集群存储，本身提供对象的高可用，错误检测和修复功能。
其设计思想有一些创新点：

第一，数据的定位是通过CRUSH算法来实现的。

传统的，或者通常的并行文件系统，数据的定位的信息是保存在文件的metadata 中的， 也就是inode结构中，通过到metadata server上去获取数据分布的信息。而在Ceph中，是通过CRUSH 这个算法来提供数据定位的。

这和GlusterFS的思想是相同的，GlusterFS 是通过Elastic Hash，类似于DHT的算法实现的。这就有点像P2P存储，所谓的完全对称的存储，这种设计架构最大的优点是，其理论上可以做到 线性扩展的能力（line scale）。

在GlusterFS架构中，是完全去掉了metadata server，这就导致GlusterFS文件系统上的元数据操作，例如ls， stat操作非常慢，要去各个stripe的节点上收集相关的元数据信息后聚合后的结果。在Ceph中，为了消除完全的p2p设计，提供了metadata server 服务，提供文件级别的元数据服务，而元数据服务中的文件数据定位由CRUSH算法代替。

当有OSD失效，恢复或者增加一个新的OSD时，导致OSD cluster map的变换。Ceph处理以上三种情况的策略是一致的。为了恢复，ceph保存了两类数据，一个是每个OSD的一个version，另一个是PG修改的log，这个log包括PG修改的object 的名称和version。

当一个OSD接收到cluster map的更新时：

1）检查该OSD的所属的PG，对每个PG，通过CRUSH算法，计算出主副本的三个OSD

2）如何该PG里的OSD发生了改变，这时候，所有的replicate向主副本发送log，也就是每个对象最后的version，当primay 决定了最后各个对象的正确的状态，并同步到所有副本上。

3）每个OSD独立的决定，是从其它副本中恢复丢失或者过时的（missing or outdated）对象。 (如何恢复? 好像是整个对象全部拷贝，或者基于整个对象拷贝，但是用了一些类似于rsync的算法？目前还不清楚）

4）当OSD在恢复过程中，delay所有的请求，直到恢复成功。

Paxos算法选择出leader。

1.客户端输入池ID和对象ID。（例如，池=“liverpool”和对象ID =“john”） 
2.CRUSH取得的对象ID和散列它。
3.CRUSH计算散列模数的OSD。（例如，0x58）得到一个PG的ID。
4.CRUSH得到池ID池的名称（如“liverpool”= 4）
5.CRUSH预先考虑到PG ID对应池ID（例如，4.0x58）。


 关于池

Ceph的存储系统支持'池'，这是用于存储对象的逻辑分区的概念。池设置以下参数：

所有权/访问对象
对象副本的数目
放置组的数目
CRUSH规则集的使用
 
<ceph_flatten_sh>/root/vm/vmd/ceph_flatten.sh</ceph_flatten_sh>

快照的作用主要是能够进行在线数据备份与恢复。当存储设备发生应用故障或者文件损坏时可以进行快速的数据恢复，将数据恢复某个可用的时间点的状态。快照的另一个作用是为存储用户提供了另外一个数据访问通道，当原数据进行在线应用处理时，用户可以访问快照数据，还可以利用快照进行测试等工作。所有存储系统，不论高中低端，只要应用于在线系统，那么快照就成为一个不可或缺的功能

synchronously  英 ['sɪŋkrənəslɪ] 


2880709218  企业QQ密码：cXp123456



python的包管理工具：Easy Install 和IPython
django安装：
1、apt-get install setuptools
2、easy_install django
源码安装：
	python setup.py install

安装web server
功能:
	把动态生成的HTML页面返回给浏览器，还负责处理图片和css静态的内容，以及各种系统级别的事情
	（负载均衡，代理等）
1、python内置的runserver服务器，也称“dev”，只能用于测试；
2、推荐配置：apache+mod_python

a.mkdir blog && cd blog;
b.django-admin.py  startproject my_project_name
c.cd my_project_name; python manage.py runserver

d.有了项目之后，可以在它下面创建应用；
	python manage.py  my_app_name

e.在my_project_name目录下的my_project_name/setting中；
	INSTALLED_APPS元组后加上 'my_project_name.my_app_name', (逗号不能少)

f.my app的核心部分my_project_name/my_app_name/model.py文件：
	定义myapp数据结构的地方；





/websockify.py 9111 192.168.2.18:6001  –target-config=/var/lib/one/sunstone_vnc_tokens

novnc使用说明：
1、运行一个轻量级代理，运行在本地30机器上
	nohup /root/noVNC/utils/websockify/run --web=. --target-config=/root/noVNC/vnc_token/vm_01  6080 &
2、修改/root/noVNC/vnc_token/vm_01文件，
格式如下；
	uuid: 节点:vnc端口号；

3、修改/root/noVNC/utils/auto_vnc.html
 修改如下：
	 path = WebUtil.getQueryVar('path', 'websockify/?token=目标机器uuid');

4、浏览器访问http://192.168.2.30:6080/vnc_auto.html
	
每次访问不同主机，需要更新/root/noVNC/vnc_token/vm_01文件如步骤2,以及/root/noVNC/utils/auto_vnc.html，如步骤3

测试url：http://192.168.2.30:6080/vnc_auto.html
密码：a7b6490b598951f7

rbd导出快照：
rbd export   rbd/test@snap01 




RAID0速度是最快的，因为数据是分开存放在每个组成阵列的硬盘，所以一旦其中一块硬盘有问题就会导致所有数据损坏。优点：速度快、成本低 缺点数据容易丢失，一旦损坏无法恢复。
RAID1的原理是有两块硬盘组成的阵列，其中一块拿来正常使用，另外一块是专门备份存放的，相当于你两块硬盘只能用一块硬盘，另外那块是保存这块硬盘里面的数据，
这样的话即使你有一块硬盘坏了数据也不会丢失，但速度慢，而且两块硬盘只能用一块硬盘的容量。

RAID5：分布式奇偶校验的独立磁盘结构


http://blog.csdn.net/kevin_darkelf/article/details/40980333

python获取url地址中的参数
>>> url="http://localhost/test.py?a=hello&b=world "
>>> result=urlparse.urlparse(url)
>>> result
ParseResult(scheme='http', netloc='localhost', path='/test.py', params='', query='a=hello&b=world ', fragment='')
>>> urlparse.parse_qs(result.query,True)
{'a': ['hello'], 'b': ['world ']}
>>> params=urlparse.parse_qs(result.query,True)
>>> params
{'a': ['hello'], 'b': ['world ']}
>>> params['a'],params['b']
(['hello'], ['world '])


用urlparse模块就可以解析出来了



智慧vpn：255.75

websockify/token_plugins.py L33

novnc工作流程：
vnc.html<------->websockets<------->noVNC-Proxy(websockify.py)<------>vnc server


http://114.80.119.146/ftpdir/vm/MergeIDE.zip

中级篇：
1、从硬件入手搭建集群：
	1.1、分析常见场景；
	1.2、分别为对应的场景方案配置硬件；
	1.3、ceph集群调优
2、管理ceph集群以及常见错误处理

3、ceph架构和组成
	3.1、ceph osd 文件系统
	3.2、ceph osd journal
	3.3、ceph  osd 常见命令分析
	3.4、ceph mon 常用命令分析
	3.4、ceph rbd块存储命令分析
4、crush map介绍
	1、crush lookup
	2、crush hierarchy
	3、恢复和重平衡
	4、设计crush map
	5、customizing a cluster layout
	6、修改pg和pgp
	7、ceph数据管理

5、块存储与虚拟化
	ceph与openstack




第二章：存储基石 - RADOS
程鹏

第三章：智能分布 - CRUSH
沈志伟、程鹏


第二章：存储基石 - RADOS
2.1 Ceph的分布式本质
2.2 RADOS组成
   2.2.1 MON简介
   2.2.2 OSD简介
2.3 快速搭建RADOS环境
2.4 LIBRADOS介绍
   2.4.1 LIBRADOS的C语言demo
   2.4.2 LIBRADOS的PYTHON语言demo
   2.4.3 LIBRADOS的JAVA语言demo

第三章：智能分布 - CRUSH
3.1 CRUSH的本质
3.2 CRUSH基本原理
   3.2.1 Object与PG
   3.2.2 PG与OSD
   3.2.3 PG与POOL
3.3 CRUSH关系分析
（新建pool，上传object，搞清楚RADOS里面，object与pool、PG、OSD的映射关系）





openstack 王者归来pdf
http://down.51cto.com/data/2065224

                    pthread_rwlock_wrlock(&g_flattenImg_rwlock);
                    g_flattenImg_map.insert(make_pair(pData->UUID, disk_name));
                    pthread_rwlock_unlock(&g_flattenImg_rwlock);

extern std::map<std::string, std::string> g_flattenImg_map;
extern pthread_rwlock_t g_flattenImg_rwlock;


Ceph 的选项
“rbd_flatten_volume_from_snapshot”: RBD Snapshot 在底层会快速复制一个元信息表，但不会产生实际的数据拷贝，因此当从 Snapshot 创建新的卷时，用户可能会期望不要依赖原来的 Snapshot，这个选项开启会在创建新卷时对原来的 Snapshot 数据进行拷贝来生成一个不依赖于源 Snapshot 的卷。

“rbd_max_clone_depth”: 与上面这个选项类似的原因，RBD 在支持 Cinder 的部分 API(如从 Snapshot 创建卷和克隆卷)都会使用 rbd clone 操作，但是由于 RBD 目前对于多级卷依赖的 IO 操作不好，多级依赖卷会有比较严重的性能问题。因此这里设置了一个最大克隆值来避免这个问题，一旦超出这个阀值，新的卷会自动被 flatten。对于这个问题，实际上社区已经有相关的解决方案了(​RBD: Shared flag, object map)，这个实现目前是由本人完成大部分工作，由于依赖 Sage 的 notify 改进，因此还需要一定时间。

“rbd_store_chunk_size”: 每个 RBD 卷实际上就是由多个对象组成的，因此用户可以指定一个对象的大小来决定对象的数量，默认是 4 MB，在不太了解 Ceph 核心的情况下，这个选项的默认值已经足够满足大部分需求了。


只读打开源不带快照copy：
now time is Fri Aug  7 18:48:02 2015
rbd_copy ok!!
after flatten, time is Fri Aug  7 18:50:50 2015


只读打开源，带快照copy：
now time is Fri Aug  7 18:53:53 2015

rbd_copy ok!!
after flatten, time is Fri Aug  7 18:56:38 2015


普通打开不带快照copy：
rbd_open success !!
now time is Fri Aug  7 19:09:08 2015

rbd_copy ok!!
after flatten, time is Fri Aug  7 19:11:39 2015


普通打开带快照copy：
now time is Fri Aug  7 19:37:40 2015

rbd_copy ok!!
after flatten, time is Fri Aug  7 19:40:07 2015


 ceph问题交流邮箱：questions@packtpub.com
 ceph申报bug：http://tracker.ceph.com/projects/ceph/issues?set_filter=1
账号：cepher_keke
密码: cp123456

允许某ip访问ssh
iptables -I INPUT -s 46.166.150.22 -p TCP --dport 80 -j ACCEPT

1、异步实现克隆问题（涉及克隆新建，克隆），备注：功能已基本完成，进行测试；
2、宽惠迁移

bash shell的命令分为两类：外部命令和内部命令。外部命令是通过系统调用或独立的程序实现的，如sed、awk等等。内部命令是由特殊的文件格式(.def)所实现，如cd、history、exec等等。


disk_type;  		#目标磁盘类型
dest_disk_type; 
src_disk_type;			
same_storage_cluster	#Info.SrcVmInfo.StorageCluster
same_node = Info.IsSameNode


ps auxf |grep "rbd flatten kvm/3752hda.img"  | awk '{print $2}' |xargs -n 1 -i kill -9 {}


ps auxf |grep "flatten" | grep  3752hda.img| awk '{print $2}' |xargs -n 1 -i kill -9 {}


     pthread_rwlock_wrlock(&g_flattenImg_rwlock);
        if((g_flattenImg_map.find(Info.ImgFile) != g_flattenImg_map.end()){
            g_flattenImg_map.erase(g_flattenImg_map.find(Info.ImgFile));
            pthread_rwlock_unlock(&g_flattenImg_rwlock);    
        }else{
            int exit_status = 0; 
            std::string cmd_output = "";
            std::string cmd =" ps auxf |grep flatten | grep "+ Info.ImgFile +" | awk '{print $2}' |xargs -n 1 -i kill -9 {}";
            cdn_log(LOG_MAIN).error(args_str("kill -9 flatten process error, cmd:%s", cmd.c_str()));
            for(int ii = 0; ii<3; ii++){
                exit_status = execute_cmd_noreturn(cmd.c_str());
                if(exit_status < 0){
                    cdn_log(LOG_MAIN).error(args_str("kill -9 flatten process error, Img:%s", Info.ImgFile.c_str()));
                }else{
                    break;
                }    
            }    
        }    

            std::string dest_disk = p->first;
            if(g_conf.disk_type == 2){
                std::vector<std::string> str_arr;
                if(dest_disk.find("/") !=std::string::npos){
                    split(dest_disk, "/", str_arr);
                }    
                dest_disk = str_arr[str_arr.size()-1];
            } 



http://hanover.iteye.com/blog/881972

如果父进程不关心子进程什么时候结束，那么可以用signal(SIGCLD, SIG_IGN)或signal（SIGCHLD, SIG_IGN）通知内核，自己对子进程的结束不感兴趣，那么子进程结束后，内核会回收，并不再给父进程发送信号
[cpp] view plaincopy在CODE上查看代码片派生到我的代码片
void AvoidZombie(void)  
{  
 struct sigaction act;  
 act.sa_handler = SIG_IGN;  
 act.sa_flags = SA_NOCLDWAIT;  
 sigemptyset (&act.sa_mask);  
 sigaction(SIGCHLD,&act,NULL);  
 return;  
}  


74上
/dev/sdb1       1.6T  1.1T  356G  76% /data

75：
/dev/sdb1       1.6T  1.3T  186G  88% /data

76：
/dev/sdb1       1.6T  731G  729G  51% /data


77上：
/dev/sdb1       1.6T  904G  555G  62% /data



cryptsetup-bin
cryptsetup

gdisk
python-flask
libgoogle-perftools0
btrfs-tools 
libradosstriper1

ceph-fuse
 ceph ceph-common ceph-fs-common ceph-mds gdisk libcephfs1
 libgoogle-perftools0 librados2 librbd1 libtcmalloc-minimal0 libunwind7
 python-cephfs python-flask python-jinja2 python-markupsafe python-rados
python-rbd python-requests python-six python-urllib3 python-werkzeug radosgw




ceph osd crush dump p
ceph osd set nodown //标志位，flag
ceph osd unset nodown 


width:1019
height:573

https://www.ustack.com/blog/build-block-storage-service/


wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
echo deb http://ceph.com/debian-giant/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list

ceph-deploy  install  –-release giant  admin  node1 node2  node3  


欢迎各位参加由ceph学院推出ceph中级课程
在这期课程中，我们将全面讲解ceph，涵盖安装，配置，基本操作，性能调优，开发入门，以及集群维护等方面
这堂课是中级课程第一节课，ceph基础入门，包括ceph的历史、现状，以及ceph集群搭建，
ok，下面开始

项目角度来审视ceph
存储系统角度

scp 192.168.99.115:/vmfs/volumes/VM2store-2/kh-cl-fltk-win03-210.14.78.38-20/kh-cl-fltk-win03-210.14.78.38-20-flat.vmdk ./

scp 192.168.99.115:/vmfs/volumes/VMstore7/khtest-cl-fltk-win03-1106-78.38/khtest-cl-fltk-win03-1106-78.38-flat.vmdk ./


由于一个OSD上大约承载数百个PG，每个PG内通常有3个OSD，因此，一段时间内，一个OSD大约需要进行数百至数千次OSD信息交换。

欢迎收看ceph学院初中级课程，这节课我们来探讨一下
ok,

1、rm  /var/lib/ceph/*  -rf  每个节点上执行
2、mkdir -p  /var/lib/ceph/{bootstrap-mds,bootstrap-osd,bootstrap-rgw,mds,mon,osd,radosgw,tmp}


ceph-deploy  key 
ceph-deploy  gatherkeys 
ceph-deploy  gatherkeys  ubuntu-ceph-06


第二章：存储基石 - RADOS
2.1 Ceph的分布式本质
2.2Ceph架构
   2.2.1 MON简介
   2.2.2 OSD简介
2.3 快速搭建RADOS环境
2.4 LIBRADOS介绍
   2.4.1 LIBRADOS的C语言demo
   2.4.2 LIBRADOS的PYTHON语言demo
   2.4.3 LIBRADOS的JAVA语言demo


确定变量的类型：type（）,最好使用isinstance（）
instance(1, int) 	//返回bool



list相关方法：
添加元素：
mylist.append()
mylist.extend([1, 2])
mylist.insert(1, "pos")

删除元素：
mylist.remove(value)
#del语句，并非函数
del mylist[pos]
#del mylist	 	#从内存中删除mylist，mylist不存在了
mylist.pop() 	#list利用栈，弹出
mylist.pop(pos)

slice:
mylist[pos1:pos2]
mylist[pos1:]
mylist[:pos2]
mylist[:s]


清空list
mylist=[]

str操作：
slice:
	mystr[:3]
	mystr[1:]
	#使用以AA为作为分隔符来分割mystr
	mystr.split("AA")
	#使用AA来来连接mystr
	例如：mystr = “123”
	mystr.jooin("AA")   #'A123A'




字符串格式化函数：format()
位置参数形式：
>>> "one = {0}, and tow = {1}, and three = {2}".format("a", "b", "c")
'one = a, and tow = b, and three = c'
关键字参数
>>> "one = {a}, and tow = {b}, and three = {d}".format(a="a",   b="b", d="c")
'one = a, and tow = b, and three = c'
综合位置参数与关键字参数：(format函数中，位置参数必须放在关键字参数前)
>>> "one = {0}, tow = {1}, three={a}, four = {b}".format("a", "b", a="c", b="d")
'one = a, tow = b, three=c, four = d'
使用{}来转译花括号
>>> "{{0}}".format("Not print")
'{0}'

>>> '%c %c %c' %(97,98,99)
'a b c'
>>> mystr="AAA"
>>> '%s' % mystr
'AAA'
#打印多个字符串，必须用元组形式
>>> '%s %s' % (mystr, mystr)
'AAA AAA'

字符串格式化代码：

格式	描述
%%	百分号标记
%c	字符及其ASCII码
%s	字符串
%d	有符号整数(十进制)
%u	无符号整数(十进制)
%o	无符号整数(八进制)
%x	无符号整数(十六进制)
%X	无符号整数(十六进制大写字符)
%e	浮点数字(科学计数法)
%E	浮点数字(科学计数法，用E代替e)
%f	浮点数字(用小数点符号)
%g	浮点数字(根据值的大小采用%e或%f)
%G	浮点数字(类似于%g)
%p	指针(用十六进制打印值的内存地址)
%n	存储输出字符的数量放进参数列表的下一个变量中

重复操作符：*
拼接操作符：+
成员关系操作符：in/not int

下列函数可用于list，tuple，str等类型
list/tuple里面的必须是同一类型才能使用max，min方法
len(mylist)
max(mylist)
min(mylist)

sum(mylist)		#只能用于list/tuple的数字类类型
sorted(mylist) 	

对存储的数据无限制
倒序：
list（reversed(mylist))
#生成存储一组元组的list，其中元祖的序号为0，1，2 ...
>>> list(enumerate(mylist1))
[(0, 'a'), (1, 'b'), (2, 'cd')]
>>> mylist1
['a', 'b', 'cd']
#返回存储一组元组的list，元组的第一个值为mylist里面值，第二个值为mylist1里面的值；取mylist，mylist1中的最短做截断
list(zip(mylist, mylist1)
[(0, 'a'), (1, 'b'), (2, 'cd')]
>>> mylist3 = list(zip(mylist, mylist1))
>>> mylist3
[(0, 'a'), (1, 'b'), (2, 'cd')]
>>> list(zip(mylist, mylist3))
[(0, (0, 'a')), (1, (1, 'b')), (2, (2, 'cd'))]


def myadd(a, b)
	'this is add function'
	return a+b

myadd.__doc__	#打印函数文档，双下划线一般是系统函数













>>> try:
...     sum = 1 + '1'
...     int('abc')
... except OSError as reason:
...     print str(reason)
... except TypeError as reason:
...     print str(reason)
... 
unsupported operand type(s) for +: 'int' and 'str'
>>> try:
...     sum = 1 + 
  File "<stdin>", line 2
    sum = 1 + 
             ^
SyntaxError: invalid syntax
>>> try:
...     sum = 1 + '1'
...     int('abc')
... except (OSError, TypeError) as reason:
...     print "error",str(reason)
... 
error unsupported operand type(s) for +: 'int' and 'str'

>>> try:
...     with open('mytest.py', 'w') as f:
...             for eachline in f:
...                     print eachline
... except IOError as reason:
...     print "my Error:", str(reason)
... 
my Error: File not open for reading

利用本地是否存在指定镜像，不存在则需要从公有仓库下载
利用镜像创建并启动一个容器
分布一个文件系统，并在只读的镜像层外面挂载一层可读写层；
从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中
从地址池中配置一个ip地址给容器
执行用户指定的应用程序
执行完毕后容器被终止；

获取容器PID
docker inspect --format "{{ .State.Pid }}" <container>
nsenter --target $PID --mount -uts --ipc --net --pid

docker数据管理
在容器中管理数据主要有两种方式：
	1、数据卷（data volumes）
	2、数据卷容器（data volume container）
数据卷：
	1、可在容器之间共享和重用
	2、对数据卷的修改会马上生效
	3、对数据卷的更新，不会影响镜像
	4、卷会一直存在，直到没有容器使用

docker网络：
	1、端口映射， -p  ip:local_port:container_port
	2、container自带的连接系统（linking)

使用brctl命令来查看网桥和端口连接信息：
	1、安装apt-get install bridge-utils

删除已存在的网桥docker0:
	1、ip link set dev docker0 down
	2、brctl delbr  docker0
新建网桥bridge0:
	1、brctl addbr bridge0
	2、ip addr add 192.168.5.1/24 dev  bridge0
	3、ip link set dev bridge0 up
	4、确认网桥创建并启动
		ip addr show bridge0


21415

30707

nginx:
#yum -y install zlib zlib-devel openssl openssl-devel pcre-devel

zlib: Nginx提供gzip模块，需要zlib库支持
openssl:Nginx提供ssl功能
pcre：支持地址重写rewrite功能



802.1Q VLAN不是一个协议，因为互连的设备之间没有协议层面的报文交互。802.1Q 
VLAN只定义了数据帧的封装格式，即，在以太网帧头中插入了4个字节的VLAN字段。其主要内容为VLAN 
TAG，紧随其后的数据类型和802.1p报文优先级的标识。

1、支持vlan的交换机的交换原理：
	引入vlan概念之后，数据帧只在相应的vlan进行交换。通俗的讲，一个交
	换机被虚拟出了多个逻辑交换机，每个vlan内的端口都是一个逻辑上的交
	换机；专业的讲，一个交换机被划分了多个不同的广播域，每一个vlan内
	的端口，在同一个广播域内；

	学习和转发都只有同一个vlan中进行，数据帧不能跨vlan交换或转发；

2、数据帧该在哪个vlan中进行交换？
	支持vlan的交换机将数据帧限制在同一个vlan中进行交换，那么数据帧
	到底哪个VLAN中交换呢？
	如果收到的数据帧携带了VLAN信息（“TAGED数据帧”，前面已经介绍了带
	vlan tag的以太帧格式），该vlan信息中的vlan tag就是交换该帧的vlan

	如果收到的数据帧没有携带VLAN信息（通常称为‘UNTAGED数据帧’），收到
	该帧的端口的PVID就是交换该帧的VLAN。

	当端口收到一个UNTAGED的数据帧时，无法确定在哪个vlan中进行交换，PVID定义了在这种情形下交换该帧的vlan。从某种意义上讲，把PVID理解为
	端口的default vlan。在支持vlan的交换机，每个端口都有一个PVID值，
	该值有一个缺省值，当然该值可以更改它；

linux包含了unix os的全部特点：
	虚拟内存、虚拟文件系统、
	轻量级进程、可靠的信号
	SVR4进程间通信、支持对称多处理器系统（SMP)
选择哪个进程执行是有程序调度scheduler决定的

文件系统用来管理文件的所有信息包含在一个叫做索引节点（node）
的数据结构中，每个文件都有自己的inode，文件系统用它来识别一个
文件；

当一个程序在用户态下运行时，它不能直接访问内核数据结构或
内核的程序，
当一个应用程序在内核态下运行时，可以直接访问内核数据结构或
内核的程序；

一个逻辑地址： 段标识符 和 一个指定段内相对地址的偏移量


private：只能由该类的成员函数、友元访问，该类的实例无法访问；
protected：只能由该类的成员函数，友元、子类访问，该类的实例无法访问；
public:均可访问；

查看某个osd上的pg数目：
ceph daemon osd.0 perf dump|grep pg

糟糕的宏：
#define CALL_WITH_MAX(a, b) f((a)>(b))?(a):(b))

int a = 5, b=0;
CALL_WITH_MAX(++a, b);
CALL_WITH_MAX(++a, b+10)；


CORS(Cross-Origin Resource Sharing, 跨域资源共享)
CORS（Cross-Origin Resource Sharing，跨域资源共享）是W3C的一个工作草案，定义了在必须访问跨域资源时
，浏览器与服务端应该如何沟通。基本思想是：使用自定义的HTTP头部让浏览器与服务器进行沟通，从而决定请
求或响应是否成功。
在发送请求时，需要附加一个额外的Origin头部，其中包含请求页面的源信息（协议，域名和端口），以便服务
器根据这个头部信息来决定是否给予响应。如：Origin：http://www.nczonline.net
	如果没有这个头部或者有这个头部但源信息不匹配，浏览器会驳回请求。注意，请求和响应都不包含cookie信息；
Preflighted Requests：

       CORS通过一种叫做Preflighted Requests的透明服务器验证机制支持开发人员使用自己的头部、GET或POST之外的方法，以及不同类型的主题内容。发送Preflighted Requests使用OPTIONS方法，发送下列头部：
       1、Origin：与简单的请求相同。
       2、Access-Control-Request-Method：请求自身使用的方法。
       3、Access-Control-Request-Headers：（可选）自定义的头部信息，多个头部以逗号隔开。
       服务器可以决定是否允许这种类型的请求。服务器通过在响应中发送如下头部与浏览器进行沟通。
       1、Access-Control-Allow-Origin：与简单的请求相同。
       2、Access-Control-Allow-Method：允许的方法，多个方法以逗号隔开。
       3、Access-Control-Allow-Headers：允许的头部，多个头部以逗号隔开。
       4、Access-Control-Max-Age：应该将这个Preflighted Requests缓存多长时间（以秒表示）。
       Preflighted 请求结束后，结果将按照响应中指定的时间缓存起来。而为此付出的代价只是第一次发这种请求时会多一次HTTP请求。

带凭据的请求：

       默认情况下跨源请求提供凭据（cookie、HTTP认证以及客户端SSL证明等）。通过withCredentials属性设置为true，可以指定某个请求应该发送凭据。如果服务器接受带凭据的请求，会用下面的HTTP头部来响应。
       Access-Control-Allow-Credentials：true。
       如果发送的是带凭据的请求，但服务器的响应中没有包含这个头部，那么浏览器就不会把响应交给Javascript（于是，responseText将是空字符串，status的值是0，而且会调用onerror（）事件处理程序）。服务器也可以在Preflight响应中发送这个HTTP头部，表示允许源发送带凭据的请求。
	




















ceph daemon  osd.0 config show |less  -----------查看ceph集群参数设置；

http://qimo601.iteye.com/blog/1727645

wireshark：
	过滤条件：
		host 192.168.1.18 and port 80
		host 192.168.1.163 and icmp
Frame：物理层的数据帧概括；
Ethernet II：数据链路层以太网帧头部信息
Internet Protocol Version 4： 网络层IP包头部信息
Transmission Control Protocol： 传输层的数据段头部信息，此处是TCP
Hypertext Transfer Protocol：应用层的信息，此处是http协议；

ceph  tcmalloc 与jemalloc
ceph  osd perf

tcpdump -n -i vnet1 host 8.8.8.8
tcpdump -n -i vnet1 host 10.128.0.×

志伟翻墙：118.26.201.224:5522 

ctrl+p //查找文件或者是函数 @表示查找函数，：表示跳转行

https://mail.mxhichina.com/alimail/auth/login?custom_login_flag=1&reurl=%2Falimail%2F
阿里邮箱：cp@ceph.org.cn  密码：Chirs123456！

python -O -m py_compile file.py

vim利用正则表达式将空格替换成换行：
	：%s/ +/\r/gc
参数说明：
	%s:在整个文件范围查找替换 
	/: 分隔符
	 +:空格匹配 其中“ ”表示空格，+表示重复1次或多次，加在一起表示一个或多个
	\r：换行符
	g:  全局替换
	c:	替换前确认

：%s/vivian/sky/g（等同于 ：g/vivian/s//sky/g） 替换每一行中所有 vivian 为 sky 
ovs目录：/var/lib/openvswitch/pki

创建KVM虚拟机流程：
	1、新建KVM虚拟机的XML定义文件：
		格式参见：http://libvirt.org/formatdomain.html

使用libvirt创建kvm虚拟机
（1）制作虚拟机镜像
		qemu-img create -f qcow2 test.qcow2 10G
（2）下载并复制iso镜像到指定目录，本文将所有镜像及配置文件放到/var/lib/libvirt/images/目录下，注意：有些系统因为SELinux的原因，限定了qemu的访问，
	 所以，可以根据自己需求调整，默认放在/var/lib/libvirt/images/下。
（3）创建安装配置文件，demo.xml如下，可以根据自己需求更改。

virsh start test_ubuntu //启动虚拟机
virsh vncdisplay test_ubuntu //查看虚拟机的vnc端口， 然后就可以通过vnc登录来完成虚拟机的安装
//virtual network contorl   

显卡在虚拟化中分为三类：pass-through方式，这种方式，一个显卡只能支持1个虚拟机，xenserver一般采用此方式。share-gpu，一个gpu可以支持多个虚拟机，这种方式一般使用hypervison这一层来实现，esxi和hyper-v一般支持。vgpu，一个gpu可以支持多个虚拟机，这种方式是使用gpu硬件驱动来实现，xenserver即将会支持。


glusterfs 命令：
	1、glusterfs -V
	2、gluster  peer status	查看状态
	3、server gluster stop
	4、server gluster start
为存储池添加/移除服务器节点
	在其中一个节点上操作即可：
	# gluster peer probe <SERVER>
	# gluster peer detach <SERVER>

	注意，移除节点时，需要提前将该节点上的Brick移除

查看所有节点的基本状态（显示的时候不包括本节点）：
	# gluster peer status

挂载分区
	#mount -t ext4 /dev/sdd1/ mnt/brick1

创建/启动/停止/删除卷
	#gluster volume start <VOLNAME>
	#gluster volume stop  <VOLNAME>
	#gluster volume delete <VOLNAME>
注意，删除卷的前提是先停止卷；

查看cache中的最新内核版本
apt-cache search kernel

升级到指定内核版本：
sudo apt-get install linux-image-3.8.0-25-generic
sudo apt-get install linux-headers-3.8.0-25-generic

安装完成新内核reboot

查看安装的内核版本：
dpkg --get-selections | grep linux-image

卸载内核：
apt-get remove linux-image-3.8.0-29-generic

查看glib库版本：
 apt-cache show libc6 

/*
  在ICE文档中只需要声明module名称，接口名称，方法名称
*/

#ifndef SIMPLE_ICE
#define SIMPLE_ICE

module Demo{                     //module名称
  interface Printer              //接口名称
  {
    void printString(string s);  //方法名称

  };

};

#endif

s端示例：
try{
    ic = Ice.Util.initalize(ref args);
    Ice.ObjectAdapter adapter = ic.createObjectAdapterwithEndpoints("SimplePrintAdapter",
                              "default -p 10000");
    Ice.Object obj = new Printerl();  //Printerl类继承了Printer
    adapter.add(obj, ic.stringToldentity("SimplePrinter"));
    adapter.activate();
    ic.waitForShutdown();
  }catch (Exception e){
    Console.Error.WriterLine(e);
    status =1;

  }

功能：
  1、创建一个对象适配器（ObjectAdapter）对象IOAdapter，并初始化之；
  2、参数“SimplePrinterAdapter”：表示适配器的名字；
  3、参数“default -p 10000”：表示适配器使用缺省协议（TCP/IP）在端口10000处监听到来的请求；
  4、服务器配置完成；
  5、为Printerl接口创建一个servant；
  6、激活适配器，以使服务器开始处理来自c端的请求；
  7、挂起发出调用的线程，知道服务器实现终止为止；
  8、或者是通过发出一个调用关闭运行（run time）的指令来使服务器终止；

C端：
try{
  ic = Ice.Util.initialize(ref args);                                //获取远程对象代理
  Ice.ObjectPrx obj= ic.stringToProxy("SimplePrinter:default -p 10000");  //创建一个代理对象，并用通信器的stringToProxy（）方法初始化之；
  PrinterPrx printer = PrinterPrxHelper.checkCast(obj);       //调用服务鉴别函数
  if(printer == NULL)
  {
    throw new ApplicationException("Invalid proxy");

    printer.printString("Hello World!!");  //成功则远程调用方法
  }catch (Exception e){
    Console.Error.WriteLine(e);
    status = 1;
  }
}

功能：
  1、获取远程对象代理
  2、创建一个代理对象，并用通信器的stringToProxy（）方法初始化之；
  3、提供参数：“SimplePrinter：default -p 10000”
  4、调用服务鉴别函数，如果不成功则抛出异常信息“Invalid proxy”；成功则
  远程调用方法：printer.printString("hello world";)

dynamic_cast //动态转换


Ice序列的映射：
sequence<Fruit> FruitPlatter;
Slice编译器会为FruitPlatter生成这样的C++定义：
	typedef  std::vector<Fruit> FruitPlatter;
序列会简单地映射到STL向量；可以将平常所有的STL迭代器和算法用于该向量；

FruitPlatter p;
p.push_back(Apple);
p.push_back(Orange);
 

词典的映射：
dictionary<long, Employee> EmployeeMap;
生成：typedef std::map<Ice::Long, Employee> EmployeeMap;

重启ceph服务
restart ceph-all    //时间有点长

filestore xattr use omap = true
osd max attr size = 655360

ceph-authtool /etc/ceph/ceph.mon.keyring --create-keyring --gen-key -n mon.


#include <Ice/Ice.h>
int main(int argc, char *argv[])
{
	int status = 0;
	Ice::CommunicatorPtr ic;	//该对象由Application提供；
	try {
		ic = Ice::initialize(argc, argv);	//将Ice::initialize的调用放在try模块里
											//并且会负责把正确的退出状态返回os；只有在初始化成功之后，
											//代码才会尝试销毁通信器
		//Server code here....
	}catch (const Ice::Exception & e){
		cerr << e << endl;
		status = 1;
	}catch (const std::string & msg){
		cerr << msg << endl;	
		status = 1;
	}
	if(ic)
		ic->destory();
	return status;
}

Ice::Application类：
由Ice提供，它封装了所有正确的初始化和结束活动；
namespace Ice{
	class Application /*.....*/ {
		public:
				Application();
			virtual ~Application();
				int main(int, char *[], const char * = 0);
			virtual int run(int, char* []) = 0;	//纯虚方法
			static const char* appName();
			static CommunicatorPtr communicator();
	}
};

#include <Ice/Ice.h>
class MyApplication : virtual publice Ice::Application{
public:
	virtual int run(int,  char *[]){
		//Server code here....
		return 0;
	}

};

int main(int argc, char* argv[])
{
	MyApplication app;
	return app.main(argc, argv);
}


iptables -A INPUT -ptcp --dport 9102 -j ACCEPT 	//开放9102端口

strace -f service ceph start mon >a.log 2>&1

 /usr/bin/radosgw -d --debug-rgw 20 --debug-ms 1 start

查看内存信息：
free -l
查看cpu信息：
cat /proc/cpuinfo | grep processor


设置连接ceph集群句柄：
	1、创建一个集群句柄用于连接存储集群；
	2、使用句柄进行连接，App must supply a monitor address, a username and an authentication key 
once you have a cluster handle, you can:
	1、Get cluster statistic
	2、Use Pool Operation(exists, create , list , delete)
	3、Get  and set the configuration

CREATING AN I/O CONTEXT
	I/O Context functionality includes:
		write/read data and extended attributes;
		List and iterate ove objects and extended attributes
		Snapshot pools, list snapshots, etc
asychronous IO
	rados_aio_write()
	rados_aio_append()
	rados_aio_write_full()
	rados_aio_read()


namespace IceUtil{
	template <class T>
	class Monitor {
		public:
			void lock() const;
			void unlock() const;
			bool tryLock() const;
			
			void wait()   const;
			bool timeWait(const Time&) const;
			void notify();
			void notifyAll();

			typedef LockT<Monitor<T> > Lock;
			typedef TryLockT<Monitor<T> > TryLock;
	}

}
cookie包含啥信息


不重启修改hostname
1、修改/etc/host/
	127.0.0.1  new-hostname
2、hostname new-hostname
3、退出shell。再重新登录


nslookup www.baidu.com 8.8.8.8

route -n  查看网关

linux内网ip设置
/etc/network/interfaces

加上：
auto eth1
iface eth1 inet static
	address 10.0.0.50
	netmask 255.255.255.0
/etc/init.d/networking restart


[root@localhost ~] vi /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0(设置网卡设备名)
BOOTPROTO=static(设置网卡以static或dhcp方式运行)
BROADCAST=192.168.1.255(广播地址，可以不设)
HWADDR=00:40:D0:13:C3:46(mac地址，默认不需要设置)
IPADDR=192.168.1.100(本机IP地址)
NETMASK=255.255.255.0(子网掩码)
NETWORK=192.168.1.0(网络地址，可以不设)
GATEWAY=192.168.1.1(默认网关，可以不设)
ONBOOT=yes(设置是否开机启动，yes为自动启动)
MTU=500(1设置最大传输单元的值，一般很少用到)
以上就是ifcfg-eth0的设置值了。
一般来说，如果设置静态IP的话，只需要设置以下几个值:
DEVICE、ONBOOT、BOOTPROTO、IPADDR、NETMASK
如果设置动态IP，只需设置:
DEVICE、ONBOOT、BOOTPROTO

vim中删除到行首/尾：d^/d$

1、首先停止osd进程后，清空相应目录中的内容
/etc/ceph/
/var/lib/ceph/osd
/var/lib/ceph/bootstrap-osd
/mnt/ceph/osd1
2、在mon节点执行
ceph-deploy admin osd-hostname
3、从crush map中移除osd的信息
ceph osd out osd.x
out出去，待重新平衡之后，删除就可以
ceph osd crush remove osd.x
ceph auth del osd.x
ceph osd rm osd.x
//osd挂掉之后，最好不要直接out掉该osd点；


ceph存储区别：
块存储例如iscsi，对象存储例如fastdfs、文件系统就是本地存储或者mfs


ceph ceph-common ceph-fs-common ceph-mds librados2 librbd1 python-ceph   // 卸载这些
ceph-deploy install --release firefly  admin node1 node2 node3

apt-get install librbd-dev  //单独安装

Ceph的底层是RADOS(可靠、自动、分布式对象存储)，可以通过LIBRADOS直接访问到RADOS的对象存储系统。RBD(块设备接口)、
RADOS Gateway(对象存储接口)、Ceph File System(POSIX接口)都是基于RADOS的。


grep -n rbd_create * -r

rbd -p libvirt-pool ls   //查找池里文件


int ret = -1;
char *name = "my_image";
uint64_t  size = 4 * 1024**3 ;
int obj_order = 1;
rbd_image_t* image = NULL;
char *snap_name = "snap_name";

ret = rbd_create(io_ctx,  name, bytes,  &obj_order );
if(ret == 0){
	printf("success \n");
}else{
	printf("failure \n");
}

ret = rbd_open(io_ctx, name, image, snap_name);
if(ret == 0){
	printf("rbd_open image success \n");
}else{
	printf("rbd_open image failure \n");
}


rados_ioctx_destroy(io_ctx);
rados_shutdown(cluster);

#!/usr/bin/python
#coding=utf-8

import rbd 
import rados

cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
try:
	cluster.connect()
	ioctx = cluster.open_ioctx('libvirt-pool')
	try:
		rbd_inst = rbd.RBD()
		size = 4 * 1024**3  # 4 GiB
		rbd_inst.create(ioctx, 'myimage', size)
		image = rbd.Image(ioctx, 'myimage')
		try:
			data = 'foo' * 200 
			image.write(data, 0)
		finally:
			image.close()
	finally:
		ioctx.close()
finally:
	cluster.shutdown()

g++ test2.cpp -lrados  -lrbd -o client++


qemu-img create -f rbd rbd:libvirt-pool/new-image 2G


size   			obj_order 
2147483648 B	22

rbd命令集合：
rbd -p pool-name ls


radosgw-admin user create --uid=cptest12  --display_name=cptest12 --access_key=cptest12 --max_buckets=10

vim 显示tab键  set list
1、显示 TAB 键

文件中有 TAB 键的时候，你是看不见的。要把它显示出来：

:set list

现在 TAB 键显示为 ^I，而 $显示在每行的结尾，以便你能找到可能会被你忽略的空白字符在哪里。 
这样做的一个缺点是在有很多 TAB 的时候看起来很丑。如果你使用一个有颜色的终端，或者使用 GUI 模式，Vim 可以用高亮显示空格和TAB。 
使用 ‘listchars’ 选项：

:set listchars=tab:>-,trail:-

现在，TAB会被显示成 ">—" 而行尾多余的空白字符显示成 "-"。看起来好多了，是吧？
100000000000000000000000000000000


create table base_info(
	id int IDENTITY(1, 1) PRIMARY KEY,
	domain_name char(50)  default NULL,
	domain_admin char(50) default NULL,
	domain_tel   char(50) default NULL,
	domain_mail  char(50) default NULL,
	domain_addr  varchar(50) default NULL,
)


参数：poolname,  image_name（单位是B）, bytes(镜像大小)， 分块默认值：22
写入的内容，内容的大小，偏移量ofs, 

创建kvm虚拟机：
使用命令行：
	#qemu-img create -f qcow2 /var/lib/libvirt/images/centos6.6-x86_64.qcow2 2G
	#virt-install --virt-type kvm --name centos6.6-x86_64 --ram 1024 	\
	--disk /var/lib/libvirt/images/centos6.6-x86_64.qcow2,  format=qcow2	\
	--network network=default	\
	--graphics vnc, listen=0.0.0.0 --noautoconsole	\
	--os-type=linux --os-variant=rhel6	\
    --cdrom=/opt/iso/CentOS-6.6-x86_64-minimal.iso
或者使用图形化界面：
打开 virt-manager
远程登录到装的 kvm 平台的服务器上，打开 virt-manager（Xshell 在会话属性-连接-SSH-隧道-X11 转移-勾选转发X11连接到 Xmanager）



stresslinux
ceph稳定性测试工具：iozone



查看ceph相关文档：
ceph保存数据一致性的方法：
	Ceph的OSD可以比较放置在一个组对象元数据存储与在其他的OSD布置组及其副本。清理（通常每天执行）捕获OSD的错误或文件系统错误。
	OSD还可以通过比较对象中的数据比特位进行更深层次的清理。深度清理（通常每周执行）可以发现在平时清理中不会被发现的磁盘上的坏扇区。

ceph状况的详细状态：ceph health detail

配置文件直接下发
	ceph-deploy admin cs-node4（配置点）	//先cd到/ceph目录下
修复pg点：
	 ceph pg repair 3.7b


crush算法通过计算数据的存储位置确定了如何存储和检索数据，crush授权ceph的客户端直接与osd沟通，而不是通过
一个通过集中的服务器或代理。拥有一个决定存储和检索数据方法的一个算法；ceph避免了单点故障，性能瓶颈，它的
规模的物理极限；
	crush需要集群的一个映射，并使用crush有映射在整个集群的数据是均匀分布在osd上防伪随机的存储和检索数据；
crush映射包含一系列的osd，一系列聚集设备至物理位置的buckets和一系列的告诉crush如何在ceph的存储池上复制数据
的规则。通过底层物理组织安装的反馈。crush可以模拟相关


ceph基本操作：
	/etc/init.d/ceph -a start/stop  //即在所有节点上执行

	单个操作：
		start/stop ceph-osd id={id}
		start/stop ceph-mon  id={hostname}
		start/stop ceph-mds  id={hostname}

	集群监控：监控osd状态、monitorstatus、placement组（pg）、元数据服务器状态；
	#ceph        //进入ceph会话模式
	ceph> health
	ceph> status
	ceph> quorum_status		//？？
	ceph> mon-status	

	实时监控集群：
	#ceph -w

	#ceph osd stat        //检测osd的状态
	#ceph osd  dump		  //检测osd的详细信息
	#ceph osd tree		  //根据Crush Map查看osd的状态

	monitor监控：
	#ceph mon stat 		
	#ceph mon dump
	#ceph quorum_status

	mds监控：
	#ceph mds stat
	#ceph mds dump

	使用admin socket
	ceph管理员通过一个Socket接口查询一个守护进程，默认情况下，在/var/run/ceph下
		#ceph --admin-daemon /var/run/ceph/{socket-name}

		#ceph --admin-daemon /var/run/ceph/{socket-name} help

	监控守护进程OSD的4中状态：
		In----Out	（是否在集群里面）	
		Up----Down	（是否是开启状态）

	pg集群：（placement  groups）
		查看pg列表
		#ceph pg dump
		#ceph pg stat
		#ceph pg dump -o {filename} --format=json  #输出json格式，并保持文件



		查看pg map 及 参数
		#ceph pg map {pg-num}
 
 		#获取池子副本个数：
 		#ceph osd pool get POOLNAME size
		#查看osd池子个数
		ceph osd lspools
		#池子名字必须是两次
		ceph osd pool delete {pool-name} {pool-name} --yes-i-really-really-mean-it



du -sh  /var/local //查看目录下所有文件大小


从crush map中移除osd的信息
ceph osd out osd.x
ceph osd crush remove osd.x
ceph auth del osd.x
ceph osd down osd.x
stop ceph-osd id=x
ceph osd rm osd.x

sudo passwd root


命令待分析：
	1、ip route
	2、nohap ./run &
	3、 ip route add  222.132.16.48/28 dev br0  proto kernel  scope link  src 
		 222.132.16.50
	4、ip route del 222.132.16.48/28
	5、ip rule
	6、arp -n
	7、netstat -tlunp   #查看存在的连接
	8、lsblk

many objecs map to one PG
each object maps to exactly one PG
One PG maps to a single list of OSDs, 
Many PGs can map to one OSD

A PG represents nothing but a grouping of objects; you configure the number of PGs you want
(http://ceph.com/wiki/Changing_the_number_of_PGs )

//伪随机算法
locator = object_name
obj_hash = hash(locator)
pg = obj_hash%num_pg
OSDs_for_pg = crush(pg)
primary = osds_for_pg[0]
replicas = osds_for_pg[1:]


def crush(pg):
	all_osds =['osd.0', 'osd.1', 'osd.2', ....]
	return = []
	while len(result) < size:
		r = hash(pg)
		chosen = all_osds[r%len(all_osds)]
		if chosen in result:
			#OSD  can be picked only once
			continue
		return.append(chosen)

	return result

ceph计算pg数量：
	每个存储池的PG = （OSD*100）/replica_size（副本数）
	若PG  - 2^(X) < 0.25* PG， 那么设置PG数为2^(X)；否则设置为2^(X+1);
	例如：
		3个osd，存储池的size是2，那么
		PG = 3 * 100 / 2 = 150
		0.75 * 150 < 2^7,所以PG=128，X=7

开源监控利器grafana
ceph命令：
	ceph osd pool set rbd pgp_num 100
Important：Ceph doesn’t support QCOW2 for hosting a virtual machine disk. Thus 	 if you want to boot virtual machines in Ceph (ephemeral backend or 			boot from volume), the Glance image format must be RAW.
Note that image must be RAW format. You can use qemu-img to convert from one format to another：
	qemu-img convert -f {source-format} -O {output-format} {source-filename} {output-filename}
	qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw

 rbd是一个实用程序，用于操纵rados块设备（RBD）的镜像，QEMU/KVM就是使用的Linux rbd驱动和rbd存储驱动
 打印池子详细信息：rbd -p libvirt-pool ls  -l

ceph log路径：/var/log/ceph/ceph

详细信息查看osd0
ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | less

ceph设置池子pg_num:
ceph osd pool set {pool-name} pg_num

PG:Placement Group，用途是对object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object（数千上万），
但一个object只能被映射到一个PG咋红，即：PG和object之间是“一对多”的映射关系，
object id;即：oid


导入池子：
rbd import ./ceph_test.qcow2  --dest-pool libvirt-pool


比如你现在有3个osd。rbd pool size是3，min_size是2。集群健康的情况下，每个object有3个副本。如果一个osd挂掉，object就只剩两个副本了。这时虽然集群不健康了，但仍可以正常访问pool里的数据。然后要是再挂掉一个osd，object就只剩一个副本了，这时就无法正常访问pool里的数据了。


服务器启动跳过检查硬盘
shell大于：greater  than  //gt
	小于：less  than      //lt
	等于：equal    		  //eq 	 

openstack 命令：
nova-manage service list

ceph认证加入：libvirt：
1、ceph auth get-key client.cinder | tee client.cinder.key
2、virsh secret-define --file secret.xml
3、virsh secret-set-value --secret bfe573d1-0a91-42e8-941a-49d5730c4150  --base64 $(cat client.cinder.key) 

ntpdate asia.pool.ntp.org  //同步时间

for i in {1..10}; do echo "$i"; done;
for((i=0;i<=10;i++));do echo "$i"; done
find    /opt   -mmin   -1  //查找1分钟之内修改过的文件

shell多行注释：
:'
echo "ni"
echo "ni"
echo "ni"
echo "ni"
'
1、增加monitors
ceph-deploy mon add ceph-node2 
ceph-deploy mon add ceph-node3
ceph-deploy mon create ceph-node2 ceph-node3

ceph-deploy gatherkeys ceph-node2
ceph-deploy gatherkeys ceph-node3


调试网络
ping www.baidu.com
arp -n 

tcpdump -n arp
tcpdump -en arp -i eth0

mtr ip
dig ip

证书验证：放在window下，linux下的.pem文件改成.crt文件，打开即可查看；


ubuntu下强制更新dhcp模式下获取的ip
$ sudo dhclient -r //release ip 释放IP
$ sudo dhclient //获取IP
win：(卸载网卡，之后在安装，方法之一)
ipconfig /release
ipconfig /renew

tcpdump -en -i vnet5 port 67 or port 68

vim多行操作
<ESC>之后按v进入visual模式。 
<ESC>之后按CTRL+v进入visual block模式（列编辑）。
按下I之后插入操作，按下x删除操作
再按下ctrl+[ 退出编辑即可

substitutil-g 显示目前编辑的文件名、是否经过修改及目前光标所在之位置。
:f 文件名改变编辑中的文件名。(file)
:r 文件名在光标所在处插入一个文件的内容。(read)
:35 r 文件名将文件插入至35 行之后

on 替换

:f 或Ctr

cw:删除一个单词并进入插入模式,cc:删除一行并进入插入模式。
r:然后输入的字母将替换当前字母并保持命令模式,R 则是不停的替换(一个挨着一个)。

yfa 表示拷贝从当前光标到光标后面的第一个a 字符之间的内容.
dfa 表示删除从当前光标到光标后面的第一个a 字符之间的内容.

qemu-img check -f qcow2 -r all copy.img  //检测并修复磁盘
ifconfig   br0
ovs-ofctl show br0

ip route add default via  203.156.196.1 src 203.156.196.3
ip route change default via  203.156.196.1 src 203.156.196.3

证书验证：放在window下，linux下的.pem文件改成.crt文件，打开即可查看；

sed -n '1,100p' ip.txt.bak
awk -F '.' '{print $4}' 

select c.storage  from `vhost` a,  `node` b ,`cluster` c where a.uuid=uuid and a.node_id=b.id and b.cluster_id = c.id;

端口开放情况：
	netstat -antp
	 iptables -F    //端口不通也有可能是防火墙问题；

c++字符串分割：
	std::string str_buf = "hello/world";
	std::vector<std::string> str_arr;
	split(str_buf, "\n", str_arr);

	for(std::vector<std::string>::iterator it=str_arr.begin();
		it !=str_arr.end();it++){
		std::cout << "std::string is" << *it << std::endl;
		}
	std::string tmp = str_arr[str_arr.size() -1 ]
string s("12345asdf");
string a=s.substr(0,5);       //获得字符串s中 从第0位开始的长度为5的字符串//默认时的长度为从开始位置到尾


去掉字符串最后的换行符：
 if(my_string[my_string.Length()]=='\n')     
  my_string.Delete(my_string.Length(),1);

zt  		"当前行置于屏幕顶端
ctrl+f      "向前滚一整屏


开发-广州-Diluga:
搞定把每个osd和mon放置在对应的cgroup里面

Migrate
ls -all /edv/disk/by-uuid //显示分区的信息可以查看到各分区的uuid
 /etc/fstab //编辑fstab命令


rbd -p kvm ls  |xargs -n 1 -i rbd info kvm/{} |more
-i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给{}，可以用{}代替。
-n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。

去掉最开头二行，最末尾一行
virsh list --all | awk 'NR>3{print p}{p=$0}' |awk '{print "virsh dumpxml "$2" > "$2".xml"}'|sh
awk 'NR>2{print p}{p=$0}' urfile
$0:表示行号的内容，切记
第一行时, NR=1, 不执行print, p=第一行的内容
第二行时, NR=2, 不执行print, p=第二行的内容
第三行时, NR=3, 执行print p,此时p=第二行的内容, 即打印第二行, 然后p=第三行
............
最后一行时, 执行print p, 打印倒数第二行, 然后p=最后一行

也就是去除了第一行和最后一行... 



auth_key

libvirt 迁移命令：
virsh  migrate --live 53fc497f-72be-80c4-a177-bb20c356929c qemu+tcp://192.168.2.17/system


VIR_MIGRATE_UNDEFINE_SOURCE, VIR_MIGRATE_PEER2PEER

qemu 迁移使用方法
 
迁移是qemu中支持的，libvirt只是封装好命令并传递给qemu的监控模块。
1、qemu中使用方法：
在server端，在启动qemu的命令行中加入-incoming tcp:0:4444(4444为端口）参数，启动后可以应用netstat -apn 命令查看，4444端口是不是在监听。
在client端，启动qemu后（sdl模式下）， 使用ctrl+alt+2切换到监视端口，输入命令:migrate -d tcp:10.10.10.1:4444。（可以使用info migrate 查看migrate状态）

2、virsh中使用实例：
migrate 待迁移域名 qemu://10.10.10.1/system （tls 模式）
migrate 待迁移域名 qmeu+ssh://10.10.10.1/system （ssh 模式）
migrate 待迁移域名 qmeu+tcp://10.10.10.1/system （tcp 模式）

上面是静态迁移，其中tls模式需要加密和鉴权文件，详细操作见下面网址：

http://wiki.libvirt.org/page/TLSSetup#Full_list_of_steps

migrate --live 待迁移域名 qemu+ssh://10.10.10.1/system（动态迁移）

3、在virt-manager中，在虚拟机名字上右击鼠标右键，弹出的对话框中，有迁移一项，选上offline为静态迁移，不选为动态迁移。

迁移flag：VIR_MIGRATE_LIVE	：动态迁移 
		  VIR_MIGRATE_PERSIST_DEST ：
		  VIR_MIGRATE_PEER2PEER	   ：
		  VIR_MIGRATE_NON_SHARED_INC：增量形式
		  VIR_MIGRATE_NON_SHARED_DISK：完全拷贝

migrate command:
virsh migrate 53fc497f-72be-80c4-a177-bb20c356929c --live qemu+tls://192.168.2.17/system --xml  /etc/libvirt/qemu/53fc497f-72be-80c4-a177-bb20c356929c.xml 


1、增量迁移；完全迁移；
2、磁盘信息已使用/容量区别


select b.storage_type, b.type, b.vlan_con_node_id, b.volume_name,b.port, b.control_ip, b.auth_key from node a, cluster b where a.control_ip='192.168.2.18' and a.cluster_id=b.id

egraded状态是pg的副本数不够，看下是不是存储池的副本数大于crush的分组数

undersized是up集合和acting集合不一致



ceph存储集群API:
	apt-get install librados-dev (ubuntu)
	apt-get install librados2-devel (centos)


windowserver 2012链接:
9600.16384.WINBLUE_RTM.130821-1623_X64FRE_SERVER_SOLUTION_ZH-CN-IRM_SSSO_X64FRE_ZH-CN_DV5.ISO 

模板密码:yunvm.com789

p/src# rbd snap create template/centos_6.0_x86.img@snapname_01
root@u18:~/cp/src# rbd snap protect template/centos_6.0_x86.img@snapname_01
root@u18:~/cp/src# rbd clone template/centos_6.0_x86.img@snapname_01 kvm/centos_test_01
root@u18:~/cp/src# rbd -p kvm ls |grep centos

ceph auth caps client.libvirt mon 'allow r'  osd 'allow class-read object_prefix rbd_children, allow rwx pool=kvm, allow rx pool=template'

rbd import --image-format 2 ./ubuntu_12.04_x86.img   --dest-pool template


valgrind


先tell ，然后写ceph.conf，然后push

ceph编辑ceph map:
导出并编辑：
	ceph osd getcrushmap -o map
	crushtool -d map -o map.txt
	vim map.txt
导入：
	crushtool -c map.txt -o map
	ceph osd setcrushmap -i map

参照：http://ceph.com/docs/master/rados/operations/crush-map/



windows查看log：Eventvwr.msc;

for i in $(cpeh osd ls);  do ceph tell osd.${i} version; done;
{ "version": "ceph version 0.80.9 (b5a67f0e1d15385bc0d60a6da6e7fc810bde6047)"}
{ "version": "ceph version 0.80.9 (b5a67f0e1d15385bc0d60a6da6e7fc810bde6047)"}
{ "version": "ceph version 0.80.9 (b5a67f0e1d15385bc0d60a6da6e7fc810bde6047)"}

osd配置：
  ssh {new-osd-host}
  mkfs -t {fstype}  /dev/{disk}
  mount -o user_xattr /dev/{hdd}  /var/lib/ceph/osd/ceph-{osd-number}

下面命令列出Ceph作业和例程：
  initctl list |grep ceph


1. apt-get update; apt-get install pptpd
2. vim /etc/pptpd.conf    # 修改客户端 ip 范围
3. vim /etc/ppp/options  # 修改 ms-dns
4. vim /etc/ppp/chap-secrets  # 添加用户名密码
5. sysctl -w net.ipv4.ip_forward=1
    vim /etc/sysctl.conf   # 开启 ip_forward
6. iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j MASQUERADE    # 地址伪装




L爷 2015/5/4 17:43:54
xuzhiquan	*	Xj0Eh2iK0IidIp3B		192.168.0.101
xulei		*	J2oKuJj6m7a6fMJC		192.168.0.102
daiwei		*	uM1sb7JRcR7kGc7W		192.168.0.103
chenxingguo	*	rOWj33tGnvYM09Ep		192.168.0.104
louting		*	0e8qAGdP6OZjfi8M		192.168.0.105
yanxiaoming	*	1WJt7oGlOVWrh3h6		192.168.0.106
jingqiang	*	7K1hRLznHd1d8bWG		192.168.0.107
xiangliang	*	nP1f4zTKGvhNw3M4		192.168.0.108
shenzhiwei	*	Uo9WQ9kmFHH8rk6s		192.168.0.109
genghanghang	*	QtoU80iQlKj3QB2j		192.168.0.110
liyang		*	3SeMqCn7ILu4uG5r		192.168.0.111

L爷 2015/5/4 17:45:02
mkpasswd.pl -l 16 -d 4 -C 6 -c 6 -s 0
密码生成规

less than 5 OSDS set pg_num  to 128
Between 5 and 10 OSDS set pg_num to 512
Between 10 and 50 OSDS set pg_num to 4096


crushmap 简单使用
获取crush map
ceph osd getcrushmap -o /tmp/map  或者  ceph osd  crush  dump ，关于crush的命令集，可以通过　ceph osd crush help 查看

反编译map
crushtool -d /tmp/map -o /tmp/map.txt

 编译map
crushtool -c /tmp/map.txt -o /tmp/map.new

 设置map
ceph osd setcrushmap -i  /tmp/map.new

crushmap设置参看  http://ceph.com/docs/master/rados/operations/crush-map/

设置pool 的crush rule

ceph osd pool set <poolname> crush_ruleset 4   #此处 4 是指在rule 里 ruleset 设置的值


f1bf5374-20fa-46c1-a3f6-c2aa21ce0d0e 96密钥


在crushmap里面加入
研发-武汉-杜衡 2015/5/7 16:39:40
手动修改下crushmap，或者ceph osd crush add osd.20 WEIGHT
研发-武汉-杜衡 2015/5/7 16:39:46
最好是修改crushmap


ceph tell osd.0 injectargs "--rbd_cache true" 

ceph-deploy disk zap --fs-type xfs osd0:/dev/sd


"osd_scrub_load_threshold": "0.5",

load average 低于0.5 才会srcub 


virsh list --all| awk '{if (NR>2) print $2}' |xargs -n 1 -i  virsh undefine {}
virsh list --all|awk '{if(NR>2) print $2}' |xargs -n 1 -i virsh undefine {}


 命令行模式下输入  sp 另外一个文件 就可以水平分割继续打开第二个文件，如果想纵向分割，可以使用vsp 文件名


RBD的I/O路径很长，要经过网络、文件系统、磁盘：
librbd -> networking -> OSD -> FileSystem(xfs) -> Disk

int rbd_stat(rbd_image_t image, rbd_image_info_t *info,  size_t infosize);

Type     迁移方式，1：动态增量拷贝迁移，2：静态增量拷贝迁移，如果虚拟机在运行中，将强制关机, 3:动态完全拷贝迁移
http://youxiyun.cc/

#define random(x) (rand()%x)

python字典：
	for key, value in data.iteritems()
		print key, value

	for key in data:
		print data[key]

	data.get('key', 'N/A')		//字典的get函数，当key不存在时候，返回第二个参数，
	默认是None
	data[key]   //key不存在时，返回错误，不安全

	data.setdefault('key', 'chengpeng')	//key不存在字典里将该键值对加入字典；

	data['key'] ="cp"		//当字典里含有该key时，会覆盖value，

列表和字典dict是应用程序使用最频繁的数据结构；
元组和字典主要用在函数调用之间交换参数和返回值；

for line in open('./aa.txt'):
	print line


data=[123, 'abc'. 'ddf']
for i, value in enumerate(data):	//使用计数函数enumerate()
	print i, value
0 123
1 abc
2 ddf

使用raise函数抛出异常：
Django创建项目：(提供一个django-admin.py的命令)---（Django内置web服务器）
	django-admin.py startproject mysite
生成mysite目录，里面包含__init__.py ; manage.py  ; setting.py ;  urls.py
manage.py文件：一个同Django项目一起工作的工具；
setting.py文件：包含项目的默认设置，包括数据库信息，调试标志以及其他一些重要的变量；
urls.py文件：在Django里叫urlconf，它是一个url模式映射到你应用程序上的配置文件；
	启动服务：./manage.py  runserver
有了项目之后，我们就可以在项目下面创建应用（“app”）：
	./manage.py  startapp blog      #创建一个blog应用，blog应用是项目里面的一部分；
创建应用之后，我们的项目目录下有一个blog目录，
	 里面包含：__init__.py,   models.py,    views.py

要让Django知道blog 应用是项目的一部分，你应该去编辑setting.py（配置一下）
	打开setting.py在文件尾部找到INSTARLLED_APPS元组，把blog应用以模块的形式加入元组，
		‘mysite.blog’;

设置应用中的model；
	blog应用的核心部分：model.py文件，这是定义blog数据结构的地方；

from django.db  import models
class BlogPost(models.Model):
	title = models.CharField(max_length=150)
	body = models.TextField()
	timestamp = models.DataTimeField()


创建表：
	./manage.py  syncdb

通常做法是，将nginx作为服务器最前端，它将接收WEB的所有请求，统一管理请求。nginx把所有静态请求自己来处理（这是NGINX的强项）。
然后，nginx将所有非静态请求通过uwsgi传递给Django，由Django来进行处理，从而完成一次WEB请求。

http://www.2cto.com/os/201412/359411.html

/home/site/june/june
./templates/node/create.html

c++随机
#include <iostream>
#include <stdlib.h>
#include <time.h>

#define MAX 100

int main()
{
    srand((unsigned)time(NULL));//srand()函数产生一个以当前时间开始的随机种子
    for(int i=0;i<10;i++){
        std::cout <<"rand value:"<<rand()%MAX << std::endl;
    }   
    return 0;
}

flask初始化：
所有的flask程序都必须创建一个程序实例，web 服务器使用一种名为web服务器网关接口（web server gateway interface； WSGI）协议，把接收客户端的所有请求都转交给这个对象处理。程序实例是flask类的对象；
	from flask import Flask
	app = Flask(__name__)
Flask类的构造函数只有一个必须指定的参数，即程序主模块或包的名字；

路由和视图函数：
	客户端（web浏览器）把请求发送给web server， web server再把请求发送给Flask程序实例；
程序实例需要知道对每个url请求运行哪些代码，所以保存了一个url到python函数的映射关系。
处理url和函数之间关系的程序成为路由。

在Flask程序中定义路由的最简便方式，是使用程序实例提供的app.route修饰器。把修饰的函数注册为路由；
例如：
	@app.route('/')
	def index():			//该函数称为视图函数
		return '<h1>Hello World!</h1>'

ssh u62 "cat /root/.ssh/id_rsa.pub_u60>> /root/.ssh/authorized_keys"

 518   for (i = 0; i < num_snaps; i++) {
 519     printf("snap: %s\n", snaps[i].name);
 520   }
 521 


openstack架构分析：
http://blog.csdn.net/gaoxingnengjisuan/article/category/1461395
1、nova-scheduler分析：
	代码路径：/usr/lib/python2.7/dist-packages/nova/scheduler


网络术语：
birdge：网桥
	Bridge（桥）是Linux上用来做TCP/IP二层协议交换的设备，与现实世界中的交换机功能相似。Bridge设备实例可以和Linux上其他网络设备实例连接，
既attach一个从设备，类似于在现实世界中的交换机和一个用户终端之间连接一根网线。当有数据到达时，Bridge会根据报文中的MAC信息进行广播、转发、丢弃处理。
	Bridge的功能主要在内核里实现，当一个从设备被attach到Bridge上时，相当于现实世界里交换机的端口被插入了一根连有终端的网线；
什么是open vSwitch：
	Open vSwitch的目标，是做一个具有产品级质量的多层虚拟交换机。通过可编程扩展，可以实现大规模网络的自动化（配置、管理、维护）。
	它支持现有标准管理接口和协议（比如netFlow，sFlow，SPAN，RSPAN，CLI，LACP，802.1ag等，熟悉物理网络维护的管理员可以毫不费力地通
	过Open vSwitch转向虚拟网络管理）。总的来说，它被设计为支持分布在多个物理服务器，例如VMware的vNetwork分布式vSwitch或思科的Nexu
	s1000V。
那么什么是虚拟交换？虚拟交换就是利用虚拟平台，通过软件的方式形成交换机部件。跟传统的物理交换机相比，虚拟交换机同样具备众多优点，
一是配置更加灵活。一台普通的服务器可以配置出数十台甚至上百台虚拟交换机，且端口数目可以灵活选择。例如，VMware的ESX一台服务器可以仿
真出248台虚拟交换机，且每台交换机预设虚拟端口即可达56个；二是成本更加低廉，通过虚拟交换往往可以获得昂贵的普通交换机才能达到的性能
，例如微软的Hyper-V平台，虚拟机与虚拟交换机之间的联机速度轻易可达10Gbps

今天任务：
	1、选定基于html5的vnc客户端novnc；备注：了解基本配置使用，运行； 后续待修改部分代码用以兼容web端；



L215

WebSocketProxy token_plugin
websockify/websocket.py  start_server


websockify/websocketproxy.py   L103
一个python字典结构

websockify/token_plugins.py  L13

self._targets

class TokenFile

我们先看看什么是CGI (Common Gateway Interface)。CGI是服务器和应用脚本之间的一套接口标准，目的是让服务器程序运行脚本程序，将程序的输出作为response发送给客户。通常来说，支持CGI的服务器程在接收到客户的request之后，根据request中的URL，运行对应的脚本文件。服务器会将HTTP request信息以及socket信息输入给脚本文件，也负责收集脚本的输出，并组装成为合法的HTTP response。利用CGI，我们可以充分发挥服务器的可编程性，动态的生成response，而不必局限于静态文件。
服务器和CGI脚本之间通过CGI标准作为接口。这样就可以让服务器与不同语言写的CGI脚本相配合，比如说使用Apache服务器与Perl写的CGI脚本，或者Python服务器与shell写的CGI脚本




离线恢复RBD的工具(麒麟云)
https://github.com/ceph/ceph/tree/master/src/tools/rbd_recover_tool


























































arp -s 设置绑定网关的mac地址


云主机存在的bug：自动备份的bug

 cmake -G "Unix Makefiles"  ~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp -DEXTERNAL_LIBCLANG_PATH=~/ycm_temp/llvmsrc/lib/libclang.so

 v是可以选定一行任意个字符的,V是行选定的,一次一整行，然后通过向下或向上移动光标而选定多行

按v选定后按=就是自动格式化代码,自动缩进,
 ctrl+r可以恢复



ceph内部
在这章中，将包含以下几点：
	1、ceph 对象
	2、crush算法
	3、归置组
	4、ceph存储池
	5、ceph数据管理

ceph底层：
	现在很清楚ceph的结构以及其核心组件；下一步，我们将着力于ceph在后台是如何工作的；组成一个ceph集群，有很多不为人知的基本元素；下面就让我们来详细了解他们吧；

对象：
	一个对象通常由数据和被绑定在一个唯一标识符的元数据组件构成；这些唯一标识符确保在同一集群中不存在相同标识符的其他对象，这样就保证了对象的唯一性；

	不同于以文件为基础的存储，由于文件大小被限制了大小；随着元数据的变化，对象可以是巨大的；一个对象中，数据存储了元数据信息，这些信息包括数据上下文以及数据内容；对象存储的元数据允许用户妥善管理和使用非结构化的数据；
	参照以下将病人病历作为对象存储的例子：

	一个对象是不会被任何类型和数量的元数据所限制的，这就使得可以给对象弹性的增加自定义类型的元数据，并保证了用户对数据的所有权。它没有使用一个目录层次结构或者存储树结构，相反，它是存储在有序存放了数10亿对象的地址空间中，对象可以存放在本地，但他们也可以存放在地理位置分开的地址空间；在一个连续的存储空间。这类存储机制有助于对象唯一的存储在整个集群中；任何应用程序可以通过RESTful API调用基于对象的唯一标识来从对象中检索数据；
	URL通过相同的方式，一个对象标识符作为对象的唯一指针。这些对象被存储在OSD(基于对象存储的设备)中通过多副本方式；这就使得数据具有高可用性。当ceph存储集群接受到来自客户端写数据的请求，它把数据作为对象存储起来，osd守护进程随后在osd文件系统中把数据写入一个文件；

	定位对象：
		在ceph中，每组数据都以对象的形式存储在内置存储池中，ceph pool是用来存储对象的逻辑分区，它提供了一种有序存储方式；我们将会本章的稍后部分进行详细的理解；现在让我们来深入理解对象，这个ceph存储的中的最小的数据存储单元；当一个ceph集群部署好之后，该集群创建了一些默认的存储池像data，metadata， rbd池；在元数据服务器部署在集群中其中一个节点上之后，元数据服务器在元数据池中创建对象。cephFS只有在元数据服务器部署之后才能正常工作；由于我们在本书讲解之前部署了ceph集群，下面让我们来测试这个对象：
			注意：在ceph Firefly发行版之后的Ceph Giant发行版中，元数据池和data 池只有在配置元数据服务器之后才会被创建，唯一默认的池只有rbd 池

		1、使用以下命令来检测ceph集群状态，你会发现三个池和一些对象：
			#ceph -s

		2、使用以下命令列出ceph集群中池名字，该命令将会显示默认的池，由于没有创建任何池，下面将列出三个池
			#rados lspools

		3、最后，列出从元数据池中列出对象的名，你将发现在这个池中由系统生成的对象
			#rados -p metadata ls


		CRUSH
			在过去的三年里，存储机制涉及到存储数据和存储元数据；所谓的元数据就是关于数据的数据，存储信息等，例如数据实际存储在那些存储节点以及哪些磁盘阵列上；每一次的新数据增加到存储系统中，它的元数据首先更新数据将要存储的物理位置，随后实际数据才被存储。这种工作流程在几兆大小存储规模到几百万兆字节的存储规模下证明是有效的。但是对于存储PB级或百亿字节数据呢？这一机制将可以肯定不适于上述存储，而且，它会存储单点的存储系统故障问题，不幸的是，如果你丢失了你的元数据，那么你就失去了你所有的数据。所以说，保证元数据的安全以及各种形式的灾备是至关重要的，通过在一个节点上多副本拷贝或者对这个数据以及元数据做多副本用以保证高程度的容错。元数据的管理始终是影响存储可扩展性、高可用性、以及整体性能的瓶颈;

			ceph是数据存储和管理的一次改革，它使用可控的扩展散列复制哈希算法；（crush Replication Unber Scalable Hashing）以及智能数据分发机制；crush算法是ceph皇冠上的一颗宝石（crush算法的重要性），它是ceph整个数据存储机制的核心；

			有别于此传统的存储系统依赖于存储和管理元数据和索引表，ceph使用crush算法来确定数据写入或读取的位置，不用于存储元数据，crush算法只有在需要的时候才计算元数据，这样就消除了用传统方式存储元数据对元数据的限制；
		crush 查找
			crush机制工作在元数据的计算工作只有在有需要时候才被分配并执行这一机制下；元数据的计算过程也可以认为是一个crush 查找，如今的计算机硬件可以快速并高效执行crush查找操作；

			关于crush查找算法的特点之一是它不是系统依赖；ceph为客户端提供了良好的扩展执行触发式元数据计算，即利用自身的系统资源执行crush 查找，从而消除了中心查找；

			对于一个有读写操作的ceph集群，客户端首先是要联络上ceph monitor和简索出一个集群map的副本；集群map使得客户端了解ceph集群的状态和配置信息，数据被通过结合池名字以及id转换为对象；为了得到在需要的ceph池中生成一个归置组，对象将被哈希成多个规则组，经计算得到的归置组通过crush 查找来确定存储和检索数据的主osd位置；在计算出精确的osd id号之后，客户端直接连接上该osd并开始存储数据，所有这些操作都是客户端进行的，因此它不影响集群性能；

			一旦数据被写入主OSD，同一节点上执行一次crush查找操作和计算两次规则组和OSD定位,
			由此，数据利用集群的高可用被副本存储到集群中；

			下面看看一个crush查找算法和osd中规则组的例子：
				首先，对象名称和集群规则组号是应用了哈希函数和基于池id，归置组id（PGID）来生成的，其次，crush查找基于PGID来查找到主osd和次osd来写入数据；

			crush压缩
				crush算法是一种让完全让用户了解和配置的设施；它为所有组件维护这一个嵌套层次结构；crush算法设置列表包括磁盘，节点、排、开关、电源电路、机房、数据中心等等，这些组件被称为破坏区和crush桶；crush map列出了可用桶设置的物理位置；包括了一系列的规则来告之crush如何为不同的池中复制数据；下来的图将展示crush如何在基本物理设备进行查找；

	iftop：查看网络问题

	10字符任意病毒
		210.14.78.77


与迁移相关的最主要的是 .vmx 文件和 .vmdk 文件。
arp -n 
tcpdump -i eth1 icmp -en


vmware 转到kvm，转vmx到xml：
	http://blog.csdn.net/epugv/article/details/15504029

http://www.linuxyunwei.com/2013/02/%E8%BF%81%E7%A7%BBvmware%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%88%B0kvm/
qemu-img convert -f vmdk   -O qcow2 Win2008R2.vmdk Win2008R2.img 
Vmware2libvirt –f centos-H.vmx > libvirt.xml

搭建openvpn：
https://docs.ucloud.cn/software/vpn/OpenVPN4Ubuntu.html


openvpn：openssl openssl-dev gcc
翻墙：
http://www.xiaohui.com/dev/server/20070514-install-openvpn.htm


出现的无法启动的信息
[root@mail ~]# cat /var/log/openvpn.log
Sat Sep 12 20:36:36 2009 OpenVPN 2.0.9 i686-pc-linux [SSL] [EPOLL] built on Sep 12 2009
Sat Sep 12 20:36:36 2009 WARNING: --ifconfig-pool-persist will not work with --duplicate-cn
Sat Sep 12 20:36:36 2009 Cannot open dh1024.pem for DH parameters: error:02001002:system library:fopen:No such file or directory: error:2006D080:BIO routines:BIO_new_file:no such file


cp  cp  /root/openvpn-2.0.9/sample-keys/*  keys/

openvpn故障：
http://www.metsky.com/archives/646.html

[i for  i in range(10) if i%2 ==0]
[0, 2, 4, 6, 8]

seq = ["one", "two", "three"]
for  i, element  in enmerate(seq):
	seq[i] = '%d : %s' %(i, seq[i])

seq
['0: one', '1:two', '2: three']

python修饰器：

def  login_required(fn):
    def ff():
        #if user.is_login():
        if True:
            print "0000"
            fn()
        else:
            pass
    return ff

@login_required
def A():
    print "AAAAA"

@login_required
def B():
    print "BBBB"
    #pass

if __name__ == "__main__":
    A()   #执行过程，将函数A作为参数传递给修饰器login_required()，修饰器必须定义一个函数，并返回该函数；否则报错
    #B()

在方法A上边写一个@修饰符，调用方法A的时候会调用修饰符后边的方法B，方法B以A方法为参数，而且需要返回一个可调用的对象，这个可调用的对象会使用A方法提供的参数执行。看这个例子：

#!/usr/bin/env python  
  
def a(fn):  
    print 'a'  
    def d(st):  
        print st+'d'  
    return d  
  
def b(fn):  
    print 'b'  
    return fn  
 
@a  
@b  
def c(st):  
    print st  
      
c('c')  

输出结果：bacd
我们调用c('c')的时候会先调用b(c)，b(c)打印字符"b"然后返回c，然后再调用a(c)，a(c)打印字符"a"，然后返回方法d，然后再执行d('c')，打印cd。

python生成器：
生成器是这样一个函数，它记住了上一次返回时在函数体中的位置，对生成器函数的第二次（第
n次）跳转至该函数的中间，而上次调用的所有局部变量都保持不变。

生成器不仅“记住”了它的数据状态；还“记住”了它在流控制构造在（命令式编程中，这种构造不只是数据值）中的位置。
特点：
	生成器是一个函数，而且函数的参数都会保留。
	迭代到下一次的调用时，所使用的参数都是第一次所保留下的，即是说，在整个所有函数调用的参数都是第一次所调用时保留的，而不是新创建的；
在python中，yield就是这样一个生成器；

yield 生成器的运行机制：
当你问生成器要一个数时，生成器会执行，直至出现 yield 语句，生成器把 
     yield 的参数给你，之后生成器就不会往下继续运行。 当你问他要下一个数时，他会从上次的状态。开始运行，直至出现yield语句，把参数给你，之后停下。如此反复
     直到退出函数；

yield的使用：
	在python中，当你定义一个函数，是哟嗬那个了yield关键字时，这个函数就是一个生成器。它的执行会和其他普通的函数有很多不同，函数返回的是一个对象，而不是你平常所用return语句那样，能得到结果值。如果想取得值，那得调用next()函数，如：






radosgw-admin user info --uid=<uid>
radosgw-admin bucket list  --uid=37


python 
split（）函数字符串分割函数
join（） 字符串连接函数
iptables-restore < /etc/iptables 

ceph osd把所有的数据都看做对象来存储在一个平面空间（就是说没有目录结构层次），
一个对象拥有集群范围唯一标识符，二进制数据以及由一对键值对构成的元数据；
ceph在其客户端定义了数据分布，the Ceph block device将一个块设备镜像映射到集群里存储的一系列对象上


包含唯一的ID，数据和名称/值配对元数据的对象可以代表
结构化和非结构化数据，以及传统和领先的数据存储接口。

一个Ceph的存储集群可容纳大量的Ceph节点的有效无限
可扩展性，高可用性和性能。每个节点利用商用硬件和
智能Ceph的守护进程，相互沟通（node之间）：
1、存储和检索数据；
2、复制数据；
3、监控和报告集群健康状态（心跳）
4、动态地重新分发数据（回填）
5、确保数据的完整性（洗涤过程）
6、从故障中恢复；

从ceph客户端读写数据的接口看，ceph存储集群看起来像一个简单的存储数据的池子；
然而，存储集群通过一种对客户端接口看来是完全透明的方式执行很多复杂的操作；
ceph 客户端和ceph OSD均使用了crush算法，以下各节就如何使CRUSH的Ceph无缝地执行
这些操作细节。

池：
ceph存储集群把数据对象存储在一种叫做pool的逻辑分区上。您可以创建池
对于特定类型的数据，如用于块设备，对象网关，或只是简单地以分离用户。

从Ceph的客户端的角度来看，存储群集是很简单的。当Ceph的客户端读取
或向其写入数据（即，称为I / O上下文）时，它总是连接到ceph存储集群中的
其中一个存储池，客户端指定的池的名称，用户和秘密密钥，所以池就好象充当
与访问控制数据对象的逻辑分区；

事实上，一个CEPH池不仅用于存储数据对象的一个逻辑分区，它还扮演了一个关键角色
即如何在Ceph的存储集群分布和存储数据，然而这些复杂的操作对ceph客户端来说都是
可见的。
ceph 池定义如下：
	池类型：在早期的ceph版本中，池只是简单地为对象维护多、深拷贝副本，如今
Ceph的可以维护对象的多个副本，以及使用擦除编码。由于保证数据持久性的方式不同于深拷贝以及编码，Ceph的支持池类型。池类型是完全透明的客户端。

缓存层：
在Ceph的早期版本，客户端只能数据直接写入到OSD。如今，
CEPH还可以支持高速缓存层和后备存储层。缓存层是由一池
较高的成本/性能更高的硬件，如固态硬盘。后备存储层是
主存储池。高速缓存层次加快读取和写入的高性能运算
硬件。缓存层（及其池名称）是完全透明的客户端。

规则组：在艾字节规模的存储集群，一个Ceph的池可以存储数百万数据对象或更多。
由于ceph必须处理数据持久性（副本或删除代码块），擦洗，复制，再平衡和恢复，
每个对象的基础上管理数据呈现可伸缩性和性能瓶颈。 
Ceph通过规则组来屏蔽池解决这一瓶颈， crush算法分配每个对象一个规则组，
每个规则组对应一组OSD;

CRUSH规则设置：高可用性，耐用性和性能都在Ceph的极为重要的。
crush算法计算出存储对象的归置组和OSD中的活跃组（acting set即[0, 5, 6]）
（0，是组primary osd id， 5，6是副本OSD ID）；
CRUSH也扮演其他重要作用：即，crush可以识别失败域和性能域
（即，类型的存储介质以及节点的，机架，行等包含其中）。 CRUSH使客户能够跨越
失败域写入数据（房间，机架，行等），这样，如果一个簇的大粒度部分失败（例如，齿条），该
集群仍然可以在降级状态下运行，直至其恢复。 CRUSH使客户能够写入数据
到特定类型的硬件（性能域），如固态硬盘，固态盘硬盘
期刊，或者与同一驱动器上的数据刊物硬盘。crush规则集
确定出现故障域和域表现为池。 
高可用性：
CEPH提供高耐用性的数据在两个方面：第一，副本池将多个存储
对象的使用CRUSH故障域的深层副本以物理方式分离一个数据对象
从另一个副本（即副本获得分配到单独的硬件）。这增加了耐用性
在硬件故障。其次，擦除编码池存储每个对象KM块，其中，
K代表数据块，M表示编码块。总和表示的数
的OSD用于存储对象​​和M个值表示的OSD可以失败的数目
仍然应该恢复的的OSD的M号失败的数据。

从客户的角度来看，Ceph的是优雅而简洁。客户端简单的读取和写入
池。然而，池在维护数据耐久性，性能和高可用性起到了重要作用；

认证：
为了识别用户，并防止人在这方面的中间人攻击，Ceph的提供其cephx
认证系统进行身份验证的用户和守护进程；

cephx协议并没有解决数据加密传输（例如，SSL / TLS）或
在其他形式的加密

Cephx使用共享秘密密钥进行认证，这意味着在客户端和监视器群集
有客户端的秘密密钥的副本。认证协议是这样的，双方都能够
证明给对方，他们有钥匙的副本，而无需实际透露出它。这提供了相互
认证，这意味着该集群是确保用户拥有的秘密密钥，并且用户
确保集群拥有密钥的副本。


PLACEMENT GROUPS (PGS);归置组
在整个集群中，ceph 将池通过伪随机的均匀分布到每个pgs。
（即每个池都被分成一块一块的；）
crush算法将每个对象指定到一个安置组，并且赋予每
安置组到OSD set，（在ceph client与存储了对象副本的osd之间创建的间接层）
如果ceph client知道ceph osd中存在的对象，在将在osd与client之间存在紧耦合；
反之，crush算法动态的分配每个对象到一个归置组，再将归置组分配到一组osd；
当新的osd加入或集群中的osd失效时候，这个间接层使得ceph存储集群开始动态重平衡；
通过管理在成百上千个归置组上下文中的数以万计的对象，ceph集群可以增长、缩小、
从失败中有效地恢复；


一个pool中只有很少的pg，这个“少”是相对于整个集群的大小而言的，ceph将很多数据
存放在一个pg里面，这样使得ceph性能不是那么良好；而当一个pool中有太多的pg时；
这个“多”也是想对于整个集群而言的，ceph osd将消耗太多的内存和cpu资源导致ceph
整体性能不好；因此设置在每个pool中设置一个合适的pg值以及在osd中限制pg值的上限，
对ceph的整体性能是十分关键的；

crush算法：
ceph为每个pool都分配了一个crush 规则组；当客户端在pool中存储或检索数据时；
ceph在一个规则下指定crush规则组、规则、以及顶层bucket来存储或检索数据；

由于ceph执行crush规则，ceph标识包含pg主osd成一个对象；
这就使得client直接连接osd并开始读写数据；

为了映射pgs到osds上，一个crush map定义了bucket type的分级列表；
（这些类型是在crush map中定义的产生的），
创建这些bucket层级结构的目的是；将故障域从性能域中分离，例如驱动型偏析的叶节点，
主机，机箱，机柜，配电单元，豆荚，行，室和数据中心。

除了叶节点代表的OSD，层次结构的其余部分是任意的，并且
您可以根据自己的需要确定，如果默认类型不适合您的要求。
CRUSH支持有向无环图，模拟的Ceph的OSD节点，通常在一
层次。所以，你可以支持多个层次结构，在一个CRUSH映射多个根节点。
例如，您可以创建固态硬盘层次的缓存层，与带有SSD日志的硬盘驱动器的层次结构等；

I/O操作；
CEPH客户端从CEPHmonitor检索crushmap，以及执行I/O在pool中的对象。
pool crush 规则组和pgs的数量是决定
CEPH将如何放置数据的主要因素。最新版本的cluster map中，客户端知道所有的
monitor以及集群中所有 的osds。然而，客户端不知道对象的位置。

客户端唯一需要的输入是对象ID和池名称。这很简单：CEPH存储
在某名的pool中存储数据（例如，“利物浦”）。当客户想要存储命名对象（例如，“约翰”
“保罗”，“乔治”，“林檎”等）中在一个pool中，利用对象名、散列码，该pool中的PG值和
池名称作为输入 通过crush计算为pg得到pg组ID以及主osd；

Ceph clients use the following steps to compute PG IDs.
1、1. The client inputs the pool ID and the object ID. (e.g., pool = "liverpool" and object-id = "john")

2、CRUSH takes the object ID and hashes it.

3、CRUSH calculates the hash modulo of the number of PGs. (e.g., 58) to get a PG ID

4、CRUSH calculates the primary OSD corresponding（对应的） to the PG ID.

5、The client gets the pool ID given the pool name (e.g., "liverpool" = 4)

6、The client prepends（预先设置） the pool ID to the PG ID (e.g., 4. 58).

7、The client performs an object operation (e.g., write, read, delete, etc.) by communicating
directly with the Primary OSD in the Acting Set.（活跃组）

ceph pg  dump  //获取ceph集群的所有活跃组；
ceph pg map  0.3f   //查看活跃组

rados -p data ls (这里会列举pool中的对象名称)
//通过object name查看活跃组：
//ceph osd map {pool-name} {object-name}

ceph osd map data test.txt

apt-get install libzero-ice34-dev
apt-get install libmysqlclient-dev

old_storage_type;   
storage_cluster;

dest_storage_cluster
dest_storage_type


叶子节点：没有子节点的节点；

Twisted学习：
多线程异步：
在异步编程模型与多线程模型之间还有一个不同：在多线程程序中，对于停止某个线程启动另外一个线程，其决定权并不在程序员手里而在操作系统那里，因此，程序员在编写程序过程中必须要假设在任何时候一个线程都有可能被停止而启动另外一个线程。相反，在异步模型中，一个任务要想运行必须显式放弃当前运行的任务的控制权。这也是相比多线程模型来说，最简洁的地方。
值得注意的是：将异步编程模型与同步模型混合在同一个系统中是可以的。但在介绍中的绝大多数时候，我们只研究在单个线程中的异步编程模型。

为什么要阻塞一个任务呢？最直接的原因就是等待I/O的完成：传输数据或来自某个外部设备。一个典型的CPU处理数据的能力是硬盘或网络的几个数量级的倍数。因此，一个需要进行大I/O操作的同步程序需要花费大量的时间等待硬盘或网络将数据准备好。正是由于这个原因，同步程序也被称作为阻塞程序。

mstsc /v:ip /admin

mstsc /v:210.14.78.113:9530  /admin

VIR_MIGRATE_LIVE:Do not pause the VM during migration
VIR_MIGRATE_PEER2PEER：Direct connection between source & destionation hosts
VIR_MIGRATE_TUNNELLED:Tunnel migration data over the libvirt PRC channel


ceph osd reweight OSDID NEW-WEIGHT


基础篇
引言
第一章：ceph的前世今生
1.1 ceph的诞生
1.2 ceph的市场前景
第二章：ceph的根基-RADOS
2.1 ceph与分布式文件系统
2.2 RADOS的组成
2.2.1 MON简介
2.2.1 OSD简介
2.2.1 MDS简介
2.3 快速搭建RADOS环境
2.4 LIBRADOS介绍
2.5LIBRADOS的C语言demo
2.6LIBRADOS的PYTHON语言demo
2.7LIBRADOS的JAVA语言demo
第三章：ceph的灵魂-CRUSH
3.1 CRUSH解决了什么问题
3.2 CRUSH基本原理
3.2.1 object->PG原理
3.2.1 PG->OSD原理
3.2.1 PG与POOL的关系
3.3 CRUSH原理验证（新建pool，上传object，搞清楚RADOS里面，object与pool、PG、OSD的映射关系）

第四章：ceph的图形化管理
4.1 Calamari介绍
4.2 Calamari快速安装
4.2 Calamari基本操作
第五章：ceph的性能与测试
5.1 需求模型与设计
5.2 硬件选型
5.3 性能调优
5.3.1 硬件层面
5.3.2 操作系统
5.3.3 网络配置
5.3.4 ceph配置
5.4 ceph的测试
5.4.1 cephfs的测试(iozone)
5.4.2 rbd的测试(fio)
5.4.3 RGW的测试(cosbench)
5.4.4 RADOS的测试(rados-bench)
第六章：ceph的运维与排错
6.1 ceph日常运维经验分享
6.2 ceph常见错误分享
6.1 cephfs应用案例
6.2 rbd应用案例
6.3 rgw应用案例


中级篇
第七章：ceph的三种存储形式
7.1 CEPHFS文件存储介绍
7.1.1 MDS介绍
7.1.2 快速搭建CEPHFS环境
7.1.3 CEPHFS的应用场景
7.2 RBD块存储介绍
7.2.1 RBD介绍
1)LIBRBD介绍
2)KRBD介绍
7.2.2 常见RBD CLI操作demo
7.2.3 RBD的应用场景
7.3 RADOS GATEWAY对象存储介绍
7.3.1 RGW介绍
7.3.2 快速搭建RGW环境
1)RGW与S3和swift接口兼容情况
2)S3 GUI调用demo
3)S3 CLI调用demo
4)S3 python调用demo
7.3.3 RGW的应用场景
第八章：cephfs在大数据中的应用例：RBD在虚拟化中的应用
8.1 用CEPHFS替代HDFS
8.1 ceph与KVM的整合
8.2 ceph与XEN的整合
8.3 ceph与openstack的整合
8.4 ceph与cloudstack的整合
8.5 基于rbd的iscsi搭建
第十章：RGW在互联网中的应用
9.1 网盘方案：RGW与owncloud的整合
9.2 备份方案：ceph与zmanda的整合
9.3 RGW的异地同步方案
9.4 RGW的多媒体转换网关设计


高级篇
第十章：CRUSH MAP的设计
10.0 一致性hash算法以及crush算法深入
10.1 CRUSH MAP基本组成
10.2 高可靠CRUSH MAP设计实例讲解
10.3 SSD与SATA混合下的实例讲解
10.3.1 SATA和SSD zone的划分
10.3.2 主OSD副本在SSD，其他在SATA的策略实现
第十一章：CACHE POOL和EC
11.1 CACHE POOL原理与应用场景
11.2 CACHE POOL搭建
11.3 EC原理与应用场景
11.4 EC搭建
第十二章：ceph-deploy的二次开发
12.1 ceph-deploy的构架介绍
12.2 扩展ceph-deploy模块实现对iscsi服务的安装
第十三章：calamari的二次开发
13.1 calamari的构架介绍
13.2 saltstack模块开发
13.2 django-resful接口的封装


<disk type='network' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <auth username='libvirt'>
        <secret type='ceph' uuid='cb936eba-e328-4fce-932e-7117a0dbbdbf'/>
      </auth>
      <source protocol='rbd' name='kvm/3439hda.img'>
        <host name='192.168.8.18' port='6789'/>
      </source>
      <backingStore/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>



宽惠迁移：

210.14.78.77
210.14.78.76     //迁移机器
210.14.78.75     //迁移机器
210.14.78.74
这四台机器从114.80.119.160中上去；
连接113：210.14.78.113:9530，密码：khadmin@20130618
存储节点：192.168.99.115;    密码：khadmin
操作流程：

1、copy  XXX.vmx,  xxx-flat.vmdk  到我们的节点;
2、mv   xxx-flat.vmdk  xxx.vmdk
3、vmware2libvirt -f XXXX.vmx > libvirt.xml
4、qemu-img convert -f raw -O  qcow2 xxx.vmdk  xxx.img


eb42d7f1-5cbd-e304-f94d-2786785fff02
dd06e1d2-a39b-d9c4-d92b-0d97d98a8bb9





update `vhost_disk` set is_independence = 0 where  img_file = "3721hdb.img";
select  a.uuid, b.img_file from  `vhost` a, `vhost_disk` b where  a.id = b.vhost_id and b.is_dependence=1 



负载均衡：
	1、 Load  Balancing（负载均衡）


package one
type ReadWrite interface{
	Read(buf [] byte) (n int, err  error)
	Write(buf [] byte) (n int, err error)
}

package two
type IStream interface{
	Write(buf [] byte) (n int,  err error)
	Read(buf []  byte) (n int,  err error)
}

go语言最主要的特性：
1、自动垃圾回收
2、更丰富的内置类型
3、函数多返回值
4、错误处理
5、匿名函数和闭包
6、类型和接口
7、并发编程
8、反射
9、语言交互性

内存泄露的最佳解决方案是在语言级别引入自动垃圾回收算法（ Garbage
Collection，简称GC）。所谓垃圾回收，即所有的内存分配动作都会被在运行时记录，同时任何对
该内存的使用也都会被记录，然后垃圾回收器会对所有已经分配的内存进行跟踪监测，一旦发现
有些内存已经不再被任何人使用，就阶段性地回收这些没人用的内存。当然，因为需要尽量最小
化垃圾回收的性能损耗，以及降低对正常程序执行过程的影响，现实中的垃圾回收算法要比这个
复杂得多，比如为对象增加年龄属性等，但基本原理都是如此。

特殊的数据类型：
1、字典类型（map）
2、数据切片：一种可动态增长的数组；类似c++中的vector；

多返回值：
func getName()(firstName, middleName, lastName, nickName string){
	return "may", "M", "chen", "Babe"
}

并不是每一个返回值都必须赋值，没有被明确赋值的返回值将默认的空值。
如果开发者只对该函数其中的某几个返回值感兴趣的话，也可以直接用下划线作为占位符来
忽略其他不关心的返回值。下面的调用表示调用者只希望接收lastName的值，这样可以避免声
明完全没用的变量：
_, _, lastName, _ := getName()


错误处理：
关键字：defer、panic、和recover

匿名函数和闭包：
在Go语言中，所有的函数也是值类型，可以作为参数传递。 Go语言支持常规的匿名函数和
闭包，比如下列代码就定义了一个名为f的匿名函数，开发者可以随意对该匿名函数变量进行传
递和调用：
f := func(x, y int) int {
	return x+y
}


c++中的变量相与：
if(info.flag&DATA_HOST)


负载均衡：
	1、以集群为单位，即同一个集群内节点的负载是均衡的；
	2、负载的指标：
		a、以cpu利用率为指标；
		b、以内存使用比例为指标；
		c、以load  average为指标；
	3、触发重平衡条件：
		 a、各指标触发值；
		 b、触发时间；

ceph：
	journal的内容，在内存里面，在一定周期，提交到filestore
	副本池就是数据有多个复制版本，纠删池就是数据被采用纠删编码的方式分块存放
	按照min_size，写入journal后，就返回client
	例如，一般情况是size 3, min_size 2，主需要写入两个osd的journal，就返回


自我管理操作：p15/24
ceph集群自动执行了大量自我监控和管理操作；例如:ceph osd检测集群的健康状态和
报告给ceph  monitor；通过crush算法将object发配到pgs以及pgs到一组osd；
ceph osd利用crush算法重平衡或者动态恢复故障域；
下面介绍一些ceph操作：
心跳机制：
	ceph osd加入集群并将自己的状态报告给ceph monitor；在最低水平下，
ceph osd的状态是up或者down反应了他们是否在运行以及是否能为ceph client
提供服务；如果一个ceph osd在集群中是down状态，则表明该osd是失效的
如果一个osd没有运行（比如它崩溃了）,ceph osd不能通知到ceph monitor自己是
down的状态；ceph monitor可以ping ceph 守护进程来确定它是否在运行；
然而，ceph 也授权给ceph osd，让ceph osd去确定他们相邻的osd是否down状态；
以及更新cluster map，并上报给ceph monitor。这 就意味这ceph monitor
可以保持轻量级进程；

对等：
Ceph的OSD守护程序执行对等，这是使所有存储的屏上显示的过程中
放置集团（PG）到有关的所有对象的状态协议（及其元数据）的
该PG。对等的问题，通常自行解决。

注意：	
当Ceph monitor认可存放了pgs的OSD状态，这不
并不意味着pgs总是拥有最新内容

当Ceph要将pg存放到osd的活跃组中时，它们被分为初级，次级，等等。
按照惯例，活跃组中的第一个osd被认为主osd，并且是
负责协调对等过程的每个pg在那里充当主，
并且是唯一的OSD，这将接受客户端发起的写入对象到一个给定的pg；

一组osd负责一个pg时，对于这组osd，我们称之为an Acting set（活跃组）；
一个活跃组指的是对pg负责的ceph osd守护进程；ceph osd守护进程
是那些不总是up状态的活跃组中的一部分；当在活跃组中的eosd是up状态时；

这些up状态的组是很有重要作用的，因为ceph可以重新remap pgs到其他fail
的osd；

例如：
在这些包含osd.25, osd.32， osd.61 的活跃组中，第一个osd是osd.25, 它是
主osd，如果这osd 失效了，次级osd(osd.32) 将变为主osd；然后osd.25将被移出up
状态的活跃组中；

重平衡和恢复：


.执行如下命令查看后端网卡所属ovs。
ovs-dpctl show
发现统计到该ovs上lost值为0，表明ovs没有丢包，继续查看连接在该ovs上的物理网卡聚合为bond28。

ovs-ofctl show br0

   查看虚拟交换机ovsbr0的信息：
$ovs-ofctl  show ovsbr0
2.    查看ovsbr0上各交换机端口的状态
$ovs-ofctl  dump-ports ovsbr0
3.    查看ovsbr0上的所有流规则
$ovs-ofctl  dump-flows ovsbr0
4.    丢弃从2号端口发来的所有数据包
$ovs-ofctl  add-flow ovsbr0
idle_timeout=0,in_port=2,actions=drop
注意：此处的in_port是指虚拟网卡(vif,tap)的号码，并非传输层的端口号(如www:80,ftp:21,22等)，通过ovs-ofctl  show ovsbr0可查得端口号，传输层的端口号有tp_src/tp_dst指定。
5.    删除条件字段中包含in_port=2的所有流规则
$ovs-ofctl  del-flows ovsbr0 in_port=2
6.    丢弃所有收到的数据包
$ovs-ofctl  add-flow ovsbr0
dl_type=*,nw_src=ANY,action=drop

网络虚拟化或者软件定义网络（Software Defined Network；  sdn
实现数据交换和OpenFlow流表功能，是OVS的核心


ovs：
在虚拟化平台下，ovs可以为动态变化的端点提供2层交换功能；很好的控制虚拟网络中的
访问策略、网络隔离、流量监控等等；
ovs提供了对OpenFlow协议的支持，用户可以使用任何支持OpenFlow协议的控制器对ovs进行
远程管理控制；在ovs中的非常重要的概念：
1、Bridge:Bridge代表一个从以太网交换机（Switch），一个主机中可以创建一个或者多个
Bridge设备；
2、Port：端口与物理交换机的端口概念类似，每个Port都隶属于一个 Bridge；
3、Interface:连接到Port的网络接口设备（在主机端），在通常情况下，Port和Interface是
一对一的关系,(相当于一根网线连接的两端)；只有在配置Port为bond模式后，Port和Interface
才是一对多的关系；注：bond模式即级联模式；
4、Controller： OpenFlow控制器，OVS可以同时接受一个或多个OpenFlow控制器的管理；
5、datapath：在ovs中，datapath负责执行数据交换，也就是把从接收端口收到的数据包在表中
进行匹配，并执行匹配到的动作；
6、Flow table：每个datapath都和一个“flow table”关联，当datapath接收到数据之后，
ovs会在flow table中查询可以匹配的flow，执行对应的操作，例如转发数据到另外的端口；

在OVS中，给一个交换机，或者说一个桥，用了一个专业的名词，叫做DataPath！

查看datapath的信息：
ovs-dpctl show
查看交换机中所有的table：
ovs-ofctl dump-tables br0
查看交换机中所有的流表项：
ovs-ofctl dump-flows  br0;
查看交换机上端口信息：
ovs-ofctl show br0;

除此之外还可以通过Floodlight 管理 OVS


ovs流规则管理
每条流规则由一系列字段组成，分为基本字段、条件字段和动作字段三部分。
基本字段包括:

    生效时间 duration_sec
    所属表项 table_id
    优先级 priority、
    处理的数据包数 n_packets
    空闲超时时间 idle_timeout 等空闲超时时间 idle_timeout 以秒为单位，超过设置的空闲超时时间后该流规则将被自动删除，空闲超时时间设置为 0 表示该流规则永不过期，idle_timeout 将不包含于 ovs-ofctl dump-flows brname 的输出中。

条件字段包括:

    输入端口号 in_port
    源目的 mac 地址 dl_src/dl_dst
    源目的 ip 地址 nw_src/nw_dst
    数据包类型 dl_type
    网络层协议类型 nw_proto

这些字段可以任意组合，但在网络分层结构中底层的字段未给出确定值时上层的字段不允许给确定值，即一 条流规则中允许底层协议字段指定为确定值，高层协议字段指定为通配符(不指定即为匹配任何值)，而不允许高层协议字段指定为确定值， 而底层协议字段却为通配符(不指定即为匹配任何值)，否则，ovs-vswitchd 中的流规则将全部丢失，网络无法连接。
动作字段包括正常转发 normal、定向到某交换机端口 output：port、丢弃 drop、更改源目 的 mac 地址 mod_dl_src/mod_dl_dst 等，一条流规则可有多个动作，动作执行按指定的先后顺序依次完成。



3.2 流规则：

每条流规则由一系列字段组成，分为基本字段、条件字段和动作字段三部分：

基本字段包括生效时间duration_sec、所属表项table_id、优先级priority、处理的数据包数n_packets，空闲超时时间idle_timeout等，空闲超时时间idle_timeout以秒为单位，超过设置的空闲超时时间后该流规则将被自动删除，空闲超时时间设置为0表示该流规则永不过期，idle_timeout将不包含于ovs-ofctl dump-flows brname的输出中。

条件字段包括输入端口号in_port、源目的mac地址dl_src/dl_dst、源目的ip地址nw_src/nw_dst、数据包类型dl_type、网络层协议类型nw_proto等，可以为这些字段的任意组合，但在网络分层结构中底层的字段未给出确定值时上层的字段不允许给确定值，即一条流规则中允许底层协议字段指定为确定值，高层协议字段指定为通配符(不指定即为匹配任何值)，而不允许高层协议字段指定为确定值，而底层协议字段却为通配符(不指定即为匹配任何值)，否则，ovs-vswitchd 中的流规则将全部丢失，网络无法连接。其中dl是datalink的缩写，nw是network的缩写，tp是transport的缩写。

动作字段包括正常转发normal、定向到某交换机端口output：port、丢弃drop、更改源目的mac地址mod_dl_src/mod_dl_dst等，一条流规则可有多个动作，动作执行按指定的先后顺序依次完成。

什么是Tap/Tun、网桥
在计算机网络中，TUN与TAP是操作系统内核中的虚拟网络设备。不同于普通靠硬件网路板卡实现的设备，这些虚拟的网络设备全部用软件实现，并向运行于操作系统上的软件提供与硬件的网络设备完全相同的功能。
TAP 等同于一个以太网设备，它操作第二层数据包如以太网数据帧。TUN模拟了网络层设备，操作第三层数据包比如IP数据封包。
操作系统通过TUN/TAP设备向绑定该设备的用户空间的程序发送数据，反之，用户空间的程序也可以像操作硬件网络设备那样，通过TUN/TAP设备发送数据。在后种情况下，TUN/TAP设备向操作系统的网络栈投递（或“注入”）数据包，从而模拟从外部接受数据的过程。
服务器如果拥有TUN/TAP模块，就可以开启VPN代理功能。
虚拟网卡TUN/TAP 驱动程序设计原理：

tc qdisc del dev eth0 root

也就是说每次只能同时使用localtime()函数一次，要不就会被重写！
因此localtime()不是可重入的。同时libc里提供了一个可重入版的函数localtime_r()；


//开启osd
service ceph-osd start id=0；
//关闭osd
service ceph-osd stop id=0；
//关闭一个节点上的多osd
service ceph-osd-all stop


vm_mon.rt_node表

grep的--exclude-dir=参数就是为了排除某个目录的，即不包含等号后面的目录，所以我们可以利用此参数去掉.svn的隐藏目录。
grep -nr  --exclude="tags"  --exclude-dir=".svn"  "VmOffNetwork"  * 
export GREP_OPTIONS="--exclude=tags --exclude-dir=\.svn"

#ifndef __LOAD_BALANCE_H__
  2 #define __LOAD_BALANCE_H__
  3 
  4 #include <IceUtil/Thread.h>
  5 #include <pthread.h>
  6 
  7 class LoadBalance: public IceUtil::Thread
  8 {
  9 
 10 };
 11 
 12 
 13 #endif
 14 


2、undefined reference to `vtable for ...'

    产生问题的原因是::

            基类中声明了virual 方法（不是纯虚方法），但是没有实现。在子类中实现了，当子类创建对象时，就出现这个问题。

     class   Base

     {

                 public:

                      virtual  int run();

     };

     class  Test:public Base              //必须实现run才可以

     {

                public:

                      Test()

                      {

                      }

     };

     在Test中必须实现run，否则Test不能创建对象，创建对象，编译时会报 undefined reference to `vtable for ...' 这种错误。

     如果是“纯虚函数”，即 virtual int run() = 0;编译时会报" because the following virtual functions are pure within 'Test' "

     如果在基类中实现了虚函数，或者在子类中实现纯虚函数或虚函数，就不会报错。

     包含有虚函数的类，是不能创建对象的。

解决 令人生厌的 multiple definition of

慢慢的自己写的代码 ，有很多了，自己总是加入一些新的东西，并一点点地完善着它，后来想编译一下，看看能否运行了，却出现了几十行的“multiple   definition   of ”

我把所有的全局变量写在一个global.h里，然后其他文件都include 了它 ，于是出现了 multiple   definition   of  .....

（编译器 gcc ) 

后来在网上搜到了很多类似的错误，大家各有各的烦心事。

我的代码结构

main.cpp
#include "global.h"

WinMain(....)
{
...
}

file_1.cpp
#include "global.h"
....

file_2.cpp
#include "global.h"

...

由于工程中的每个文件都是独立的解释的，
（即使头文件有
#ifndef _x_h 
....
#enfif   )

在其他文件中只要包含了global.h 就会独立的解释,然后生成每个文件生成独立的标示符。在编译器连接时，就会将工程中所有的符号整合在一起，由于，文件中有重名变量，于是就出现了重复定义的错误。


下面是解决方法：

在global.c（或.cpp)  中声明变量，然后建一个头文件global.h 在所有的变量声明前加上extern ...
如 extern HANDLE ghEvent;
注意这儿不要有变量的初始化语句。

然后在其他需要使用全局变量的 cpp文件中包含.h 文件而不要包含 .cpp 文件。编译器会为global.cpp 生成目标文件，然后连接时，在使用全局变量的文件中就会连接到此文件 。

select  a.type from `idc_route` a, `vhost_eth` b, `node_eth` c  where  a.id = c.idc_route_id and  b.node_eth_id = c.id  and b.id= VhostEthId;

企业培训(cp部分)：产品对比，部分文档资料，API的demo演示、ceph架构
产品对比：ceph与gluster
		对比项：架构方法、数据分布能力、IO测试、扩展性能、cache、抗逆能力、安装配置和维护、故障处理、
		gluster中遇到的问题：gluster 3.6.1莫名的cpu占用超高，最后卡机；
API的demo演示：
	1、块的新建、读、写、删除等基本操作；
	2、获取块信息、克隆、快照信息、快照创建、回滚等；
	3、异地灾备的原理说明；
ceph架构：
	1、ceph系统的层次结构及其pg状态名称解释：
	2、CEPH的工作原理及流程
	3、浅析RADOS组件以及分发策略–CRUSH算法并对比一致性hash算法
	4、FileJournal的意义；


今天任务：
	1、负载均衡，备注：vmc端收集同集群节点由负载、带宽情况以及同一节点上虚拟机的负载、带宽信息收集；





	一、归置组状态
1. Creating

创建存储池时,它会创建指定数量的归置组。ceph 在创建一或多个归置组时会显示 creating;创建完后,在其归置组的 Acting Set 里的 OSD 将建立互联;一旦互联完成,归置组状态应该变为 active+clean,意思是ceph 客户端可以向归置组写入数据了。

2. peering

ceph 为归置组建立互联时,会让存储归置组副本的 OSD 之间就其中的对象和元数据状态达成一致。ceph 完成了互联,也就意味着存储着归置组的 OSD 就其当前状态达成了一致。然而,互联过程的完成并不能表明各副本都有了数据的最新版本。

3. active

ceph 完成互联进程后,一归置组就可变为 active。active 状态通常意味着在主归置组和副本中的数据都可以读写。

4. clean

某一归置组处于 clean 状态时,主 OSD 和副本 OSD 已成功互联,并且没有偏离的归置组。ceph 已把归置组中的对象复制了规定次数。

5. degraded

当客户端向主 OSD 写入数据时,由主 OSD 负责把副本写入其余复制 OSD。主 OSD 把对象写入复制 OSD 后,在没收到成功完成的确认前,主 OSD 会一直停留在 degraded 状态。
归置组状态可以是 active+degraded 状态,原因在于一 OSD 即使没所有对象也可以处于 active 状态。如果一OSD 挂了,ceph 会把相关的归置组都标记为 degraded;那个 OSD 重生后,它们必须重新互联。然而,如果归置组仍处于 active 状态,即便它处于 degraded 状态,客户端还可以向其写入新对象。
如果一 OSD 挂了,且 degraded 状态持续,ceph 会把 down 的 OSD 标记为在集群外(out)、并把那些 down 掉的 OSD 上的数据重映射到其它 OSD。从标记为 down 到 out 的时间间隔由 mon osd down out interval 控制,默认是 300 秒。
归置组也会被降级(degraded),因为归置组找不到本应存在于归置组中的一或多个对象,这时,你不能读或写找不到的对象,但仍能访问其它位于降级归置组中的对象。

6. recovering

ceph 被设计为可容错,可抵御一定规模的软、硬件问题。当某 OSD 挂了(down)时,其内容版本会落后于归置组内的其它副本;它重生(up)时,归置组内容必须更新,以反映当前状态;在此期间,OSD 在recovering 状态。
恢复并非总是这些小事,因为一次硬件失败可能牵连多个 OSD。比如一个机柜的网络交换机失败了,这会导致多个主机落后于集群的当前状态,问题解决后每一个 OSD 都必须恢复。
ceph 提供了很多选项来均衡资源竞争,如新服务请求、恢复数据对象和恢复归置组到当前状态。osd recovery delay start 选项允许一 OSD 在开始恢复进程前,先重启、重建互联、甚至处理一些重放请求;osd recovery threads 选项限制恢复进程的线程数,默认为 1 线程;osd recovery thread timeout 设置线程超时,因为多个OSD 可能交替失败、重启和重建互联;osd recovery max active 选项限制一 OSD 最多同时接受多少请求,以防它压力过大而不能正常服务;osd recovery max chunk 选项限制恢复数据块尺寸,以防网络拥塞。

7. back filling

有新 OSD 加入集群时,CRUSH 会把现有集群内的归置组重分配给它。强制新 OSD 立即接受重分配的归置组会使之过载,用归置组回填可使这个过程在后台开始。回填完成后,新 OSD 准备好时就可以对外服务了。

8. remapped

某一归置组的 Acting Set 变更时,数据要从旧集合迁移到新的。主 OSD 要花费一些时间才能提供服务,所以它可以让老的主 OSD 持续服务、直到归置组迁移完。数据迁移完后,主 OSD 会映射到新 acting set。

9. stale

虽然 ceph 用心跳来保证主机和守护进程在运行,但是 ceph-osd 仍有可能进入 stuck 状态,它们没有按时报告其状态(如网络瞬断)。默认,OSD 守护进程每半秒(0.5)会一次报告其归置组、出流量、引导和失败统计
状态,此频率高于心跳阀值。如果一归置组的主 OSD 所在的 acting set 没能向监视器报告、或者其它监视器已经报告了那个主 OSD 已 down,监视器们就会把此归置组标记为 stale。启动集群时,会经常看到 stale 状态,直到互联完成。集群运行一阵后,如果还能看到有归置组位于 stale 状态,就说明那些归置组的主 OSD 挂了(down)、或没在向监视器报告统计信息。

二、找出故障归置组

一般来说,归置组卡住时 ceph 的自修复功能往往无能为力,卡住的状态细分为:

1. unclean

不干净:归置组里有些对象的复制数未达到期望次数,它们应该在恢复中。

2. inactive

不活跃:归置组不能处理读写,因为它们在等着一个持有最新数据的 OSD 再次进入 up 状态。

3. stale

发蔫:归置组们处于一种未知状态,因为存储它们的 OSD 有一阵子没向监视器报告了(由 mon osdreport timeout 配置)。

为找出卡住的归置组,执行:

?
1
ceph pg dump_stuck [unclean|inactive|stale]

获取pg分布：
ceph pg dump | awk ' /^pg_stat/ { col=1; while($col!="up") {col++}; col++ } /^[0-9a-f]+\.[0-9a-f]+/ { match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0; up=$col; i=0; RSTART=0; RLENGTH=0; while(match(up,/[0-9]+/)>0) { osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) } for(i in osds) {array[osds[i],pool]++; osdlist[osds[i]];} } END { printf("\n"); printf("pool :\t"); for (i in poollist) printf("%s\t",i); printf("| SUM \n"); for (i in poollist) printf("--------"); printf("----------------\n"); for (i in osdlist) { printf("osd.%i\t", i); sum=0; for (j in poollist) { printf("%i\t", array[i,j]); sum+=array[i,j]; poollist[j]+=array[i,j] }; printf("| %i\n",sum) } for (i in poollist) printf("--------"); printf("----------------\n"); printf("SUM :\t"); for (i in poollist) printf("%s\t",poollist[i]); printf("|\n"); }'

insert(std::map<int, NodeInfo>::value_type(1, tmp));


ceph中的4个map：osdmap、mdsmap、monmap、pg map、还有auth信息保存在mon端；


select vhost_eth.id, node_eth.idc_route_id 


"SELECT i.`id`, i.`name`, i.`pub_ip`, ii.`id` AS routeid, ii.`name` AS routename 
		FROM `node` n 
		LEFT JOIN `node_eth` i ON i.`node_id`=n.`id` 
		LEFT JOIN `idc_route` ii ON ii.`id`=i.`idc_route_id` 
		WHERE n.`control_ip`='$nodeip'";

//选择目标节点的线路
 id 	name 	pub_ip 	routeid 	routename
56 	eth0 	192.168.2.30 	1 	本地公网线路
57 	eth1 	192.168.8.30 	9 	本地内网线路

cluster_id=目标节点所在集群; route_id=目标线路ID user_id=当前用户


//获取指定节点上所以网卡的idc_route_id;
select b.id,  b.idc_route_id from  `node` a,  `node_eth` b where  b.node_id = a.id and  b.statue=1  and a.control_ip = "192.168.2.30"

select a.id,  from `vhost_eth` a, `vlan` b， `vhost` c where  c.id = a.vhost_id and a.vlan_id = b.id and b.idc_route_id =2 and c.uuid="a90bd1d2-da13-bb24-5192-b77931cb2700" 


SELECT i.`id` , i.`name` , i.`pub_ip` , ii.`id` AS routeid, ii.`name` AS routename
FROM `node` n
LEFT JOIN `node_eth` i ON i.`node_id` = n.`id`
LEFT JOIN `idc_route` ii ON ii.`id` = i.`idc_route_id`
WHERE n.`control_ip` = '192.168.2.30'

等待线程返回 int pthread_join(pthread_t thread, void **value_ptr); value_ptr存放线程返回值，线程返回值和线程函数返回值类型一样，也是void*


SELECT i.`id` , ii.`id` FROM `node` n  LEFT JOIN `node_eth` i ON i.`node_id` = n.`id` LEFT JOIN `idc_route` ii ON ii.`id` = i.`idc_route_id`
WHERE n.`control_ip` = '192.168.2.30'


SELECT a.id FROM `vhost_eth` a, `vlan` b, `vhost` c WHERE c.id = a.vhost_id AND a.vlan_id = b.id AND b.idc_route_id =1 AND c.uuid = "a90bd1d2-da13-bb24-5192-b77931cb2700"

node数据库密码：openstack

keystone tenant-create --name admin
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |                                  |
|   enabled   |               True               |
|      id     | 1f14a11f61be4520a7f8ef86670fbaa5 |
|     name    |              admin               |
|  parent_id  |                                  |
+-------------+----------------------------------+

root@node1:~# keystone tenant-create --name service
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |                                  |
|   enabled   |               True               |
|      id     | 0acfd48af0a64639a0ff679c5e1b850b |
|     name    |             service              |
|  parent_id  |                                  |
+-------------+----------------------------------+

openstack：
1、控制节点安装所有，计算节点只有nova-compute；
2、网络选择：
	nova-network还是neutron；
	nova-network比较简单，
	neutron功能强大


openstack所有数据库密码：openstack
rabbit guest密码： openstack
keystone ADMIN_TOKEN:openstack
keystone  admin user 密码：openstack
keystone  demo  user 密码：openstack
glance  密码：openstack


	
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
  IDENTIFIED BY 'openstack';
(crontab -l -u keystone 2>&1 | grep -q token_flush) || \
  echo '10 /usr/bin/keystone-manage token_flush >/var/log/keystone/keystone-tokenflush.log 2>&1' \
  >> /var/spool/cron/crontabs/keystone

keystone endpoint-create   --service-id  2503a30d9928474188f65e7707a0d6f9  --publicurl http://node1:5000/v2.0   --internalurl http://node1:5000/v2.0  --adminurl http://node1:35357/v2.0  --region regionOne




 






1.0.6版本：
192.74.235.97  //ok

122.136.32.5    //ok
198.2.192.193   //ok

211.101.15.196  //ok
59.188.252.102  //ok
59.188.252.101   //ｏｋ
192.74.245.161   //ok
198.200.46.193	 //ok
220.167.104.169		//ok
122.112.81.2    //ok
122.112.81.3  //ok

1.2.12版本
211.101.15.195
114.80.119.121 //

apt-get update

./start && tailf vmd.log
md5sum vmd
7684e1b37c7050dee0bf9e823ba8ed59  vmd

apt-get install libboost-thread-dev libboost1.46-dev 

 apt-get install  libcrypto++-dev
rm  /usr/lib/libgfapi.so.0 && ln -s  /usr/lib/libgfapi.so.0.0.0  /usr/lib/libgfapi.so.0


tar -C / -zxf   glusterfs-3.6.2.tgz  && rm  /usr/lib/libgfapi.so.0 && ln -s  /usr/lib/libgfapi.so.0.0.0  /usr/lib/libgfapi.so.0 && tar -C / -zxf curl.tgz

virsh list --all


CB1788E64B609BA74902优惠券

1、win2003R2-32Bit纯净版-----------偶现问题；
2、切换模板无法进入系统-----------主机xml被删除了系统盘驱动配置，

        if(virDomainUndefineFlags(dom,VIR_DOMAIN_UNDEFINE_SNAPSHOTS_METADATA)){
            if(virDomainUndefine(dom)){
                rt_con.errNum = -2;  
                rt_con.errMsg = "Failed to undefine domain:" + UUID;
                pErr = virConnGetLastError(conn);
                if(pErr){
                    rt_con.errMsg += pErr->message;
                }    
                virDomainFree(dom);
                virConnectClose(conn);  
                post_err(args_str("Failed to undefine domain:%s", UUID.c_str()), 20, 2, 1);
                cdn_log(LOG_VIRT).error(rt_con.errMsg);   
                return rt_con; 
            }    
        } 

      
valgrind --tool=memcheck --leak-check=full --show-reachable=yes  --track-origins=yes ./demo

librados用于在客户端用来访问 rados对象存储设备
ceph rados对象映射：
ceph osd map kvm  hw
api文档：
http://ceph.com/docs/master/rados/api/librados/

对象存储
Ceph文件系统中的数据和元数据都保存在对象中。 对于对象存储，通常的定义是：一个Object，由三部分组成（id，metadata，data），id是对象的标识，这个不必多说。所谓的metadata，就是key/value的键值存储，至于用来保存什么信息，由文件系统的语义定义。data就是实际存储的数据。

Ceph的对象，包括四个部分（id，metadata，attribute，data），在Ceph里，一个Object，实际就对应本地文件系统的一个文件，一个对象的attribute，也是key/value的键值对，其保存在本地文件系统的文件的扩展属性中。对象的metadata就是key/value的键值对，目前Ceph保存在google开源的一个key/value存储系统leveldb中，或者自己写的一个key/value 存储系统中。数据就保存在对象的文件中。对于一个对象的更新，都需要写日志中来保持一个Object数据的一致性（consistence），日志有一个单独的设备或者文件来保存。

文件扩展属性：lsattr 查看文件的扩展属性,
扩展属性（xattrs）提供了一个机制用来将《键/值》对永久地关联到文件，让现有的文件系统得以支持在原始设计中未提供的功能；

Ceph最大的特点是分布式的元数据服务器  通过CRUSH，一种拟算法来分配文件的locaiton，其核心是 RADOS（resilient automatic distributed object storage)，一个对象集群存储，本身提供对象的高可用，错误检测和修复功能。
其设计思想有一些创新点：

第一，数据的定位是通过CRUSH算法来实现的。

传统的，或者通常的并行文件系统，数据的定位的信息是保存在文件的metadata 中的， 也就是inode结构中，通过到metadata server上去获取数据分布的信息。而在Ceph中，是通过CRUSH 这个算法来提供数据定位的。

这和GlusterFS的思想是相同的，GlusterFS 是通过Elastic Hash，类似于DHT的算法实现的。这就有点像P2P存储，所谓的完全对称的存储，这种设计架构最大的优点是，其理论上可以做到 线性扩展的能力（line scale）。

在GlusterFS架构中，是完全去掉了metadata server，这就导致GlusterFS文件系统上的元数据操作，例如ls， stat操作非常慢，要去各个stripe的节点上收集相关的元数据信息后聚合后的结果。在Ceph中，为了消除完全的p2p设计，提供了metadata server 服务，提供文件级别的元数据服务，而元数据服务中的文件数据定位由CRUSH算法代替。

当有OSD失效，恢复或者增加一个新的OSD时，导致OSD cluster map的变换。Ceph处理以上三种情况的策略是一致的。为了恢复，ceph保存了两类数据，一个是每个OSD的一个version，另一个是PG修改的log，这个log包括PG修改的object 的名称和version。

当一个OSD接收到cluster map的更新时：

1）检查该OSD的所属的PG，对每个PG，通过CRUSH算法，计算出主副本的三个OSD

2）如何该PG里的OSD发生了改变，这时候，所有的replicate向主副本发送log，也就是每个对象最后的version，当primay 决定了最后各个对象的正确的状态，并同步到所有副本上。

3）每个OSD独立的决定，是从其它副本中恢复丢失或者过时的（missing or outdated）对象。 (如何恢复? 好像是整个对象全部拷贝，或者基于整个对象拷贝，但是用了一些类似于rsync的算法？目前还不清楚）

4）当OSD在恢复过程中，delay所有的请求，直到恢复成功。

Paxos算法选择出leader。

1.客户端输入池ID和对象ID。（例如，池=“liverpool”和对象ID =“john”） 
2.CRUSH取得的对象ID和散列它。
3.CRUSH计算散列模数的OSD。（例如，0x58）得到一个PG的ID。
4.CRUSH得到池ID池的名称（如“liverpool”= 4）
5.CRUSH预先考虑到PG ID对应池ID（例如，4.0x58）。


 关于池

Ceph的存储系统支持'池'，这是用于存储对象的逻辑分区的概念。池设置以下参数：

所有权/访问对象
对象副本的数目
放置组的数目
CRUSH规则集的使用
 


快照的作用主要是能够进行在线数据备份与恢复。当存储设备发生应用故障或者文件损坏时可以进行快速的数据恢复，将数据恢复某个可用的时间点的状态。快照的另一个作用是为存储用户提供了另外一个数据访问通道，当原数据进行在线应用处理时，用户可以访问快照数据，还可以利用快照进行测试等工作。所有存储系统，不论高中低端，只要应用于在线系统，那么快照就成为一个不可或缺的功能

synchronously  英 ['sɪŋkrənəslɪ] 


2880709218  企业QQ密码：cXp123456


    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='writeback'/>
      <source file='/data/kvm/2271hdb.img'/>
      <target dev='hdb' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='1'/>
    </disk>

     <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/data/iso/ubuntu-12.04.2-server-amd64.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>

    641f80b2-44e7


admin
admin0717Q

查看机器型号：
dmidecode | grep "Product Name" 


virsh snapshot-delete controller snapshot02 
virsh snapshot-list controller --tree 



2015-07-29 13:18:40.9 [13616][0]:LOG_VIRT:D:before waiting:52:54:00:4c:d1:9c;
2015-07-29 13:20:52.6 [13616][0]:LOG_VIRT:D:after waiting, rt:1, mac address: 52:54:00:4c:d1:9c, status:2, msg:



2015-07-29 14:45:24.7 [13616][0]:LOG_VIRT:D:before waiting:52:54:00:ef:97:ff;
2015-07-29 14:48:16.3 [13616][0]:LOG_VIRT:D:after waiting, rt:1, mac address: 52:54:00:ef:97:ff, status:2, msg:

L65

python的包管理工具：Easy Install 和IPython
django安装：
1、apt-get install setuptools
2、easy_install django
源码安装：
	python setup.py install

安装web server
功能:
	把动态生成的HTML页面返回给浏览器，还负责处理图片和css静态的内容，以及各种系统级别的事情
	（负载均衡，代理等）
1、python内置的runserver服务器，也称“dev”，只能用于测试；
2、推荐配置：apache+mod_python

a.mkdir blog && cd blog;
b.django-admin.py  startproject my_project_name
c.cd my_project_name; python manage.py runserver

d.有了项目之后，可以在它下面创建应用；
	python manage.py  my_app_name

e.在my_project_name目录下的my_project_name/setting中；
	INSTALLED_APPS元组后加上 'my_project_name.my_app_name', (逗号不能少)

f.my app的核心部分my_project_name/my_app_name/model.py文件：
	定义myapp数据结构的地方；


log对比：
中信机房：
2015-07-29 15:20:40.2 [13616][0]--------vmd开始新建机器
2015-07-29 15:20:47.1 [13616][0]--------vmd创建完成，返回给vmc到web；此时机器开始启动；
2015-07-29 15:23:21.8 [13616][0]--------机器到达登录界面

科技网：
2015-07-28 20:32:22.8 [18575][0]--------vmd开始新建机器
2015-07-28 20:32:30.1 [18575][0]--------vmd创建完成，返回给vmc到web；此时机器开始启动
2015-07-28 20:33:10.5 [18575][0]--------机器到达登录界面

智慧：
2015-07-28 10:29:46.4 [5352][0]--------vmd开始新建机器
2015-07-28 10:29:51.5 [5352][0]--------vmd创建完成，返回给vmc到web；此时机器开始启动
2015-07-28 10:31:08.6 [5352][0]--------机器到达登录界面

影响因素：节点配置（磁盘、cpu；以及分布式存储的存储节点性能），节点此时的负载等，


1、virsh destroy uuid 
2、virsh start uuid && date  -----记录机器启动时间
3、登录vnc；到达界面，再次date记录时间
 3 # Create your models here.
  4 #models.Model子类是Django强大的对象关系映射(ORM)系统的核心；



/websockify.py 9111 192.168.2.18:6001  –target-config=/var/lib/one/sunstone_vnc_tokens

novnc使用说明：
1、运行一个轻量级代理，运行在本地30机器上
	nohup /root/noVNC/utils/websockify/run --web=. --target-config=/root/noVNC/vnc_token/vm_01  6080 &
2、修改/root/noVNC/vnc_token/vm_01文件，
格式如下；
	uuid: 节点:vnc端口号；

3、修改/root/noVNC/utils/auto_vnc.html
 修改如下：
	 path = WebUtil.getQueryVar('path', 'websockify/?token=目标机器uuid');

4、浏览器访问http://192.168.2.30:6080/vnc_auto.html
	
每次访问不同主机，需要更新/root/noVNC/vnc_token/vm_01文件如步骤2,以及/root/noVNC/utils/auto_vnc.html，如步骤3

测试url：http://192.168.2.30:6080/vnc_auto.html
密码：a7b6490b598951f7

rbd导出快照：
rbd export   rbd/test@snap01 




RAID0速度是最快的，因为数据是分开存放在每个组成阵列的硬盘，所以一旦其中一块硬盘有问题就会导致所有数据损坏。优点：速度快、成本低 缺点数据容易丢失，一旦损坏无法恢复。
RAID1的原理是有两块硬盘组成的阵列，其中一块拿来正常使用，另外一块是专门备份存放的，相当于你两块硬盘只能用一块硬盘，另外那块是保存这块硬盘里面的数据，
这样的话即使你有一块硬盘坏了数据也不会丢失，但速度慢，而且两块硬盘只能用一块硬盘的容量。

RAID5：分布式奇偶校验的独立磁盘结构


http://blog.csdn.net/kevin_darkelf/article/details/40980333

python获取url地址中的参数
>>> url="http://localhost/test.py?a=hello&b=world "
>>> result=urlparse.urlparse(url)
>>> result
ParseResult(scheme='http', netloc='localhost', path='/test.py', params='', query='a=hello&b=world ', fragment='')
>>> urlparse.parse_qs(result.query,True)
{'a': ['hello'], 'b': ['world ']}
>>> params=urlparse.parse_qs(result.query,True)
>>> params
{'a': ['hello'], 'b': ['world ']}
>>> params['a'],params['b']
(['hello'], ['world '])


用urlparse模块就可以解析出来了



2====1aa1569675ba4186
3====596b8083749d6f22


智慧vpn：255.75

websockify/token_plugins.py L33

novnc工作流程：
vnc.html<------->websockets<------->noVNC-Proxy(websockify.py)<------>vnc server


系统默认用户名：administrator
vivi  2015/8/3 15:25:01
密码是 135!#%qetadgzcb
vivi  2015/8/3 15:25:13
210.14.78.51


2015-08-03 15:19:18.0 [22359][0]:LOG_MAIN:E:rbd_copy2 start! now_time:Mon Aug  3 15:19:18 2015
, src_img: 50683hdb.img, dest_img: 51591hdb.img

唯宝服务器IP地址：125.215.37.39:9677

用户名administrator密码135!#%qetadgzcb

IP  210.14.78.78/79/80/81
user: administrator
pass: transrv
6点可以开始

IP  210.14.78.198
用户名:administrator
口令：135!#%qetadgzcb
9点可以开始


1、香港中信ceph集群上线事宜，修改增加磁盘的配置问题，网卡删除机器节点网卡down掉问题，以及对克隆磁盘测试不同方案等；
2、负载均衡，整体框架完成，
3、寻找web vnc解决方案，暂定novnc，待修改，需要兼容web端代码；
4、学习openstack中sdn部分；

1、使用异步实现ceph磁盘克隆问题；
2、完善负载均衡；
3、跟进线上系统bug；
4、novnc兼容web代码问题；
5、学习openstack管理平台；

http://114.80.119.146/ftpdir/vm/MergeIDE.zip

中级篇：
1、从硬件入手搭建集群：
	1.1、分析常见场景；
	1.2、分别为对应的场景方案配置硬件；
	1.3、ceph集群调优
2、管理ceph集群以及常见错误处理

3、ceph架构和组成
	3.1、ceph osd 文件系统
	3.2、ceph osd journal
	3.3、ceph  osd 常见命令分析
	3.4、ceph mon 常用命令分析
	3.4、ceph rbd块存储命令分析
4、crush map介绍
	1、crush lookup
	2、crush hierarchy
	3、恢复和重平衡
	4、设计crush map
	5、customizing a cluster layout
	6、修改pg和pgp
	7、ceph数据管理

5、块存储与虚拟化
	ceph与openstack


晚上6点要迁额密码给你哦

杨晶晶 2015/8/5 15:08:47
用户名: zsadmin
密码:pssw0rdPSSW)RD
210.14.78.119

db19838b-907f-7fc4-6d9d-89db89b8bbfd

IP:210.14.78.98
用户名：administrator
密码：bcmis@bc201



nohup ./startWebLogic.sh >out.log 2>&1 &


nohup command &


select 	b.storage_type from  `vhost` a , `cluster` b  where  a.id= and  a.cluster_id = b.id   



第二章：存储基石 - RADOS
程鹏

第三章：智能分布 - CRUSH
沈志伟、程鹏


第二章：存储基石 - RADOS
2.1 Ceph的分布式本质
2.2 RADOS组成
   2.2.1 MON简介
   2.2.2 OSD简介
2.3 快速搭建RADOS环境
2.4 LIBRADOS介绍
   2.4.1 LIBRADOS的C语言demo
   2.4.2 LIBRADOS的PYTHON语言demo
   2.4.3 LIBRADOS的JAVA语言demo

第三章：智能分布 - CRUSH
3.1 CRUSH的本质
3.2 CRUSH基本原理
   3.2.1 Object与PG
   3.2.2 PG与OSD
   3.2.3 PG与POOL
3.3 CRUSH关系分析
（新建pool，上传object，搞清楚RADOS里面，object与pool、PG、OSD的映射关系）





 NotifyCephFlatten



宽惠：
ip:210.14.78.49:33890
用户名：administrator
密码：xsw2@zaq1

210.14.78.194 UUID: a34b583a-8c74-d3a4-7944-3a23024973c1

78.76

openstack 王者归来pdf
http://down.51cto.com/data/2065224

                    pthread_rwlock_wrlock(&g_flattenImg_rwlock);
                    g_flattenImg_map.insert(make_pair(pData->UUID, disk_name));
                    pthread_rwlock_unlock(&g_flattenImg_rwlock);

extern std::map<std::string, std::string> g_flattenImg_map;
extern pthread_rwlock_t g_flattenImg_rwlock;


Ceph 的选项
“rbd_flatten_volume_from_snapshot”: RBD Snapshot 在底层会快速复制一个元信息表，但不会产生实际的数据拷贝，因此当从 Snapshot 创建新的卷时，用户可能会期望不要依赖原来的 Snapshot，这个选项开启会在创建新卷时对原来的 Snapshot 数据进行拷贝来生成一个不依赖于源 Snapshot 的卷。

“rbd_max_clone_depth”: 与上面这个选项类似的原因，RBD 在支持 Cinder 的部分 API(如从 Snapshot 创建卷和克隆卷)都会使用 rbd clone 操作，但是由于 RBD 目前对于多级卷依赖的 IO 操作不好，多级依赖卷会有比较严重的性能问题。因此这里设置了一个最大克隆值来避免这个问题，一旦超出这个阀值，新的卷会自动被 flatten。对于这个问题，实际上社区已经有相关的解决方案了(​RBD: Shared flag, object map)，这个实现目前是由本人完成大部分工作，由于依赖 Sage 的 notify 改进，因此还需要一定时间。

“rbd_store_chunk_size”: 每个 RBD 卷实际上就是由多个对象组成的，因此用户可以指定一个对象的大小来决定对象的数量，默认是 4 MB，在不太了解 Ceph 核心的情况下，这个选项的默认值已经足够满足大部分需求了。

http://vgyun.96590.net/admin/index/login

admin
admin0717Q


待定事项：
	1、删除虚拟机考虑在flatten磁盘；
	2、中断回填，快照仍然有children




只读打开源不带快照copy：
now time is Fri Aug  7 18:48:02 2015
rbd_copy ok!!
after flatten, time is Fri Aug  7 18:50:50 2015


只读打开源，带快照copy：
now time is Fri Aug  7 18:53:53 2015

rbd_copy ok!!
after flatten, time is Fri Aug  7 18:56:38 2015


普通打开不带快照copy：
rbd_open success !!
now time is Fri Aug  7 19:09:08 2015

rbd_copy ok!!
after flatten, time is Fri Aug  7 19:11:39 2015


普通打开带快照copy：
now time is Fri Aug  7 19:37:40 2015

rbd_copy ok!!
after flatten, time is Fri Aug  7 19:40:07 2015


 ceph问题交流邮箱：questions@packtpub.com
 ceph申报bug：http://tracker.ceph.com/projects/ceph/issues?set_filter=1
账号：cepher_keke
密码: cp123456

允许某ip访问ssh
iptables -I INPUT -s 46.166.150.22 -p TCP --dport 80 -j ACCEPT

1、异步实现克隆问题（涉及克隆新建，克隆），备注：功能已基本完成，进行测试；
2、宽惠迁移

bash shell的命令分为两类：外部命令和内部命令。外部命令是通过系统调用或独立的程序实现的，如sed、awk等等。内部命令是由特殊的文件格式(.def)所实现，如cd、history、exec等等。


disk_type;  		#目标磁盘类型
dest_disk_type; 
src_disk_type;			
same_storage_cluster	#Info.SrcVmInfo.StorageCluster
same_node = Info.IsSameNode


ps auxf |grep "rbd flatten kvm/3752hda.img"  | awk '{print $2}' |xargs -n 1 -i kill -9 {}


ps auxf |grep "flatten" | grep  3752hda.img| awk '{print $2}' |xargs -n 1 -i kill -9 {}


     pthread_rwlock_wrlock(&g_flattenImg_rwlock);
        if((g_flattenImg_map.find(Info.ImgFile) != g_flattenImg_map.end()){
            g_flattenImg_map.erase(g_flattenImg_map.find(Info.ImgFile));
            pthread_rwlock_unlock(&g_flattenImg_rwlock);    
        }else{
            int exit_status = 0; 
            std::string cmd_output = "";
            std::string cmd =" ps auxf |grep flatten | grep "+ Info.ImgFile +" | awk '{print $2}' |xargs -n 1 -i kill -9 {}";
            cdn_log(LOG_MAIN).error(args_str("kill -9 flatten process error, cmd:%s", cmd.c_str()));
            for(int ii = 0; ii<3; ii++){
                exit_status = execute_cmd_noreturn(cmd.c_str());
                if(exit_status < 0){
                    cdn_log(LOG_MAIN).error(args_str("kill -9 flatten process error, Img:%s", Info.ImgFile.c_str()));
                }else{
                    break;
                }    
            }    
        }    

            std::string dest_disk = p->first;
            if(g_conf.disk_type == 2){
                std::vector<std::string> str_arr;
                if(dest_disk.find("/") !=std::string::npos){
                    split(dest_disk, "/", str_arr);
                }    
                dest_disk = str_arr[str_arr.size()-1];
            } 



http://hanover.iteye.com/blog/881972

如果父进程不关心子进程什么时候结束，那么可以用signal(SIGCLD, SIG_IGN)或signal（SIGCHLD, SIG_IGN）通知内核，自己对子进程的结束不感兴趣，那么子进程结束后，内核会回收，并不再给父进程发送信号
[cpp] view plaincopy在CODE上查看代码片派生到我的代码片
void AvoidZombie(void)  
{  
 struct sigaction act;  
 act.sa_handler = SIG_IGN;  
 act.sa_flags = SA_NOCLDWAIT;  
 sigemptyset (&act.sa_mask);  
 sigaction(SIGCHLD,&act,NULL);  
 return;  
}  


74上
/dev/sdb1       1.6T  1.1T  356G  76% /data

75：
/dev/sdb1       1.6T  1.3T  186G  88% /data

76：
/dev/sdb1       1.6T  731G  729G  51% /data


77上：
/dev/sdb1       1.6T  904G  555G  62% /data



cryptsetup-bin
cryptsetup

gdisk
python-flask
libgoogle-perftools0
btrfs-tools 
libradosstriper1

ceph-fuse
 ceph ceph-common ceph-fs-common ceph-mds gdisk libcephfs1
 libgoogle-perftools0 librados2 librbd1 libtcmalloc-minimal0 libunwind7
 python-cephfs python-flask python-jinja2 python-markupsafe python-rados
python-rbd python-requests python-six python-urllib3 python-werkzeug radosgw



ps auxf | grep sleep | grep 1000 |awk '{ if ($2=="14391") print $2  }' 



目前关于网站架构一般比较合理流行的架构方案：Web前端采用Nginx/HAProxy+Keepalived作负载均衡器；后端采用MySQL数据库一主多从和读写分离，采用LVS+Keepalived的架构。

反向代理是指的，服务器代理网络上的客户机请求，将请求转达给内部真实服务器，然后在返回给Internet客户端，代理服务器上面没有任何网页资料。
反向代理和正向代理没有冲突，可以在防火墙设备中同时使用这两种结合，正向代理可以进行过滤，保护内部网络安全。 


ceph  osd set noout
ceph  osd set norecovery



#主机网卡id，以及对应本节点的线路id
select a.id, b.idc_route_id  from  `vhost_eth` a, `node_eth` b,  `vhost` c  where  c.uuid =""  and 
c.id = a.vhost_id  and a.node_eth_id = b.id  


select  vlanid  from  lvan  where idc_route_id = 上句中的idc_route_id

http://docs.openstack.org/developer/horizon/



 /usr/bin/ceph-deploy
  
from ceph_deploy.cli import main    #找到main

root@u18:/usr/lib/python2.7/dist-packages/ceph_deploy# ls cli.py
cli.py
root@u18:/usr/lib/python2.7/dist-packages/ceph_deploy# pwd
/usr/lib/python2.7/dist-packages/ceph_deploy



main函数调用_main()

args = ceph_deploy.conf.cephdeploy.set_overrides(args)

ceph_deploy/conf/cephdeploy.py中的set_overrides()

vmc/my_vm_common_return.cpp   L646  


    std::string storageType = m_db->GetStr(sql.str());
    if(storageType.empty()){
        rt_in.errNum = -21; 
        rt_in.errMsg = sql.str() + " err:" + m_db->_err_str;
        cdn_log(LOG_NET).debug(rt_in.errMsg);
        post_err(args_str("克隆磁盘查找存储类型失败:%s， 请手动操作!", m_db->_err_str.c_str()), 20, 1, 1);
        return rt_in;
    }    

    sql.str("");
    if(2 == atoi(storageType.c_str())


    wrapper  包装 



远程桌面登录:
210.14.78.124:6482
user: ysoft_admin
password: josson64822002

iptables -I INPUT  -d 0/0 -p tcp --dport 5001 -j ACCEPT 


//198.2.192.193 ---源头
2015-09-21 16:22:11.6 [21558][0]:LOG_NET:D:origin disk size:56604753920
2015-09-21 16:22:11.6 [21558][0]:LOG_NET:D:FileTransfer call FileTransferStart:Node_Node_Util:ssl -p 9105 -h 198.2.212.97
2015-09-21 16:22:11.6 [21558][0]:LOG_NET:D:FileTransfer get remote addr: 198.2.212.97:12000, pos:0

//198.2.212.97----接收方

任务总结：
	1、微信支付代码
	2、线上问题，线上节点防火墙没有配置好，导致底层连接拒绝（克隆相关）；

#iptables -L
#iptables-save > /etc/iptables.up.rules
#iptables-restore < /etc/iptables.up.rules

临时清除防火墙
#iptables -F

iptables -A INPUT -p tcp --dport 22 -j ACCEPT


        if(m_db->GetStrArray(sql.str(), temp) || temp.empty()){
            errMsg = sql.str() + " err:" + m_db->_err_str;
            m_db->clear(temp);
            g_db_pool->return_db_conn(m_db);
            cdn_log(LOG_MAIN).error(errMsg);
            return NULL;
        }   
        std::string tmp = ""; 
        for(std::vector<StrArray*>::iterator it = temp.begin(); it != temp.end(); ++it){

 29 std::map<int, float> map_bandwidth_alarm_upper;
 30 std::map<int, float> map_bandwidth_alarm_lower;
 31 std::map<int, float> map_load_alarm_upper;
 32 std::map<int, float> map_load_alarm_lower;

 "python " + g_conf.vmd_send_mail + " vmd connect vmc error " + " node ip:" + g_conf.control_ip


wget -q -O- "http://keyserver.ubuntu.com:11371/pks/lookup?op=get&search=0x4759FA960E27C0A6"  | apt-key add -
 
select test1.id, test1.name, test1.address, test2.age from test1, test2 where test2.age=test1.id;
等价于：(使用内连接)
select test1.id, test1.name, test1.address, test2.age from test1 inner join test2 on test1.id=test2.age;

inner join（内连接）：
inner join：如果表中有至少一个匹配，则返回行；
left  join：左连接，即使右表中没有匹配，也从左表返回所有的行；
select  test1.id, test2.name, test1.address, test2.age from test1 left join test2 on 
test1.id=test2.age;
(右表: test2,  左表：test1)

right join: 右连接，即使左表中没有匹配，也从右表返回所有的行；
select test1.id, test1.name, test1.address, test2.age from test1 right join test2 on
test1.id = test2.age;
(右表：test2  左表：test1)

full join:  只要其中一个表中存在匹配，就返回行；
select test1.id, test1.name, test1.address, test2.age from test1 right join test2 on
test1.id = test2.age;

union：联合统计结果
select name from test1 where age between 25 and 50 union select name from test2 where age<100;


1、创建项目：django-admin.py app
2、cd  app && ls
	__init__.py, setting.py  urls.py,  manage.py
	__init__.py:使当前目录变为工作目录；
	urls.py:根据此文件调用对应的视图方法，本质上为url和视图函数间的映射表；
		但视图函数必须在python的搜索路径中；
	setting.py：项目的配置文件
	manage.py： 命令行工具，用于启动服务器等；

Django的工作原理：
	Django使用HttpRequest和HttpResponse对象在系统间传递状态。当一个页面被请求时，Django创建一个包含请求元数据的HttpRequest对象；
	然后Django根据urls.py调用合适的视图（所谓视图函数就是接受web请求，并返回web响应的函数，响应内容可以为html、图片、xml、json等；
	把httpRequest作为视图函数的第一个参数传入（其他参数可能是由urls模块匹配出来的），每个视图函数都要负责返回一个httpResponse对象；

	1、server端收到一个http请求，一个server端特点的handler会创建HttpRequest并传递给下一个组件并处理；
	这个handler然后调用所有可用的Request或者view中间件，这些类型的中间件通常是用来增强HttpRequest对象来对一些特别类型的request做些
	特殊处理；只要其中有一个返回HttpResponse, 系统就跳过对视图的处理

	 即便是最棒的程序员也会有出错的时候, 这个时候异常处理中间件（exception middleware）可以帮你的大忙.如果一个视图函数抛出异常,控制器会传递给异常处理中间件处理.如果这个中间件没有返回HttpResponse ,意味着它不能处理这个异常,这个异常将会再次抛出.

    即便是这样,你也不用担心.Django包含缺省的视图来生成友好的404和500回应（response）.

    最后, response middleware做发送HttpResponse给浏览器之前的后处理或者清除请求用到的相关资源.

5.HttpRequest、HttpResponse对象详解

    HttpRequest对象:表示来自客户端的一个单独的HTTP请求, 包含了关于此次请求的大多数重要信息.除了session外的所有属性都应该认为是只读的.

    HttpResponse对象: 与Django自动创建的HttpRequest对象相比, HttpResponse对象则是用户自己创建的. 每个视图都需要实例化、处理、返回一个HttpResponse对象.此类存在于django.http.HttpResponse.

    HttpRequest对象的属性:
        path: 请求页面完整的地址字符串(结尾处有斜线)-不包括域名和参数, 如 /admin/index/
        method: 表示请求使用的HTTP方法, 总是大写的.
            if request.method == 'GET':
                do_something()
            else:
                do_something_else()

        GET/POST/REQUEST: 类字典对象, 不可更改的, 是django.http.QueryDict的实例, 可用来获取参数信息, 分别包含了GET参数信息, POST参数信息,GET和POST综合信息(REQUEST跟PHP中一样是GET和POST的集合).

        COOKIES: 标准的字典, 键值都为字符串.

        FILES: 来自文件上传表单, 值为一个标准的字典, 包含三个键:filename(原文件文件名), content-type(文件的内容类型), content(文件的原始内容);

        META: 包含所有的HTTP头信息, 与PHP的$_SERVER类似.包含CONTENT_LENGTH, CONTENT_TYPE, QUERY_STRING, REMOTE_ADDR, REMOTE_PORT, REMOTE_HOST, SERVER_NAME, SERVER_PORT等; 与PHP类似, 在META中有效的HTTP头信息都是代用HTTP_前缀的键. 例如:HTTP_ACCEPT_ENCODING, HTTP_HOST, HTTP_REFERER, HTTP_USER_AGENT, HTTP_ACCEPT_LANGUAGE;

        user: 暂时未去了解, 待以后补上

        session: 一个可读写的类字典对象, 表示当前的session.使用的前提是Django已激活session支持.

        raw_post_data: POST的原始数据. 用于对数据的复杂处理.

    HttpRequest对象的方法:

        __getitem(key): 获取key对应的GET/POST值, 先找POST后GET, 键不存在则引发KeyError异常.该方法使用户可以以访问字典的方式来访问一个HttpRequest实例. 如request['name'], 和先检查request.GET或request.POST是否包含所给的键一样.

        has_key(): 获取request.GET或者request.POST中是否包含所给的键, 返回True或者False

        get_full_path(): 返回path, 与request.path不同的是包含请求参数

        is_secure(): 返回请求是否是以HTTPS形式提交的.


    HttpResponse:

        以字符串的形式传递页面的内容给HttpResponse的构造函数.可以配置返回头.
        response = HttpResponse("Here is the text of the Web page.")
        response = HttpResponse("Text only, please.", mimetype="text/plain")

        可以把response当做一个类文件对象使用.
        response = HttpResponse()
        response.write("Here is the text of the Web page.")
        response.write("Here is another paragraph.")

    HttpResponse子类:

        HttpResponseRedirect: 构造函数参数为重定向的路径.可以为一个完整的URL地址('http://www.baidu.com/a/b/')或不包括域名的路径('/a/b/').返回的状态码为302重定向.
        HttpResponseNotModified:无修改, 返回304状态码/
        HttpResponseBadRequest: 类似HttpResponse, 返回400状态码.
        HttpResponseNotFound: 类似HttpResponse, 返回404状态码.
        HttpResponseForbidden: 类似HttpResponse, 返回403状态码.
        HttpResponseServerError: 类似HttpResponse, 返回500状态码.
        以上几个都可以代替HttpResponse返回.

6.urls.py详解(此决定了与PHP文件夹层次调用文件的不同)
    url.py的本质为URL和视图函数间的映射表（视图函数必须要在Python的搜索路径中, Django项目的搜索路径是在manage.py文件中添加进去的, 可以看一下）, 是根据request.path进行匹配的.同时此模块可以匹配出除了request之外的第二个, 第三个等参数(比如url(r'^admin/user/(\w)+/$', 'mysite.views.admin'), 此时mysite.views.main视图函数可以接受第二个参数).但是所有的参数全为字符串.也可以用正则进行匹配, 这里面可不要乱加空格, 我就出现错误了.

    文件内容如下:
        from django.conf.urls import patterns, include, url
        urlpatterns = patterns('',
            # Examples:
            # url(r'^$', 'mysite.views.home', name='home'),
            # url(r'^mysite/', include('mysite.foo.urls')),

            # Uncomment the admin/doc line below to enable admin documentation:
            # url(r'^admin/doc/', include('django.contrib.admindocs.urls')),

            # Uncomment the next line to enable the admin:
            # url(r'^admin/', include(admin.site.urls)),
        )

    第一行, 从django.conf.urls下导入patterns, include, url函数

    第二行, 调用patterns函数, 并将返回结果保存到urlpatterns变量. patterns()函数第一个参数为视图函数通用前缀, 即公用前缀. urlpatterns即代表了URL到视图函数的映射关系.此处匹配的时候比如随便一个匹配(r'^admin/$', 'mysite.views.admin'), ^$是很重要的分隔符, 进行精确匹配, 如果去掉^则有可能匹配到任意由admin/结束的path, 比如/a/b/admin/等, 而且不必写成^/admin/$, 因为Django进行匹配的时候会自动把前面的"/"加上.

URLconf 就像是 Django 所支撑网站的目录;
用两个大括号括起来的文件(例如：{{person_name}})成为变量（variable）这意味着在此处插入指定变量的值。 如何指定变量的值呢？

被大括号和百分号包围的文本(例如：{%if ordered_warranty%})是模板标签（template tag）；标签(tag)定义比较明确，即： 仅通知模板系统完成某些工作的标签。

1、创建Template对象，
2、使用context来传递数据给Template对象； 一个context是一系列变量和它们值的集合。
3、调用Template对象的render()方法并传递context来填充模板；





"let OmniCpp_MayCompleteDot=1    "  打开  . 操作符
"let OmniCpp_MayCompleteArrow=1  "打开 -> 操作符
"let OmniCpp_MayCompleteScope=1  "打开 :: 操作符
"let OmniCpp_NamespaceSearch=1   "打开命名空间
"let OmniCpp_GlobalScopeSearch=1
"let OmniCpp_DefaultNamespace=["std"]
"let OmniCpp_ShowPrototypeInAbbr=1  "打开显示函数原型
"let OmniCpp_SelectFirstItem = 2 "自动弹出时自动跳至第一




select sql_no_cache a.id, a.control_ip, a.cluster_id, b.id, b.pub_gate_way, b.pub_mask, b.br_name, a.bakup_node_id from `node` a, `node_eth` b, `cluster` c where a.cluster_id=59 and a.status=1 and a.vm_count < a.max_vm_count and a.service_type=1 and b.status=1 and b.idc_route_id=61 and c.status=1 and c.type=1 and c.vlan_con_node_id>0 and a.id=b.node_id and a.cluster_id=c.id and a.control_ip='103.59.216.3'

1009741 
53441hda.img
52:54:00:05:72:ac  mac地址

10.22.0.118


    try {
        std::string buf = args_str("Node_Vm_Common:%s -p %d -h %s", g_conf.node_proto.c_str(), g_conf.node_port, Ip.c_str());
        cdn_log(LOG_NET).debug(buf);
        Ice::ObjectPrx base = g_ic->stringToProxy(buf);   //字符串转代理
        if (!base)
          throw "WebCon::MyVmCommon::ListVmBasic, Could not create proxy";

        ::ConNode::VmCommonPrx vm = ::ConNode::VmCommonPrx::checkedCast(base->ice_timeout(5000));
        if (!vm)
          throw "WebCon::MyVmCommon::ListVmBasic, Invalid proxy";
        ::ConNode::VmInfoRt rt_node = vm->ListVmBasic(UUID);
        if(rt_node.errNum){
            rt_web.errNum = rt_node.errNum;
            rt_web.errMsg = rt_node.errMsg;
            cdn_log(LOG_NET).error(rt_node.errMsg);
        }else{
            for(::ConNode::VmInfoArray::iterator it = rt_node.infoArray.begin(); it!= rt_node.infoArray.end();it++){
                ::WebCon::VmInfo node1;
                VmInfoCopy(node1, *it);
                rt_web.infoArray.push_back(node1);
            }    
            rt_web.errNum = 0; 
            rt_web.errMsg = "";
        }    

    }catch(const Ice::TimeoutException &){
    	cdn_log(LOG_NET).error(e.what());
    }catch (const Ice::Exception & e) {
        cdn_log(LOG_NET).error(e.what());
        rt_web.errMsg += e.what();
    } catch (const char * msg) {
        cdn_log(LOG_NET).error(msg);
        rt_web.errMsg += msg;
    } catch (...) {
    }


sequence<Fruit> FruitPlatter;
Slice 编译器会为 FruitPlatter 生成这样的 C++ 定义：
typedef std::vector<Fruit> FruitPlatter;

dictionary<long, Employee> EmployeeMap;

下面的代码是根据这个定义生成的：
typedef std::map<Ice::Long, Employee> EmployeeMap;

接口映射到类:

interface  Simple{
	void  op();
}

slice编译器生成下面定义：
 namespace IceProxy{
 	class Simple : public virtual IceProxy::Ice::Object{
 	public:
 		void op();
 		void op(const Ice::Context &);
 		//... 	
 	} 
 }
typedef IceInternal::ProxyHandle<IceProxy::Simple> SimplePrx;

如果某个接口嵌套在模块 M 中，生成的名字
就是IceProxy::M::<interface-name>和
::M::<interface-name>Prx;
IceProxy::Simple 实例是 “远地的服务器
中的 Simple 接口的实例”的 “本地大使” ，叫作代理类实例。与服务器端
对象有关的所有细节，比如其地址、所用协议、对象标识，都封装在该实
例中

注意：Simple继承自IceProxy::Ice::object, 这反映了这样一个事实：
所有的Ice接口都隐含地继承自Ice::Object. 对于接口中的每个操作；
代理类都有两个重载的、同名的成员函数；
我们会发现操作op映射到了两个成员函数op；

其中一个函数的最后一个参数的类型是Ice::Context. Ice run time用这个
参数存储关于请求的传递方式的信息；通常并不需要为此提供一个值；
可以假装这个参数不存在；

代理类， 代理类实例，代理句柄；

代理实例总是由 Ice run time 替客户实例化；
当客户从 run time 那里接收代理时，它会得到指向该
代理的代理句柄，其类型是 < in t e r f a c e- n a m e >Prx （对于前面的例子
就是 SimplePrx）
通过代理的句柄来访问代理，
句柄负责把操作调用转发给其底层的代理，并且会对代理进行引用计数。
这意味着，你不会遇到内存管理问题：代理的释放是自动的，会在指向代理的最后一个句柄
消失时 （退出作用域时）发生。



C++类型转换分为：隐式类型转换和显式类型转换
1、隐式类型转换：
	


django

数据存取逻辑、业务逻辑和表现逻辑组合在一起的概念有时被称为软件架构的 Model-View-Controller

而 Django 里更关注的是模型（Model）、模板(Template)和视图（Views），Django 也被称为 MTV 框架 。在 MTV 开发模式中：

我们已经创建了 project , 那么 project 和 app 之间到底有什么不同呢？它们的区别就是一个是配置另一个是 代码：
一个project包含很多个Django app以及对它们的配置。

技术上，project的作用是提供配置文件，比方说哪里定义数据库连接信息, 安装的app列表， TEMPLATE_DIRS ，等等。

一个app是一套Django功能的集合，通常包括模型和视图，按Python的包结构的方式存在。

例如，Django本身内建有一些app，例如注释系统和自动管理界面。 app的一个关键点是它们是很容易移植到其他project和被多个project复用。

普通的python字符串是经过编码的，意思就是它们使用了某种编码方式（如ASCII，ISO-8859-1或者UTF-8）来编码。
果你把奇特的字符（其它任何超出标准128个如0-9和A-Z之类的ASCII字符）保存在一个普通的Python字符串里，你一定要跟踪你的字符串是用什么编码的，否则这些奇特的字符可能会在显示或者打印的时候出现乱码
当你尝试要将用某种编码保存的数据结合到另外一种编码的数据中，或者你想要把它显示在已经假定了某种编码的程序中的时候，问题就会发生。
我们都已经见到过网页和邮件被???弄得乱七八糟。 ?????? 或者其它出现在奇怪位置的字符：这一般来说就是存在编码问题了。

但是Unicode对象并没有编码。它们使用Unicode，一个一致的，通用的字符编码集。 
当你在Python中处理Unicode对象的时候，你可以直接将它们混合使用和互相匹配而不必去考虑编码细节。

Django 在其内部的各个方面都使用到了 Unicode 对象。 模型 对象中，检索匹配方面的操作使用的是 Unicode 对象，视图 函数之间的交互使用的是 Unicode 
对象，模板的渲染也是用的 Unicode 对象。 通常，我们不必担心编码是否正确，后台会处理的很好



bug跟进：底层libvirt API阻塞不返回情况下，vmc端连接数耗光问题，vmc端设置接收消息超时机制，并在本地进行测试；
		
sudo pg_createcluster 9.3 main --start
账号：cp
密码：cdnunion




2015-09-16 15:51:14.7 [30486][0]:LOG_NET:D:debug into VmDiskChange, UUID:7a450da7-65ee-41e4-75e4-9103b0c5cbe0  IP:103.59.216.2  DiskID:146979  TemplateID:57
2015-09-16 15:50:06.0 [30486][0]:LOG_NET:D:debug into VmDiskChange, UUID:7a450da7-65ee-41e4-75e4-9103b0c5cbe0  IP:103.59.216.2  DiskID:146979  TemplateID:133


2015-09-16 17:11:35.4 [30486][0]:LOG_NET:D:debug into VmDiskChange, UUID:7a450da7-65ee-41e4-75e4-9103b0c5cbe0  IP:103.59.216.2  DiskID:146979  TemplateID:82

2015-09-16 17:16:49.7 [30486][0]:LOG_NET:D:debug leave VmDiskChange, UUID:7a450da7-65ee-41e4-75e4-9103b0c5cbe0 rt_num:2 rt_msg:Vm Init domain fail


52:54:00:4c:8a:3b

2015-09-16 17:11:49.7 [29628][0]:LOG_VIRT:D:before waiting:52:54:00:4c:8a:3b;

7a450da7-65ee-41e4-75e4-9103b0c5cbe0




URL相关信息：
HttpRequest对象包含当前请求URL的一些信息：
request.path  除域名以外的请求路径，以正斜杠开头  "/hello/"
request.get_host()   主机名			  				
request.get_full_path() 请求路径，可能包含查询字符串
request.is_secure    如果通过https访问，则此方法返回True，否则返回False


除了基本的元数据，HttpRequest对象还有两个属性包含了用户所提交的信息： request.GET 和 request.POST

request.GET和request.POST是类字典对象，意思是他们的行为像python里标准的字典对象；
但在技术底层上他们不是标准字典对象；比如说，request.GET和request.POST都有get()
keys(),和value方法，可以用for key in request.GET获取所有的键；

POST数据是来自于HTML中的<form>标签提交的，而GET数据可能来自<form>提交也可能是
URL中的查询字符串（the query string)

通常表单开发分为两部分：前段HTML页面用户接口和后台view函数对所提交数据的处理过程；


微信API CA证书passwd：c5cfec7b89ef4feb9b15401ab778eb90
微信支付商户号	1271458401
商户平台登录帐号	1271458401@1271458401
商户平台登录密码	019050
申请对应的公众号	YUNVM云主机
公众号APPID	wxd9bf3112e632816f





weixin://wxpay/bizpayurl?appid=wxd9bf3112e632816f&mch_id=1271458401&nonce_str=rimfplsJFonletwbbrigvnstwpzqeela&product_id=01&time_stamp=20150918015215&key=192006250b4c09247ec02edce69f6a2d



Linux内核IP Queue机制的分析(一)

Linux内核源码研读与实战演练 ：http://www.osforce.cn/course/83Linux内核从原理到代码详解： http://www.osforce.cn/course/129


PREROUTING：数据包进入网络层马上路由前
FORWARD：数据包路由之后确定要转发之后
INPUT：数据包路由之后确定要本地接收之后
OUTPUT：本地数据包发送(详情见附录4)
POSTROUTING：数据包马上发出去之前






echo -n "#!/bin/sh  exec /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.$HOSTNAME" >> /var/www/s3gw.fcgi



            "access_key": "CTENEWRWR1F99D87A9F3",
            "secret_key": "cS8ctnzwARZZ6Z2Gk7A2CEc5hwkrOp5ALhozk9YC"

从GitHub上Clone的Ceph项目，其目录下主要文件夹和文件的内容为： 
1 根目录 
[src]：各功能某块的源代码 
[qa]：各个模块的功能测试（测试脚本和测试代码） 
[wireshark]：#wireshark的ceph插件。 
[admin]：管理工具，用于架设文档服务器等 
[debian]：用于制作debian（Ubuntu）安装包的相关脚本和文件


[doc]：用于生成项目文档，生成结果参考http://ceph.com/docs/master/ 
[man]：ceph各命令行工具的man文件 
configure.ac：用于生成configure的脚本 
Makefile.am：用于生成Makefile的脚本 
autogen.sh：负责生成configure。 
do_autogen.sh：生成configure的脚本，实际上通过调用autogen.sh实现 
ceph.spec.in：RPM包制作文件

2 src目录

[include]：头文件，包含各种基本类型的定义，简单通用功能等。 
[common]：共有模块，包含各类共有机制的实现，例如线程池、管理端口、节流阀等。 
[log]：日志模块，主要负责记录本地log信息（默认/var/log/ceph/目录） 
[global]：全局模块，主要是声明和初始化各类全局变量（全局上下文）、构建驻留进程、信号处理等。 
[auth]：授权模块，实现了三方认知机制。 
[crush]：Crush模块，Ceph的数据分布算法 
[msg]：消息通讯模块，包括用于定义通讯功能的抽象类Messenger以及目前的实现SimpleMessager 
[messages]：消息模块，定义了Ceph各节点之间消息通讯中用到的消息类型。 
[os]：对象（Object Store）模块，用于实现本地的对象存储功能， 
[osdc]：OSD客户端（OSD Client），封装了各类访问OSD的方法。 
[mon]：mon模块 
[osd]：osd部分 
[mds]：mds模块 
[rgw]：rgw模块的 
[librados]：rados库模块的代码 
[librdb]：libbd库模块的代码 
[client]：client模块，实现了用户态的CephFS客户端 
[mount]：mount模块 
[tools]：各类工具 
[test]：单元测试 
[perfglue]：与性能优化相关的源代码 
[json_spirit]：外部项目json_spirit 
[leveldb]：外部项目leveldb from google 
[gtest]：gtest单元测试框架 
[doc]：关于代码的一些说明文档 
[bash_completion]：部分bash脚本的实现 
[pybind]：python的包装器 
[script]：各种python脚本 
[upstart]：各种配置文件

ceph_mds.cc：驻留程序mds 
ceph_mon.cc：驻留程序mon 
ceph_osd.cc：驻留程序osd 
libcephfs.cc：cephfs库 
librdb.cc：rdb库 
ceph_authtool.cc：工具ceph_authtool 
ceph_conf.cc：工具ceph_conf 
ceph_fuse.cc：工具ceph_fuse 
ceph_syn.cc：工具ceph_syn 
cephfs.cc：工具cephfs 
crushtool.cc：工具crushtool 
dupstore.cc：工具dupstore 
librados-config.cc：rados库配置工具 
monmaptool.cc：工具monmap 
osdmaptool.cc：工具osdmap 
psim.cc：工具psim 
rados.cc：工具rados 
rdb.cc：工具rdb 
rados_export.cc：rados工具相关类 
rados_import.cc：rados工具相关类 
rados_sync.cc：rados工具相关类 
rados_sync.h：rados工具相关类 
sample.ceph.conf：配置文件样例 
ceph.conf.twoosds：配置文件样例 
Makefile.am：makefile的源文件 
valgrind.supp：内存检查工具valgrind的配置文件 
init-ceph.in：启动和停止ceph的脚本 
mkcephfs.in：cephfs部署脚本





            "access_key": "D6N57ZI7EHQ48N6QMK5Z",
            "secret_key": ""
        }
    ],
    "swift_keys": [
        {
            "user": "cp:swift",
            "secret_key": "cJHZJUT2NcOzPrVu5K4TtmfSYbyI2af67gf8BXcz"

CRUSH详解
https://tobegit3hub1.gitbooks.io/ceph_from_scratch/content/architecture/crush.html

c++ http实现
https://github.com/avplayer/avhttp

s3 java sdk使用
http://www.tuicool.com/articles/jqmM32

s3 python sdk使用
http://blog.csdn.net/kut00/article/details/17398677


python框架:
Pulsar：Python的事件驱动并发框架
Pulsar是一个事件驱动的并发框架，有了pulsar，你可以写出在不同进程或线程中运行一个或多个活动的异步服务器。


Diesel：基于Greenlet的事件I/O框架
Diesel提供一个整洁的API来编写网络客户端和服务器。支持TCP和UDP。

Falcon：构建云API和网络应用后端的高性能Python框架Falcon是一个构建云API的高性能Python框架，它鼓励使用REST架构风格，尽可能以最少的力气做最多的事情
Falcon是一个构建云API的高性能Python框架，它鼓励使用REST架构风格，尽可能以最少的力气做最多的事情


http://%(bucket)s.s3-website-%(location)s.amazonaws.com/
           "access_key": "CTENEWRWR1F99D87A9F3",
            "secret_key": "cS8ctnzwARZZ6Z2Gk7A2CEc5hwkrOp5ALhozk9YC"

RGW业务处理流程：

http reqest --> apache 转 FastCgi module:

FastCgi module --> radosgw  通过socket请求实现(未确定是否有其它方式)

radosgw --> ceph集群  通过socket实现，调用rados接口

radosgw常用命令：
创建容器
s3cmd mb s3://BUCKET
删除容器
s3cmd rb s3://BUCKET
查看容器内的文件
s3cmd ls [s3://BUCKET/path/
查看所有容器内的所有文件，我测试过只能显示一层目录
s3cmd la
把文件从本地上传至oos的命令
s3cmd put FILE [FILE...] s3://BUCKET[/PREFIX]
从oos下载文件到本地
s3cmd get s3://BUCKET/OBJECT LOCAL_FILE
删除容器内的特定文件
s3cmd del s3://BUCKET/OBJECT
如果要删除容器类的整个目录，可以加参数 -r -f
s3cmd del -r -f s3://容器/目录/
把本地目录同步到oos或者从oos同步到本地
s3cmd sync LOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR
查看容器所占空间
s3cmd du [s3://BUCKET[/PREFIX]]
查看容器或者文件的属性
s3cmd info s3://BUCKET[/OBJECT]
天翼oos中的文件复制命令
s3cmd cp s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]
移动文件
s3cmd mv s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]

其中sync同步命令很实用，可以加参数实现不少功能：
同步本地至oos，并在云端删除已经从本地删除的文件
s3cmd sync --delete-removed /path/ s3://bucket/path/
如果怕错删除，可以再加参数 --dry-run，它只列出--delet-removed将要删除的文件，但并不会真正的删除
s3cmd sync --dry-run --delete-removed /path/ s3://bucket/path/
sync命令默认是要校检本地文件和oos中文件的md5值的，如果不想校检只同步新文件，可以加 --skip-existing
s3cmd sync --skip-existing /path/ s3://bucket/path/
还有 --exclude（不包含） 和 --include（包含）参数
s3cmd sync --exclude '*.txt' --include 'dir2/*' . s3://bucket/path/

对于我，天翼oos是用来存储网站上的静态内容的：.css、.js、.jpg一类。这类文件加入Cache-Control header 可以减少请求数量和流量，参数是 --add-header，命令如下：

s3cmd put --add-header='Cache-Control:max-age=31536000' -M -r jpg_folder s3://assets.onepx.com/

如果想进一步降低流量消耗，可以考虑gzip压缩css和js文件，毕竟流量在云存储里是算钱的，但貌似天翼oos服务器端并不支持gzip压缩，所以我们要预先压缩文件。我的做法是先 "gzip -9" 压缩css文件得到 *.css.gz ，再批量重命名 *.css.gz 为 *.css，最后上传 *.css 文件。这时需要给已经压缩过的文件加 Content-encoding header，否则浏览器读不出来，命令如下：

s3cmd put --add-header='Cache-Control:max-age=31536000' --add-header='Content-encoding:gzip' -M *.css s3://assets.onepx.com/css/

 目前网站还在国外，这些静态内容放天翼oos后，网页加载速度真心快不少。

关于REST，这也是比较火的一种Web服务架构。简单来说，资源是由URI指定，对资源的操作包括GET、PUT、POST、DELETE和HEAD，返回结果常常是XML或者其他形式。

s3cmd 远端同步到本地：
/usr/local/bin/s3cmd sync s3://cdnsong/ /data/



python的boto
c++的libAWS

rgw架构
http://yahooeng.tumblr.com/post/116391291701/yahoo-cloud-object-store-object-storage-at

amazon  s3 document：
http://aws.amazon.com/fr/documentation/s3/


poto
https://aws.amazon.com/cn/sdk-for-python/

http://boto3.readthedocs.org/en/latest/

s3单元测试用例：
https://github.com/ceph/s3-tests/blob/master/s3tests/functional/test_s3.py


apache+php
http://blog.csdn.net/xiaoliouc/article/details/17639503



[root@mail postfix]# groupadd mailusers

[root@mail postfix]# useradd -g mailusers -s /sbin/nologin tom

[root@mail postfix]# useradd -g mailusers -s /sbin/nologin jerry

[root@mail postfix]# passwd tom

[root@mail postfix]# passwd jerry


https://www.youtube.com/watch?v=XyDcYV9doL8

it188@#$com




 temalloc 的cache 变量插入位置
修改所有存储节点的/etc/init.d/ceph, 并重启所有OSD
--- ceph    2015-09-06 01:40:29.116395273 +0800
+++ /etc/init.d/ceph    2015-09-02 12:57:52.000000000 +0800
@@ -302,6 +302,7 @@

        [ -n "$wrap" ] && runmode="-f &" && runarg="-f"
	[ -n "$max_open_files" ] && files="ulimit -n $max_open_files;"
+       cmd="TCMALLOC_MAX_TOTAL_THREAD_CACHE_BYTES=268435456 $cmd"

        if [ -n "$SYSTEMD_RUN" ]; then
	cmd="$SYSTEMD_RUN -r bash -c '$files $cmd --cluster $cluster -f'"


 通过nginx 做url的重定向 把这些操作通过前端转入到后台特定的服务节点比较好

Neutron：
https://yeasy.gitbooks.io/openstack_understand_neutron/content/gre_mode/index.html



  "access_key": "CTENEWRWR1F99D87A9F3",
   "secret_key": "cS8ctnzwARZZ6Z2Gk7A2CEc5hwkrOp5ALhozk9YC"

	ceph源码下载链接：http://download.ceph.com/tarballs/

   s3可执行文件：/root/cp/ceph0.94/src/libs3/build/bin

./s3 -h -f -u -s -r  1 list
 
./s3 -h -f -u -s -r  1  getacl test filename=aa.txt
 
./s3 -h -f -u -s -r  1  setacl test filename=aa.txt

./s3 -h -f -u -s -r  1  test   aaaaaa

./s3 -h -f -u -s -r  1  create   aaaaa1

./s3 -h -f -u -s -r  1  create   aaaaa2 cannedAcl=public-read-write location=EU

./s3 -h -f -u -s -r  1 delete aaaaa1

./s3 -h -f -u -s -r  1 put test/release filename=/root/release.asc

./s3 -h -f -u -s -r  1  copy   test/release  aaaaaa/release

 ./s3 -h -f -u -s -r  1  get  test/release filename=./release

云存储上海电信ip：
	203.156.197.253
	203.156.196.254

E:\Work\linux\ceph
rgw典型案例：
http://cephnotes.ksperis.com/blog/2015/05/12/radosgw-big-index

在C++中，内存分成5个区，他们分别是堆、栈、自由存储区、全局/静态存储区和常量存储区

So i will free the space with:
for i in `rados -p data ls`; do echo $i; rados -p data rm $i; done


移植journal 日志
### Stop osd
$ service ceph stop osd.0

### Flush Journal
$ ceph-osd --flush-journal -i 0

### Create symlink to partition
$ rm /var/lib/ceph/osd/ceph-0/journal
$ ln -s /dev/sde1 /var/lib/ceph/osd/ceph-0/journal

### Create new journal
$ ceph-osd --mkjournal -i 0
$ service ceph start osd.0


rbd.cc:
	513   if (f) {
 	514     f->open_object_section("image");
 	515     f->dump_string("name", imgname);
 	516     f->dump_unsigned("size", info.size);
 	//517     f->dump_string("block_name_prefix", info.block_name_prefix);

/usr/lib/x86_64-linux-gnu/libcurl.so.4 -> /usr/lib/libcurl.so.4.3.0*



echo -n " [client.radosgw.$HOSTNAME]  host = $HOSTNAME keyring = /etc/ceph/ceph.client.radosgw.$HOSTNAME.keyring rgw socket path = /tmp/radosgw.sock log file = /var/log/ceph/radosgw.$HOSTNAME.log  rgw dns name = $HOSTNAME  " >> /etc/ceph/ceph.conf

HOSTNAME=`hostname`
echo -n "
[client.radosgw.$HOSTNAME]
host = $HOSTNAME
keyring = /etc/ceph/ceph.client.radosgw.$HOSTNAME.keyring
rgw socket path = /tmp/radosgw.sock
log file = /var/log/ceph/radosgw.$HOSTNAME.log
rgw dns name = $HOSTNAME
" >> /etc/ceph/ceph.conf

ovs控制器功能测试：
E:\Work\vm\src\go\src\ofc

poco库
http://www.tuicool.com/articles/ZjyUZju


ceph磁盘容量回收问题：
需要在虚拟机里执行fstrim触发空间回收，并且要求磁盘bus是scsi或者ide，默认的virtio是不可以的

insert into books(name,type) values('admin','2') on duplicate key update name='admin',type=2


INSERT INTO APIaccout_2015_December(date,APIcallacount,APIcallsize) VALUES (CURDATE(),300，100) on duplicate key  update APIcallacount=APIcallacount+300 ,APIcallsize=APIcallsize+100;
 select * from  APIaccout_2015_December;

rgw获取用户已使用bucket的容量大小：
radosgw-admin  bucket stats --uid=cp

radosgw-admin  user stats --uid=cp  --sync-stats

ceph支持rdma：
	编译，启用 --enable-xio            build Ceph Accelio transport
	Accelio是一套支持rdma协议的通讯框架，并且允许扩展包含client和server，同时支持用户态和内核态。
	mellanox.com: https://community.mellanox.com/docs/DOC-2141

两台DELL R720xd，每台12块4T硬盘，96G内存，万兆互联，Ceph副本数量为2，备份速度很好，240MB/s，但是备份恢复速度很慢，30M/s。最开始的时候是用RAID6将12块硬盘变为1个OSD，后来变为独立的OSD后，仍然不能解决性能上的问题。

最终的解决方案：加大Ceph服务器上的块设备的read_ahead_kb和KVM虚拟机中的read_ahead_kb，优化后的系统备份速度为450M/s，备份恢复速度达到了600M/


//待测试
ceph pg dump_stuck stale

ceph API使用博客
http://irq0.org/articles/ceph/objectstore
http://volplugin-docs.s3-website-us-west-1.amazonaws.com/

 req_info info;

 906 struct req_info {
 907   RGWEnv *env;
 908   RGWHTTPArgs args;                
 909   map<string, string> x_meta_map;  
 910   
 911   const char *host;                
 912   const char *method;              
 913   string script_uri;               
 914   string request_uri;              
 915   string effective_uri;            
 916   string request_params;           
 917   string domain;                   
 918 
 919   req_info(CephContext *cct, RGWEnv *_env);
 920   void rebuild_from(req_info& src);
 921   void init_meta_info(bool *found_bad_meta);
 922 };


待整理
create table APIaccout (     
	     `id`                int(11) NOT NULL AUTO_INCREMENT, 
         `userid`            varchar(50) NOT NULL,
         `host`              varchar(50),
         `request_method`    varchar(20),
         `script_url`        varchar(1024),
         `request_url`       varchar(1024),
         `domain`            varchar(50) ,
         PRIMARY KEY (`id`, `userid`)
        )ENGINE=InnoDB DEFAULT CHARSET=utf8;

 create table if not exists APIaccout (         `id`                int(11) NOT NULL AUTO_INCREMENT, 
         `userid`            varchar(50) NOT NULL,
         `host`              varchar(50),
         `request_method`    varchar(20),
         `script_url`        varchar(1024),
         `request_url`       varchar(1024),
         `domain`            varchar(50) ,
         PRIMARY KEY (`id`, `userid`)
 )ENGINE=InnoDB DEFAULT CHARSET=utf8;



map在进行插入的时候是不允许有重复的键值的，如果新插入的键值与原有的键值重复则插入无效，
可以通过insert的返回值来判断是否成功插入。下面是insert的函数原型：
      pair<iterator, bool> insert(const value_type& x);


rgw_cache_enabled 参数：rgw/rgw_rados.cc
new RGWCache<RGWRados>  or RGWRados

common/config.h: 
class md_config_t, ceph 相关配置类

RGWFCGXFrontend(直接继承了RGWProcessFrontend的run函数，没有做隐藏即没有做run函数的实现)->RGWProcessFrontend（1、实现了run函数，）->RGWFrontend


RGWProcessFrontend：：实现了run函数
	new了RGWProcessControlThread（pprocess）；
	并执行回调pprocess->run()；
L442 void RGWFCGXProcess::run()

common/WorkQueue.h -----工作队列的实现，线程池


http://www.cnblogs.com/yjf512/archive/2012/06/07/2539755.html
http://blog.csdn.net/lingfengtengfei/article/details/12392449
select同步io模型编程

epoll
http://blog.csdn.net/lingfengtengfei/article/details/12398299

socketpair创建了一对无名的套接字描述符（只能在AF_UNIX域中使用），描述符存储于一个二元数组,eg. s[2] .
这对套接字可以进行双工通信，每一个描述符既可以读也可以写。这个在同一个进程中也可以进行通信，
向s[0]中写入，就可以从s[1]中读取（只能从s[1]中读取），也可以在s[1]中写入，然后从s[0]
中读取；
但是，若没有在0端写入，而从1端读取，则1端的读取操作会阻塞，即使在1端写入，也不能从1读取，仍然阻塞；反之亦然...


  93 //add by 1863
  94 #define vAPICountOnce 1000
  95 static vector<struct req_state*> vAPI;
  96 pthread_rwlock_t  vAPI_lock;

 780   dout(1) << "\n====req info=" << "\n==1863==user_id: "         << dec << s->user.user_id
 781                                << "\n==1863==req_id: "          << dec << s->req_id
 782                                << "\n==1863==host: "            << dec << s->info.host
 783                                << "\n==1863==method: "          << dec << s->info.method
 784                                << "\n==1863==script_uri: "      << dec << s->info.script_uri
 785                                << "\n==1863==request_uri: "     << dec << s->info.request_uri
 786                                << "\n==1863==effective_uri: "   << dec << s->info.effective_uri
 787                                << "\n==1863==request_params: "  << dec << s->info.request_params
 788                                << "\n==1863==domain: "          << dec << s->info.domain

struct APIInfo{
	string user_id;
	string req_id;
	string host;
	string method;
	string script_uri;
	string request_uri;
	string effective_uri;
	string request_parames;
	string domain;
	time_t req_time;
}

    $sql = "create table if not exists APIaccout (
         `id`                int(11) NOT NULL AUTO_INCREMENT, 
         `userid`            varchar(50) NOT NULL,
         `req_id`            varchar(50),
         `host`              varchar(50),
         `request_method`    varchar(20),
         `script_url`        varchar(1024),
         `request_url`       varchar(1024),
         `effective_uri`     varchar(50),
         `request_parames`   varchar(50),
         `domain`            varchar(50),
         `req_time`			 varchar(50),
         `req_operate`       varchar(50)
         PRIMARY KEY (`id`, `userid`)
        )ENGINE=InnoDB DEFAULT CHARSET=utf8";

 238 struct APIInfo{
 239     string user_id;
 240     string req_id;
 241     string host;
 242     string method;
 243     string script_uri;
 244     string request_uri;
 245     string effective_uri;
 246     string request_params;
 247     string domain;
 248     time_t req_time;
 249     string name;
 250 };
rgw/rgw_op.cc  --------rgw的具体请求操作处理； eg：L1295




"\n==1863==req_timep: "       << dec << ctime(&(s->req_timep));


grep  -v跳过某条件

c++反射机制：（基于自身来实现）
一个是通过类名的字符串创建相应的类的实例化。还有一个是通过属性的名字字符串来操作相应的类的属性。

Reflection 是 Java 程序开发语言的特征之一，它允许运行中的 Java 程序对自身进行检查，或者说“自审”，并能直接操作程序的内部属性。
例如，使用它能获得 Java 类中各成员的名称并显示出来。 
Java 的这一能力在实际应用中也许用得不是很多，但是在其它的程序设计语言中根本就不存在这一特性。
例如，Pascal、C 或者 C++ 中就没有办法在程序中获得函数定义相关的信息。 

 998    http_op op;

 valgrind --tool=memcheck --leak-check=yes --show-reachable=yes  command

我们的内部游戏云，采用1块ssd bcache ceph，能提供接
近于本地ssd方案的性能，这个解决方案应该是首创，还没查到案例。


echo deb http://gitbuilder.ceph.com/libapache-mod-fastcgi-deb-$(lsb_release -sc)-x86_64-basic/ref/master $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph-fastcgi.list

ceph-authtool -n client.radosgw.$HOSTNAME --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.$HOSTNAME.keyring


ceph源码解读：
分析了一下commond的处理过程，确实是在OsdMonitor经过Paxos处理之后更新了OSDMap操作，不过从代码逻辑看，任何一个映射的改变都会导致所有的xxxMonitor执行更新操作?
杜衡-武汉 2015/10/30 9:56:46
不会
杜衡-武汉 2015/10/30 9:57:01
osdmonitor只处理osdmap的更新
杜衡-武汉 2015/10/30 9:57:18
mdsmonitor就是处理mdsmap的更新，，，
浮世逍遥-南京 2015/10/30 9:57:39
PaxosService::refresh()是因为这个函数会触发所有的映射更新
浮世逍遥-南京 2015/10/30 9:58:04
Paxos::do_refresh()-->PaxosService::refresh()
9:59:39
浮世逍遥-南京 2015/10/30 9:59:39
在执行完成对应的更新之后，都会调用do_refresh()，这个函数就会进行所有监控器的刷新

浮世逍遥-南京 2015/10/30 10:00:21
不过有可能是其他的监控器不会执行真正意义上的操作。比如osdmap的更新会导致pgmap的改变，而不会导致其他的改变


[root@stcell01 ~]# ceph osd pool create col-pool 32 32
pool 'col-pool' created
[root@stcell01 ~]# ceph osd pool create cache-pool 8 8
pool 'cache-pool' created
[root@stcell01 ~]#  ceph osd pool create l2-pool 4 4
pool 'l2-pool' created
[root@stcell01 ~]# ceph osd tier add-cache col-pool cache-pool 400000000
pool 'cache-pool' is now (or already was) a cache tier of 'col-pool'
[root@stcell01 ~]# 
[root@stcell01 ~]# 
[root@stcell01 ~]# ceph osd tier add-cache col-pool l2-pool 100000000
pool 'l2-pool' is now (or already was) a cache tier of 'col-pool'
[root@stcell01 ~]# 



我想做的是类似:
         xxx tier
           |
         ssd tier
           |
         base-tier
Cache tier虽然可以设置多个缓冲池但是实际上只有一个缓冲池是有效的。目前的读缓存池和写缓存池就是一个池。
在代码逻辑中这部分也是分为读缓存池和写缓存池，难道未来有可能支持读写缓存池分离的情况?

from boto.pyami.config import Config, BotoConfigLocations

server、manager、driver关系：
server是一个服务进程，类似于Linux下的守护进程，一个server对于一个RpcAPI，接收
特定topic的消息。server接收到请求之后，会转发给manager去处理。一个server具体的功能；
由manager来决定；




安装Flask需要python2.5或者更高版本，目前不支持python3，其wsgi标准还未最终确定；

一、安装virtualenv，为每一个pyhton应用都“安装”一个独立的python环境；
使用命令：sudo easy_install virtualenv
或更高级的：sudo pip install virtualenv
在Ubuntu下：sudo apt-get install python-virtualenvsome code

二、virtualenv的使用：
$ mkdir myproject
$ cd myproject
$ virtualenv env //创建一个名为env的目录，这个目录就是一个独立的python环境
$ . env/bin/activate //激活名为env的环境，注意"."点后面有关空格；
//每次要先激活对应的环境，在进行相关开发；
some code

三、Flask安装，使用如下命令：
$ easy_install Flask
some code

另外，用git检索方式，安装最新的Flask：
$ git clone http://github.com/mitsuhiko/flask.git //下载安装文件
$ cd flask
$ virtualenv xxx
$ . xxx/bin/activate
$ python setup.py develop

apt-get install python-virtualenv
virtualenv ceph 	//创建一个名为ceph的目录，该目录为独立的python环境；
.  ceph/bin/activate  

python ceph-dash.py > ./cephdash.log 2>&1   &
访问：192.168.2.18：8080

lambda函数：又称匿名函数；
lambda [arg1[, arg2, arg3...argN]]:expression
lambda语句中，冒号前是参数，可以有多个，用逗号隔开，冒号右边是返回值；
lambda语句构建的其实是一个函数对象；



2 from nova.openstack.common.rpc import dispatcher as rpc_dispatcher

七牛key：
	xDM_sQV1Gx4PGrhL_NeBlUDGQSByTenMKSIBJZJG
	Y1O2JA1MIhkY6vXGFRJXlkdYT9Erfl9FPUnHCdtA

openstack nova分析：
http://www.choudan.net/2013/07/30/OpenStack-API%E5%88%86%E6%9E%90(%E4%B8%80).html

Nova api service 接收到http请求之后：
	1、通过wsgi server将http request封装成wsgi request；
	2、使用api-paste.ini文件中定义的Filter对wsgi request进行处理
	3、处理完毕后，根据mapper中的记录，讲不同的请求路由到不同的WSGI APP
	4、WSGI APP接收到请求之后，并将请求disptach 到controller中的方法上；

如果AMQP是服务之间唯一的交互方式，那么用户如何执行指令？答案是API服务，它是一个http服务
（一个python中的wsgi应用），API服务监听HTTP上的rest命令并且将他们转化成相应的AMQP消息；
同样的，来自服务的响应也通过AMQP和API服务转化成HTTP相应返回值给请求者。


什么是wsgi？
	wsgi是一个web组件的接口防范，wsgi将web组件分为三类：web服务器，web中间件，web应用程序 
	wsgi基本处理模式为：wsgi Server -> wsgi middleware -> wsgi application
wsgi server：
	理解为一个符合wsgi规范的web server，接收request请求，封装一系列环境变量，
	按照wsgi规范调用注册的wsgi app，最后将response返回给客户端。
	工作流程：
		1、服务器创建socket，监听port，等待client 连接
		2、当请求过来时，server解析client msg放到环境变量environ中，并调用绑定的handler来
			处理
		3、handler解析这个http请求，将请求消息例如method、path等放到environ中
		4、wsgi handler再将一些server端消息也放到environ中，最后server msg，client msg，
			以及本次请求msg 全部都保存到了环境变量envrion中；
		5、wsgi handler调用注册的wsgi app，并将envrion和回调函数传给wsgi app
		6、wsgi app将reponse header/status/body回传给wsgi handler
		7、handler 通过socket将response msg返回到client 

 WSGI Application
         wsgi application就是一个普通的callable对象，当有请求到来时，wsgi server会调用这个wsgi app。
         这个对象接收两个参数，通常为environ,start_response。environ就像前面介绍的，可以理解为环境变量，
         跟一次请求相关的所有信息都保存在了这个环境变量中，包括服务器信息，客户端信息，请求信息。
         start_response是一个callback函数，wsgi application通过调用start_response，
         将response headers/status 返回给wsgi server。
         此外这个wsgi app会return 一个iterator对象 ，这个iterator就是response body。	
openstack rpc模块提供了rpc.call, rpc.cast, rpc.fanout_cast  三种rpc调用方法，发送和接收rpc请求；
每个组件的api是请求发起者即client，请求通过使用rpc发送给scheduler，scheduler负责处理请求则为server端

server 负责处理请求，首先需要创建consumer， scheduler server创建了scheduler和scheduler:host的topic
consumer 和scheduler fanout consumer，分别用于接收不同的类型的msg，consumer创建时， 就会在Qpid server
上创建对应的Msg Queue，并声明Routing Key绑定到Exchange上，scheduler服务启动时，将sheduler manager注册
为了rpc_dispatcher即callback对象即client发送的请求最终会有scheduler manager 对象调用执行；
启动consumer线程，接受Queue上的消息，当线程接收到Queue上消息后，将消息传递给rpc_dispathcher即callback
由callback 对象根据消息内容，调用用对象的处理函数，并处理请求





nginx处理图片等
https://github.com/3078825/ngx_image_thumb

内置方法 	 说明
 __init__(self,...) 	 初始化对象，在创建新对象时调用
 __del__(self) 	 释放对象，在对象被删除之前调用
 __new__(cls,*args,**kwd) 	 实例的生成操作
 __str__(self) 	 在使用print语句时被调用
 __getitem__(self,key) 	 获取序列的索引key对应的值，等价于seq[key]
 __len__(self) 	 在调用内联函数len()时被调用
 __cmp__(stc,dst) 	 比较两个对象src和dst
 __getattr__(s,name) 	 获取属性的值
 __setattr__(s,name,value) 	 设置属性的值
 __delattr__(s,name) 	 删除name属性
 __getattribute__() 	 __getattribute__()功能与__getattr__()类似
 __gt__(self,other) 	 判断self对象是否大于other对象
 __lt__(slef,other) 	 判断self对象是否小于other对象
 __ge__(slef,other) 	 判断self对象是否大于或者等于other对象
 __le__(slef,other) 	 判断self对象是否小于或者等于other对象
 __eq__(slef,other) 	 判断self对象是否等于other对象
 __call__(self,*args) 	 把实例对象作为函数调用					#重点




 22 class Prefix(object):
 23     def __init__(self, bucket=None, name=None):
 24         self.bucket = bucket
 25         self.name = name
 26 
 27     def startElement(self, name, attrs, connection):
 28         return None
 29 
 30     def endElement(self, name, value, connection):
 31         if name == 'Prefix':
 32             self.name = value
 33         else:
 34             setattr(self, name, value)
 35 
 36     @property
 37     def provider(self):
 38         provider = None
 39         if self.bucket and self.bucket.connection:
 40             provider = self.bucket.connection.provider
 41         return provider


 创建bucket ssd  ,rulename ssd  

ceph osd crush add-bucket ssd  root
(move host to buckert ssd)
ceph  osd crush rule create-simple   ssd   ssd   host  firstn
ceph osd pool set <poolname> crush_ruleset $ssdruleid

示例用的crushmap 命令实现方式

(move host to buckert ssd)
ceph osd crush add-bucket ceph-osd-ssd-server-1
ceph osd  crush create-or-move  osd.1  1.00  host=ceph-osd-ssd-server-1
ceph osd crush move  ceph-osd-ssd-server-1 root=ssd
示例上面部分osd host 部分的操作




https://mystorage.yunvm.com/
s-api.yunvm.com


local-test01-163
local-test02-03

http://docs.ceph.com/docs/master/radosgw/federated-config/


python日志系统，
import logging
LOG=logging.getLogger("应用程序名字")
console = logging.StreamHandler()
console = setLevel(logging.INFO)
LOG.addHander(console)
LOG.debug()
LOG.info()
LOG.warning()
LOG.error()
LOG.critical()



rgw/rgw_op.cc






设置osd亲和性：
osd primary-affinity <osdname (id|osd.id)> <float[0.0-1.0]>
设置pool副本数目：
osd pool set <poolname> size

rados pool的export和import
pool的export和import

Error ENOENT: osd.0 does not exist. create it before updating the crush map

执行如下命令后，再重复执行上面那条启动服务的命令，就可以解决：

# ceph osd create

ceph osd set noout

rpc.create_connection

root@u204:~# cat /etc/apt/apt.conf
Acquire::http::Proxy "http://118.26.201.224:5522";



[client.radosgw.gateway]
host = u253
keyring = /etc/ceph/keyring.radosgw.gateway
rgw socket path = /tmp/radosgw.sock
log file = /var/log/ceph/radosgw.log

#Add DNS hostname to enable S3 subdomain calls
rgw dns name =  s-api.yunvm.com

rgw ops log rados = false
rgw enable ops log = false
debug rgw = 0

rgw admin entry = admin

rgw region = default
rgw region root pool = .rgw.root
rgw default region info oid = default.region



文件夹中包含一个__init__.py, python就会把文件夹当作一个package，
里面的py文件就能够被外面import了；
__init__.py文件是用来做一些初始化的工作的
sound/
	__init__.py
	formats/
		__init__.py
		aa.py
		bb.py

ceph_url
http://docs.ceph.com/docs/master/install/install-ceph-gateway/

            "access_key": "L5M4Y14GT4BENE2ZN0AD",
            "secret_key": "8JvqqDt1vkOjBmLSgStgPVqezuGnkiuW7nX9zkWs"

root@u210:~/operater# md5sum  /usr/lib/apache2/modules/mod_fastcgi.so
349725b5670acaf59f1a0058691485b1  /usr/lib/apache2/modules/mod_fastcgi.so

root@u210:~/operater# md5sum  /usr/lib/apache2/modules/mod_fastcgi.so
d8e4bfa7e2626114e1ce0ab71f01c816  /usr/lib/apache2/modules/mod_fastcgi.so



sgdisk --zap-all -- $var && sgdisk --clear --mbrtogpt -- $var &&  partprobe $var 

安装ceph-deploy之前执行
apt-get install  python-setuptools

class MyClass(object):
	pass 

MyClass = type('MyClass', (), {})
返回一个class
这是因为函数type实际上是一个元类；（元类就是用来创建这些类（对象）的），
type就是python在背后用来创建所有类的元类；
str是用来创建字符串对象的类，int是用来创建整数对象的类；
type就是创建类对象的类；可以通过检查__class__属性来看
python中所有的东西，注意，我是指所有的东西----都是对象；
这包括整数，字符串，函数以及类；它们全部都是对象，而且他们都是从一个类创建而来；

>>> age = 35
>>> age.__class__
<type 'int'>
class Foo(Bar):
    pass
Python做了如下的操作：
Foo中有__metaclass__这个属性吗？如果是，Python会在内存中通过__metaclass__创建一个名字为Foo的类对象（我说的是类对象，请紧跟我的思路）。
如果Python没有找到__metaclass__，它会继续在Bar（父类）中寻找__metaclass__属性，并尝试做和前面同样的操作。
如果Python在任何父类中都找不到__metaclass__，它就会在模块层次中去寻找__metaclass__，并尝试做同样的操作。
如果还是找不到__metaclass__,Python就会用内置的type来创建这个类对象。

现在的问题就是，你可以在__metaclass__中放置些什么代码呢？答案就是：可以创建一个类的东西。
那么什么可以用来创建一个类呢？type，或者任何使用到type或者子类化type的东东都可以。


自定义元类：
	元类的主要目的就是为了创建类时能够自动地改变类；


ethtool -s eth1 speed 100 duplex full autoneg off


rgw:
wget -q -O- https://raw.github.com/ceph/ceph/master/keys/autobuild.asc | sudo apt-key add - && echo deb http://gitbuilder.ceph.com/apache2-deb-$(lsb_release -sc)-x86_64-basic/ref/master $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph-apache.list && echo deb http://gitbuilder.ceph.com/libapache-mod-fastcgi-deb-$(lsb_release -sc)-x86_64-basic/ref/master $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph-fastcgi.list


ceph-deploy --overwrite-conf config push mon0 mon1 mon2

radosgw-admin user create --uid="cp" --display-name="keke" && radosgw-admin subuser create --uid=cp --subuser=cp:swift --access=full && radosgw-admin key create --subuser=cp:swift --key-type=swift --gen-secret  

radosgw-admin user info --uid=cp


http://djangobook.py3k.cn/2.0/chapter03/

ceph-deploy --overwrite-conf mon create mon1
ceph-deploy --overwrite-conf config push mon2

ceph-deploy gatherkeys mon1
 

创建xfs：
	apt-cache search xfs |grep xfs 
	apt-get install xfsprogs

加载xfs模块：
	modprobe xfs 
	lsmod |grep xfs 
挂载：
	mkfs.xfs -f  /dev/sdb
	mount -t xfs   /dev/sdb  /mnt

ceph osd pool create libvirt-pool 128 128
ceph osd lspools

#Check the location of an object in pool data
ceph osd map data <object name>
例如：
root@node2:~# ceph osd map data  default.4439.1_operate.py
osdmap e96 pool 'data' (4) object 'default.4439.1_operate.py' -> pg 4.309c3ed6 (4.16) -> up [2,0] acting [2,0]


//ceph支持s3操作
Feature	Status	Remarks
List Buckets	Supported	 
Delete Bucket	Supported	 
Create Bucket	Supported	Different set of canned ACLs
Bucket Lifecycle	Not Supported	 
Policy (Buckets, Objects)	Not Supported	ACLs are supported
Bucket Website	Not Supported	 
Bucket ACLs (Get, Put)	Supported	Different set of canned ACLs
Bucket Location	Supported	 
Bucket Notification	Not Supported	 
Bucket Object Versions	Supported	 
Get Bucket Info (HEAD)	Supported	 
Bucket Request Payment	Not Supported	 
Put Object	Supported	 
Delete Object	Supported	 
Get Object	Supported	 
Object ACLs (Get, Put)	Supported	 
Get Object Info (HEAD)	Supported	 
POST Object	Supported	 
Copy Object	Supported	 
Multipart Uploads	Supported	(missing Copy Part)


/usr/bin/radosgw -n client.radosgw.node2



"access_key": "68HOYJY0M7DRMJ5EA8BE",
"secret_key": "w20kxMvQfV0wE5AtPETQxgdjo5u509mAkr1ErbZV"



s3 api demo
http://docs.dreamobjects.net/s3-examples/cpp.html




61.164.252.83 
61.164.252.84
61.164.252.85

61.164.252.89
61.164.252.90
61.164.252.91

203.156.197.253
203.156.196.254


 944   op->pre_exec();
 945   op->execute();
 946   op->complete();

time_t timep;
struct tm *p;
time(&timep);
printf("time() : %d \n",timep);
p=localtime(&timep);
p->tm_hour = 0;
p->tm_min = 0;
p->tm_sec  = 0;
int num = 1000* mktime(p);
printf("time()->localtime()->mktime():%d\n",num);



<?php
/*
用户频道服务器监控

接口地址：
http://cs.vangen.cn/interface/refresh.php

传入参数：
urls 刷新地址数组
opt  操作类型  dir_refresh
name 用户名   
token  安全校验   md5(接口账户+接口密码+时间戳+'B5186C929D3EA9')
timestamp  时间戳  2014-05-07 09:00:00

输出内容：
json格式
*/
header('Content-type: text/html; charset=utf-8');

$in = array('urls' => array('cs.vangen.cn/common/index/img/menu-toggler.png'),
			'opt' => 'obj_refresh',	//'query_task'
			'sort_page' => '1',
			'sort_load' => '1',
			'sort_sync' => '2',
			'rt_url' => 'http://www.cdnunion.com',
			'name' => 'vangen',
			'pwd' => 'pwd',//'cjurkTkI157-49',//'KEmF6qer',
			'timestamp' => date('Y-m-d H:i:s')
			);

$out = send($in);

echo '<pre>';
print_r(json_decode($out));

////////////////////////////////////////////////////////////////////////////

function send($in)
{
	$in['token'] = md5($in['name'].$in['pwd'].$in['timestamp'].'B5186C929D3EA9');
	
	$ch = curl_init();
	curl_setopt ( $ch ,  CURLOPT_URL ,  'http://cs.vangen.cn/interface/refresh.php' );
	curl_setopt ( $ch ,  CURLOPT_POST ,  1 );
	curl_setopt ( $ch ,  CURLOPT_POSTFIELDS ,  http_build_query($in) );
	curl_setopt ( $ch ,  CURLOPT_RETURNTRANSFER ,  1 );
	
	//echo http_build_query($in) ;
	
	return curl_exec ( $ch );
}


ovs添加网桥
modprobe kvm-intel
ovs-vsctl  add-br  br0
ovs-vsctl  add-port br0 eth0
ifconfig br0 210.14.78.77 netmask 255.255.255.0
ifconfig eth0 up
ip route add default via 210.14.78.254 dev br0




ovs-vsctl set port vnet1 tag=61
ovs-ofctl add-flow br0 idle_timeout=0,in_port=LOCAL,arp,dl_dst=52:54:00:e0:b6:b8,nw_src=${BR_IP_0},priority=60001,actions=output:178
ovs-ofctl add-flow br0 idle_timeout=0,in_port=LOCAL,ip,dl_dst=52:54:00:e0:b6:b8,nw_src=${BR_IP_0},priority=60001,actions=output:178
ovs-ofctl add-flow br0 idle_timeout=0,in_port=LOCAL,udp,tp_dst=68,dl_dst=52:54:00:e0:b6:b8,priority=60002,actions=output:178
ifconfig br0:78.15 210.14.78.15 netmask ${OUT_MASK_0}
ovs-ofctl add-flow br0 idle_timeout=0,in_port=178,ip,dl_src=52:54:00:e0:b6:b8,nw_src=10.128.0.186,actions=mod_nw_src:210.14.78.15,mod_dl_src:${BR_MAC_0},mod_dl_dst:38:22:d6:14:c3:fa,output:1
ovs-ofctl add-flow br0 idle_timeout=0,in_port=1,ip,dl_dst=${BR_MAC_0},nw_dst=210.14.78.15,actions=mod_nw_dst:10.128.0.186,mod_dl_src:${BR_MAC_0},mod_dl_dst:52:54:00:e0:b6:b8,output:178
ovs-vsctl set interface vnet1 ingress_policing_rate=5000
ovs-vsctl set interface vnet1 ingress_policing_burst=500

#202259d4-b92e-7974-b538-1a14e5ebad22, VlanType:0, VhostIfName:vnet23;
ovs-vsctl set port vnet23 tag=2
ovs-ofctl add-flow br1 idle_timeout=0,in_port=LOCAL,arp,dl_dst=52:54:00:ad:68:6b,nw_src=${BR_IP_1},priority=60001,actions=output:3
ovs-ofctl add-flow br1 idle_timeout=0,in_port=LOCAL,ip,dl_dst=52:54:00:ad:68:6b,nw_src=${BR_IP_1},priority=60001,actions=output:3
ovs-ofctl add-flow br1 idle_timeout=0,in_port=LOCAL,udp,tp_dst=68,dl_dst=52:54:00:ad:68:6b,priority=60002,actions=output:3
ifconfig br1:80.3 192.168.80.3 netmask ${OUT_MASK_1}
ovs-ofctl add-flow br1 idle_timeout=0,in_port=3,ip,dl_src=52:54:00:ad:68:6b,nw_src=10.129.0.3,actions=mod_nw_src:192.168.80.3,mod_dl_src:${BR_MAC_1},mod_dl_dst:c8:1f:66:f8:23:9a,output:1
ovs-ofctl add-flow br1 idle_timeout=0,in_port=1,ip,dl_dst=${BR_MAC_1},nw_dst=192.168.80.3,actions=mod_nw_dst:10.129.0.3,mod_dl_src:${BR_MAC_1},mod_dl_dst:52:54:00:ad:68:6b,output:3
ovs-vsctl set interface vnet23 ingress_policing_rate=100000
ovs-vsctl set interface vnet23 ingress_policing_burst=10000
#br1 end





radosgw-admin user create --uid=tmp  --display_name=tmp --access_key=tmp --max_buckets=10



for  i in [None]+range(-1, -len(s), -1)
　　print i
　　print  s[:i]

序列之间的类型转换：

　　内建函数list（）， str（） 和tuple（）被用做各种序列类型之间的转换；

这里我们又一次“转换”这个词，不过，萎缩某python里面不简单地把一个对象转换成另一个对象呢？

　　因为：一旦一个python对象被建立，我们就不能更改其身份或类型了。如果你把一个列表对象传给list（）函数，便会创建这个对吸纳该的一个浅拷贝；然后将其插入新的列表中，同样地，在做连接操作和重复操作时；我们也会做这样的处理；

　　所谓的浅拷贝就是只拷贝了对对象的索引，而不是重新建立了一个对象；如果你想完全的拷贝一个对象，则需要用到深拷贝；


云存储服务器root密码：a2#D3E%9f^9@9E2&b74cd



 rados -p rbd listwatchers  test.rbd



ceph博客
http://www.quts.me/2015/07/25/calamari-deploy/


1、安装php包管理工具composer
apt-get install php5-cli -y  &&  curl -sS https://getcomposer.org/installer | php


intel  ceph
https://software.intel.com/en-us/blogs/2015/04/06/ceph-erasure-coding-introduction



GetObjectRequest->S3Request->AmazonSerializableWebServiceRequest
										|
		AmazonWebServiceRequest <-AmazonWebServiceRequest

aws-cpp-sdk-core/include/aws/core/AmazonWebServiceRequest.h:47
class AmazonWebServiceRequest 里面有函数： void SetResponseStreamFactory()

 LibRBD 模块是 Josh Durgin，RadosGateWay 是 Yehuda Sadeh

      pgmap v182546: 96 pgs, 11 pools, 764 GB data, 2170 kobjects
            2337 GB used, 8104 GB / 11000 GB avail
                  95 active+clean
                   1 active+clean+scrubbing

yanzhouyouma ---yanzhouyouma.s-api.yunvm.com
qcxswzg		 ---

ln -s /usr/lib/x86_64-linux-gnu/libboost_system.so  /usr/lib/libboost_system-mt.so
ln -s /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0  /usr/lib/libboost_thread-mt.so
ln -s /usr/lib/x86_64-linux-gnu/libboost_program_options.so.1.54.0  /usr/lib/libboost_program_options-mt.so

gdb 堆栈回溯
http://www.cppblog.com/BlueSky/archive/2007/11/20/37012.html


vim插件：tmux


PURGE:http://
s-api.yunvm.com/cdnsong/ammsteinPussy2009.FLV
cdnsong.s-api.yunvm.com/ammsteinPussy2009.FLV

cdnsong.storage.yunvm.com/ammsteinPussy2009.FLV
cdnsong.storage.yunvm.com/ammsteinPussy2009.FLV


查看pg信息：ceph pg PGID  query
ceph osd map POOLNAME objectname


#include <map>
#include <string>
using namespace std;
typedef struct tagStudentInfo
{
       int      nID;

       string   strName;
}StudentInfo, *PStudentInfo;  //学生信息

int main()
{
    int nSize;          //用学生信息映射分数
    map<StudentInfo, int>mapStudent;
    map<StudentInfo, int>::iterator iter;
    StudentInfo studentInfo;
    studentInfo.nID = 1;
    studentInfo.strName = “student_one”;
    mapStudent.insert(pair<StudentInfo, int>(studentInfo, 90));
    studentInfo.nID = 2;
    studentInfo.strName = “student_two”;

	mapStudent.insert(pair<StudentInfo, int>(studentInfo, 80));
	for (iter=mapStudent.begin(); iter!=mapStudent.end(); iter++)
    	cout<<iter->first.nID<<endl<<iter->first.strName<<endl<<iter->second<<endl;
}

//php  composer.phar install
// Require the Composer autoloader.
require 'vendor/autoload.php';



1<<order , 默认order 4M；
1<<22,即2^22 = 2^2×2^20 = 4 * 2^20 = 4M;



云存储统计用户调用API次数功能
	获取底层用户接口来获取用户调用API的次数，以及用户通过云存储下载的字节数，
	区分post和get中方式用于后期计费开发，对用户的数据按天进行统计


可以通过如下命令进行代码合并【注：pull=fetch+merge]
git pull --rebase origin master



root@u162:~/mygit/ceph/code# date
Mon Dec 21 14:36:01 CST 2015

 update APIaccout_2015_December set used_storage=100 where date="2015-12-22" and id=9;

insert into Table_A(userid, date, data) values('XXX', CURDATE(), 100) on duplicate key update  `data`=`data`+100; 


 INSERT INTO clients
(client_id, client_name, client_type)
SELECT 10345, 'IBM', 'advertising'
FROM dual
WHERE not exists (select * from clients
where clients.client_id = 10345);



http://61.164.252.89/p.tar.gz
12306抢票：
http://www.fishlee.net/soft/12306/


radosgw-admin metadata list
[
    "bucket",
    "bucket.instance",
    "user"

]


yunvm云存储为企业、开发者提供无限制、多备份、分布式的低成本存储解决方案。并提供兼容Amazons3的各
种开发语言SDK，轻松为企业、开发者解决存储扩容、数据可靠安全以及分布式访问等相关问题，企业、开发者可
以快速的开发出涉及存储业务的应用。
yunvm Simple Storage Service (yunvm S3) 是一种面向 Internet的存储服务。您可以
可以通 过 Amazon S3 随时 在 Web 上的任何位置存储和检索的任意大小的数据。您可以使用 AWS 管理控制台简单而直观的 web 界面来实现这些任务

Amazon Simple Storage Service (Amazon S3) 是一种面向 Internet 的存储服务。您可以通过 Amazon S3 随时在 Web 上的任何位置存储和检索的任意大小的数据。您可以使用 AWS 管理控制台简单而直观的 web 界面来实现这些任务。'

gitbook安装配置：
http://blog.csdn.net/ys743276112/article/details/45130831



ceph打印ldout输出的log到文件：
修改rados_create_common函数中的CODE_ENVIRONMENT_LIBRARY宏，修改为别的宏；




1725 void RGWPutObj::execute()


2006 void RGWPostObj::execute()



1848 int RGWRados::open_bucket_data_ctx(rgw_bucket& bucket, librados::IoCtx& data_ctx)


class RGWClientIO



2016-01-04 11:53:06.965344 7f58f77e6700  1 starting new request req=0x7f5908016ae0
2016-01-04 11:53:06.967121 7f58f77e6700  1 
====req info=
==1863=user_id: cp
2016-01-04 11:53:06.967129 7f58f77e6700  1 
====req info=
==1863=second.apicallsize: 21594271
2016-01-04 11:53:06.967132 7f58f77e6700  1 
====req info=
==1863=second.apicallcount: 11
2016-01-04 11:53:06.967133 7f58f77e6700  1 
====req info=
==1863==user_id: cp
==1863==req_id: default.474128.28
==1863==host: u163
==1863==method: HEAD
==1863==script_uri: http://u163/mybucket/
==1863==request_uri: /mybucket/
==1863==effective_uri: 
==1863==request_params: 
==1863==domain: u163
==1863==operate name: stat_bucket
==1863==obj_size: 0
2016-01-04 11:53:06.967146 7f58f77e6700  1 req done req=0x7f5908016ae0 http_status=200
2016-01-04 11:53:06.967184 7f58defb5700  1 start PostData:
2016-01-04 11:53:06.971999 7f58fafed700  1 starting new request req=0x7f59080169c0
2016-01-04 11:53:06.973465 7f58fafed700  1 
====req info=
==1863=user_id: cp
2016-01-04 11:53:06.973467 7f58fafed700  1 
====req info=
==1863=second.apicallsize: 9043
2016-01-04 11:53:06.973469 7f58fafed700  1 
====req info=
==1863=second.apicallcount: 1
2016-01-04 11:53:06.973470 7f58fafed700  1 
====req info=
==1863==user_id: cp
==1863==req_id: default.474128.29
==1863==host: u163
==1863==method: HEAD
==1863==script_uri: http://u163/mybucket/operate.py
==1863==request_uri: /mybucket/operate.py
==1863==effective_uri: 
==1863==request_params: 
==1863==domain: u163
==1863==operate name: get_obj
==1863==obj_size: 9043
2016-01-04 11:53:06.973481 7f58fafed700  1 req done req=0x7f59080169c0 http_status=200
2016-01-04 11:53:06.978708 7f59217fa700  1 starting new request req=0x7f5908016810
2016-01-04 11:53:06.980090 7f59217fa700  1 
====req info=
==1863=user_id: cp
2016-01-04 11:53:06.980096 7f59217fa700  1 
====req info=
==1863=second.apicallsize: 18086
2016-01-04 11:53:06.980098 7f59217fa700  1 
====req info=
==1863=second.apicallcount: 2
2016-01-04 11:53:06.980099 7f59217fa700  1 
====req info=
==1863==user_id: cp
==1863==req_id: default.474128.30
==1863==host: u163
==1863==method: GET
==1863==script_uri: http://u163/mybucket/operate.py
==1863==request_uri: /mybucket/operate.py
==1863==effective_uri: 
==1863==request_params: 
==1863==domain: u163
==1863==operate name: get_obj
==1863==obj_size: 9043
2016-01-04 11:53:06.980114 7f59217fa700  1 req done req=0x7f5908016810 http_status=200
2016-01-04 11:53:06.985580 7f58f6fe5700  1 starting new request req=0x7f5908016710
2016-01-04 11:53:06.986889 7f58f6fe5700  1 
====req info=
==1863=user_id: cp
2016-01-04 11:53:06.986895 7f58f6fe5700  1 
====req info=
==1863=second.apicallsize: 27129
2016-01-04 11:53:06.986897 7f58f6fe5700  1 
====req info=
==1863=second.apicallcount: 3
2016-01-04 11:53:06.986898 7f58f6fe5700  1 
====req info=
==1863==user_id: cp
==1863==req_id: default.474128.31
==1863==host: u163
==1863==method: GET
==1863==script_uri: http://u163/mybucket/operate.py
==1863==request_uri: /mybucket/operate.py
==1863==effective_uri: 
==1863==request_params: 
==1863==domain: u163
==1863==operate name: get_obj
==1863==obj_size: 9043
2016-01-04 11:53:06.986910 7f58f6fe5700  1 req done req=0x7f5908016710 http_status=200




1145   op->pre_exec();
1146   op->execute();
1147   op->complete();


ceph log分析：
http://li386-187.members.linode.com/bbs/home.php?mod=space&uid=129955&do=blog&id=14199
crush分析：
http://www.tamabc.com/article/127252.html


75 int RGWGetObj_ObjStore_S3::send_response_data


$_config['db']['main']['host'] = 'admin.cdnunion.com';
//$_config['db']['main']['host'] = '203.156.196.49';
$_config['db']['main']['user'] = 'root';
$_config['db']['main']['passwd'] = 'hMrnKeDjCW29Kudv';
$_config['db']['main']['db_name'] = 'cdnunion_v4';


static int rgw_build_policies  ---L op.cc



CGI脚本是任何运行在web服务器上的程序；
CGI脚本是用下列两种方法使用的: 作为一个表单的ACTION 或 作为一个页中的直接link。


CGI脚本是怎样工作的?

CGI脚本由服务器调用, 基于浏览器的数据输入. 其工作原理如下：
1、一个URL指向一个CGI脚本. 一个CGI脚本的URL能如普通的URL一样出现，区别于.htm/.html静态URL,CGI的URL是动态URL。如http://xxxx.com/cgiurl
2、服务器CGI接收浏览器的请求, 按照那个URL指向对应的脚本文件(注意文件的位置和扩展名),执行CGI脚本.
3、CGI脚本执行基于输入数据的操作，包括查询数据库、计算数值或调用系统中其他程序.
4、CGI脚本产生某种Web服务器能理解的输出结果.
5、服务器接收来自脚本的输出并且把它传回浏览器，让用户了解处理结果。


解剖一个CGI脚本　

输出头部

　　虽然你的CGI脚本可以让你做任何事情，但是脚本的输出还是必须有一个规定形式. 
　　这个 "脚本输出" 意思是指你的脚本发回服务器的数据. 在UNIX系统中, 输出是发向标准输出, 服务器从那儿检测它. 在其他系统和服务器, 你的脚本输出也许不一样了. 
　　首先，输出头部信息，头部是实际不是文本的一部分，是服务器与浏览器之间的信息协议，你实际看不到。 有三个类型的头部: Content-type, Location, 和Status. Content-type 是最普遍的。

　　有关content-type解释可以见有关HTML的说明, 发出text/html特定编码如下：

Content-type: text/html
　　在这个例子中，输出数据的类型是text/html; 换句话说, 输出的是个HTML文件

表1. 通用格式和content-types
Format 		Content-Type
HTML		text/html
Text		text/plain
GIF			image/gif
JPEG		image/jpeg
PostScript	application/postscript
MPEG		video/mpeg
　　注意content-type 后面必须跟一个空行. 如果你没有空行，服务器将无法搞清这个头部在哪里结束。

输出数据　　

　　你输出的数据应该符合你所规定的content-type; 如果content-type是text/html, 输出安置应该是在HTML. 如果content-type是image/gif, 输出应该是在一个二进制的GIF文件.

cgi环境变量：
SERVER_SOFTWARE: Apache/2.2.22 (Ubuntu) 
SCRIPT_NAME: /cgi-bin/env.py 
SERVER_SIGNATURE:Apache/2.2.22 (Ubuntu) Server at 192.168.1.163 Port 8888
REQUEST_METHOD: GET 
SERVER_PROTOCOL: HTTP/1.1 
QUERY_STRING: PATH: /usr/local/bin:/usr/bin:/bin 
HTTP_USER_AGENT: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0 
HTTP_CONNECTION: keep-alive 
SERVER_NAME: 192.168.1.163 
REMOTE_ADDR: 192.168.1.190 
SERVER_PORT: 8888 
SERVER_ADDR: 192.168.1.163 
DOCUMENT_ROOT: /var/www 
SCRIPT_FILENAME: /var/www/cgi-bin/env.py 
SERVER_ADMIN: 1034312575@qq.com 
HTTP_HOST: 192.168.1.163:8888 
HTTP_CACHE_CONTROL: max-age=0 
REQUEST_URI: /cgi-bin/env.py 
HTTP_ACCEPT: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 
GATEWAY_INTERFACE: CGI/1.1 
REMOTE_PORT: 58953 
HTTP_ACCEPT_LANGUAGE: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3 
HTTP_ACCEPT_ENCODING: gzip, deflate 

curl_global_init()
curl_global_cleanup()

句柄:handle = curl_easy_init()

nginx源码：可以对应rgw开发
http://chenzhenianqing.cn/articles/tag/nginx%E6%BA%90%E7%A0%81

systemtap; 代码调试器

JDK配置，运行tomcat
http://www.cnblogs.com/Johness/archive/2012/07/20/2600937.html

ceph tell mon.ubuntu-ceph-06 heap stats

nginx开发入门：
http://tengine.taobao.org/book/

for i in $(rados -p ecpool ls | grep -i 3b0c2ae8944a); do rados -p ecpool get $i $i ; done
这步骤是提取pool的object的

ObjectStore主要接口分为三部分，
	第一部分是Object的读写操作，类似于POSIX的部分接口，
	第二部分是Object的属性(xattr)读写操作，这类操作的特征是kv对并且与某一个Object关联。
	第三部分是关联Object的kv操作(在Ceph中称为omap)，这个其实与第二部分非常类似，但是在实现上可能会有所变化。

目前ObjectStore的主要实现是FileStore，也就是利用文件系统的POSIX接口实现ObjectStore
 API。
 每个Object在FileStore层会被看成是一个文件，Object的属性(xattr)会利用文件的xattr属性存取，
 因为有些文件系统(如Ext4)对xattr的长度有限制，因此超出长度的Metadata会被存储在DBObjectMap里。
 而Object的omap则直接利用DBObjectMap实现。因此，可以看出xattr和omap操作是互通的，在用户角度来说，
 前者可以看作是受限的长度，后者更宽泛(API没有对这些做出硬性要求)。

http://noahdesu.github.io/2014/06/01/tracing-ceph-with-lttng-ust.html
http://fossies.org/dox/ceph-10.0.2/classRGWClientIO.html


$ sudo wondershaper em1 100 20 （限制em1网卡下载速度100Kb，上传速度20Kb）
$ sudo wondershaper clear em1   (清除em1网卡的网速限制）

看懂这段代码的人，就看懂了librbd
小史(790177150) 14:29:21
这段代码把块的元素据，按对象的最大大小max进行切分
小史(790177150) 14:29:37

**彼岸梨花**(641409252) 14:30:13
嗯
小史(790177150) 14:30:18
切分后装到
**彼岸梨花**(641409252) 14:30:21
我前天大致看了一下
小史(7901元数据77150) 14:30:24

小史(790177150) 14:30:26
这个里面
小史(790177150) 14:30:34
元素据装到这个里面
小史(790177150) 14:30:55
就是讲一个IO数据请求进行切分
小史(790177150) 14:30:59
按max大小
牛鹏举(938285791) 14:31:06
这个是在librbd里面进行的操作？
小史(790177150) 14:31:09
但是这个max怎么计算出来的，我不是很清楚
小史(790177150) 14:31:13
是的
小史(790177150) 14:31:28
当你发起一个I/O请求的时候
小史(790177150) 14:31:38
比如你请求向集群写入10K的数据
小史(790177150) 14:31:57
这10K数据的元数据，会按max进行切分
小史(790177150) 14:32:11
切分后把元素据装到
小史

小史 2016/2/1 14:33:20

小史 2016/2/1 14:33:35
切分元数据在
小史 2016/2/1 14:33:38
这个函数里完成的
小史 2016/2/1 14:34:18
切分后把每个对象的偏移，和长度保存在
小史 2016/2/1 14:34:32
这个buffer_extents里面
小史 2016/2/1 14:34:52
然后再根据这个元素据，把实际数据切分到bl里面
小史 2016/2/1 14:34:59

小史 2016/2/1 14:35:36
这样完成了，大数据分切成对象扩展的整个过程
浮世逍遥-南京 2016/2/1 14:35:50
基本上是这个原理
**彼岸梨花** 2016/2/1 14:35:54
嗯。小史分析的不错
浮世逍遥-南京 2016/2/1 14:36:04
切分和恢复都差不多
**彼岸梨花** 2016/2/1 14:36:06
我理解的也是
小史 2016/2/1 14:36:20
，年底了，我压力不大了。可以研究下代码了
小史 2016/2/1 14:36:54
一个块数据，就是这么切分为一个个小对象
浮世逍遥-南京 2016/2/1 14:37:06
曾近我也有大把的时间研究代码，现在哭成狗
小史 2016/2/1 14:37:07

小史 2016/2/1 14:37:12
这是一个动态对象数组
小史 2016/2/1 14:37:58
至于按max切分
小史 2016/2/1 14:38:07
这个max怎么计算来的，还有待研究
小史 2016/2/1 14:38:17
我粗略看了下代码
小史 2016/2/1 14:38:29
对象是有布局的
小史 2016/2/1 14:38:36
这个max和对象布局相关
**彼岸梨花** 2016/2/1 14:39:05
我看了那个怎么从Object到PG和PG到OSD
小史 2016/2/1 14:39:28
pg和osd在服务里面
小史 2016/2/1 14:39:34
librbd里是没有的
**彼岸梨花** 2016/2/1 14:39:51
嗯。
14:41:52
小史 2016/2/1 14:41:52
切分完所有数据后
小史 2016/2/1 14:42:09
写入 rbdcache
小史 2016/2/1 14:42:15

小史 2016/2/1 14:42:35
为什么不是切分完了，直接发送？
小史 2016/2/1 14:43:07
而是先写入rbd缓存
小史 2016/2/1 14:43:19

**彼岸梨花** 2016/2/1 14:43:44
应该有其他的考虑吧
**彼岸梨花** 2016/2/1 14:43:47
我也不清楚
小史 2016/2/1 14:43:56
答对了
小史 2016/2/1 14:44:09
因为要方便数据回写
小史 2016/2/1 14:44:19

小史 2016/2/1 14:44:32
见王豪迈的文章
**彼岸梨花** 2016/2/1 14:45:43
求文章链接
小史 2016/2/1 14:46:00
http://www.wzxue.com/rbdcache/


利用 curl 验证不同服务器上的文件
很多时候网站并不是一个 IP 地址，而是分布在多个 IP 地址上。这些 IP 地址通过智能解析，分配到离用户最快的节点上（CDN 就是干这个的 ）。如何验证这些 IP 地址上的文件或者内容是否一致呢？通过 curl 抓文件的时候，如果希望指定抓取网站某个服务器的内容，除了比较土的修改 HOSTS 文件，还可以通过 curl 来做。例如 www.sina.com.cn 至少有这两个 IP 地址：
202.108.33.84 和 60.215.128.131
20101205 发布的最新版 curl 和 libcurl（7.21.3）增加了一个 --resolve 和 CURLOPT_RESOLVE ，能够更优雅的解决这个问题。本来这两个参数是为了使用者能够更灵活的调整缓 存，也能够避免出现 HTTPS 相关的证书问题。可以参考这个使用方法：

ceph osd map 池名 对象名
rados -p rbd listomapvals  rbd_directory
id_8885f74b0dc51
value: (12 bytes) :
0000 : 08 00 00 00 6d 79 69 6d 61 67 65 32             : ....myimage2

id_8886a74b0dc51
value: (7 bytes) :
0000 : 03 00 00 00 66 6f 6f                            : ....foo

name_foo
value: (17 bytes) :
0000 : 0d 00 00 00 38 38 38 36 61 37 34 62 30 64 63 35 : ....8886a74b0dc5
0010 : 31                                              : 1

name_myimage2
value: (17 bytes) :
0000 : 0d 00 00 00 38 38 38 35 66 37 34 62 30 64 63 35 : ....8885f74b0dc5
0010 : 31                             



1、atoi (表示 alphanumeric to integer),c库中字符串转int
2、<arpa/inet.h>
	char* inet_ntoa(struct in_addr);
	返回点分十进制的字符串在静态内存中的指针。

ln -s  /usr/lib/x86_64-linux-gnu/libcurl.so.4.3.0.bak.bak  /usr/local/lib/libcurl.so.4
rm  /usr/local/lib/libcurl.so.4


string str1, str2;
str1 = str2.substr(0, 3); //从第0位开始，取之后的3个字符
str1.assign(str2, 0, 3);  

curl -v -O http://www.cdnunion.com/img/index/banner.jpg  --resolve  "encrypted.google.com:443:46.82.174.68"

rados bench -p 池名  60 write

try{
	
}catch(exception& e){
cout << e.what() << endl;
}

exception& e

str2.assign(10, 'c');

fd_set set
FD_SET
FD_CLE
FD_ISSET
FD_CLR()
FD_ZERO()

try{	

}catch(exception& e){
	
}

一般的打包是写spec那个文件  这个比较复杂一点
本身的make install 是用的另外一个方式  有的直接是源码路径  
可以用 fpm 这个软件 
你把make install的路径记住 然后找个目录放入里面的相对路径 然后打个包  rpm 安装出来就是你的相对路径了

pool的容量没有变，再次写入时pool继续增加，怎样能清除pool的空间-------fstrim  

http://wuhao.s-api.yunvm.com/a.cap?resourceurl=http://www.baidu.com/logo.icon

代码中有三种，filestore. keyvaluestore memstore,最近加入bluestore

char* test = (char*)malloc(4);


ceph性能测试：
	iops:存储每秒传输的IO数量；通常情况下，IOPS指得是服务器和存储系统处理的I/O数量，但是
		由于IO传输过程中，数据包会被分割成多块（block），交由存储阵列或者磁盘处理，对于磁
		盘来说这样每个block在存储系统内部也被视为一个I/O;
	throughput吞吐量：每秒数据的传输总量；通常情况下，throughput吞吐量只会计算IO包中的数据部分，
	至于IO包头的数据则会被忽略；
  两者在不同的情况下都能表示存储的性能状况；

ceph ssd做journal；比例：1：3，与此同时应该做好故障域划分，主要是考虑到ssd坏掉的情况；

ceph中磁盘io写到buffer cache就算结束，并没有真正的落盘。filestore_wbthrottle_*类参数，定义该过程的刷新条件。
journal是direct io模式，journal直接落盘了；使用xfs和zfs之类的文件系统时，只有日志写完成了，才会开始写io数据。
而数据io只写到了磁盘的buffer cache

filestore_journal_parallel， 这个参数在osd上的文件系统时btrfs的时候使用。产生的效果是，journal和数据io并行执行。

  
calamari  架构  
是目前很流行的一个架构 
diamond +carbon + graphite    
弄清楚这三个组件  可以用到你们自己的内部监控架构上的


-- write through
-- write back
其实就是buffered IO和Direct IO，这两者的区别是？

正常情况下磁盘上有个文件，如何操作它呢？
	-读取： 硬盘->内核缓冲区->用户缓冲区
	-写回： 用户缓冲区->内核缓冲区->硬盘
 这里的内核缓冲区指的是page cache，也就是DRAM。主要作用是它用于缓存文件内容，从而加快对磁盘
 上映像和数据的访问。

 正常的系统调用read/write的流程是怎样的呢？
 	-读取：硬盘->内核缓冲区->用户缓冲区
 	-写回：数据从用户地址空间拷贝到操作系统内核地址空间的页缓存中，这是write就会直接返回，操作系统
 	会在恰当的时机写入磁盘，这就是传说中的Buffered IO，
 然而对于[自缓存应用程序] [2]来说，缓存 I/O 明显不是一个好的选择。因此出现了DIRECT IO。然而想象一下，不经内核缓冲区，直接写磁盘，必然会引起阻塞。所以通常DIRECT　IO与AIO会一起出现。
>Linux 异步 I/O 是 Linux 2.6 中的一个标准特性，其本质思想就是进程发出数据传输请求之后，进程不会被阻塞，
也不用等待任何操作完成，进程可以在数据传输的时候继续执行其他的操作。相对于同步访问文件的方式来说，
异步访问文件的方式可以提高应用程序的效率，并且提高系统资源利用率。直接 I/O 经常会和异步访问文件的方式结合在一起使用。

对于nginx来说，是否开启AIO要看具体使用场景：
-  正常写入文件往往是写入内存就直接返回，因此目前nginx仅支持在读取文件时使用异步IO。
-  由于linux AIO仅支持DIRECT IO,AIO一定回去从磁盘上读取文件。
所以从阻塞worker进程的角度上来说有一定的好处，但是对于单个请求来说速度是降低了的。
>异步文件I/O是把“双刃剑”，关键要看使用场景，如果大部分用户的请求落到文件缓冲中，那么不要使用异步 
I/O，反之则可以试着使用异步I/O,看一下是否会为服务带来并发能力上的提升。 -- [<<深入理解NGINX>> 陶辉著] [1]
 
mmap  
mmap系统调用是将硬盘文件映射到用内存中，其实就是将page 
cache中的页直接映射到用户进程地址空间中，从而进程可以直接访问自身地址空间的虚拟地址来访问page 
cache中的页，从而省去了内核空间到用户空间的copy。



/index/index/idc

ceph rgw博客：
http://lyang.top/categories/ceph/

磨磨博客：
http://www.zphj1987.com/

bottleneck， 瓶颈


osd方式：
第一种：常规方式（磁盘+文件journal）
第二种方式：磁盘+磁盘journal
第三种就是btrfs文件系统
第四种是 磁盘加ssd journal的方式
第五种：本地目录方式
以前总结的
新增 
第六种 memstore
第七种 memjournal

问题：块数据操作，是怎么切分为iops操作的，比如用librbd向集群写一个文件，最后文件怎么划分为io操作的？
答：
	osdc/Objecter.cc ---- void Objecter::_send_op(Op *op, MOSDOp *m)
	该函数的用途是发送object operation（对象操作）
	把数据封装成operation发送给ceph集群，而非仅仅是数据；
	比如执行object的read/write等操作，op的来源是msg；
	4M是一个对象，object operate封装了对该4M操作的请求；然后再建立session
	与mon通信，获取主osd；

问题：怎么把4M对象的写操作，封装在OP(object operate)里面?
	4M对象数据存放在bufflist中，op通过偏移来操作他们；
	此时，数据并不放在op里面；操作时候，op通过偏移，依次发送该4M数据；
	（simplemsg的pipe部分）

	osdc/Objecter.cc ------ _op_submit中
	2095   bool const check_for_latest_map = _calc_target(&op->target, &op->last_force_resend) == RECALC_OP_TARGET_POOL_DNE;
	2096   
	2097   // Try to get a session, including a retry if we need to take write lock
	2098   int r = _get_session(op->target.osd, &s, lc);
	通过op->target获取目标数据，_get_session来获取session；
	
	可以随意找一个obj 处理的函数来分析，例如：
	osdc/Objecter.h -----ceph_tid_t write_trunc()
2315   ceph_tid_t write_trunc(const object_t& oid, const object_locator_t& oloc,
2316         uint64_t off, uint64_t len, const SnapContext& snapc, const bufferlist &bl,
2317         utime_t mtime, int flags,      
2318         uint64_t trunc_size, __u32 trunc_seq,
2319         Context *onack, Context *oncommit,
2320         version_t *objver = NULL, ObjectOperation *extra_ops = NULL) {
2321     vector<OSDOp> ops;   
2322     int i = init_ops(ops, 1, extra_ops);
2323     ops[i].op.op = CEPH_OSD_OP_WRITE;
2324     ops[i].op.extent.offset = off; 
2325     ops[i].op.extent.length = len; 
2326     ops[i].op.extent.truncate_size = trunc_size; 
2327     ops[i].op.extent.truncate_seq = trunc_seq; 
2328     ops[i].indata = bl;
2329     Op *o = new Op(oid, oloc, ops, flags | global_op_flags.read() | CEPH_OSD_FLAG_WRITE, onack, oncommit, objver);
2330     o->mtime = mtime;
2331     o->snapc = snapc;    
2332     return op_submit(o);
2333   }




ceph优化：https://01.org/zh/node/3586?langredirect=1
fs commit latency: 在journal writeahead模式下，commit是从filestore接收到op到写完journal的时延；
fs apply latency： 在journal writeahead模式下，接收op到写本地文件系统的时延；

ceph源码分析：http://my.oschina.net/u/2460844/blog/534390?fromerr=bzBokJ2c

编译代码结束后，进入src目录
root@cci-test3:~/blog/ceph-0.94.3/src/#：MON=1 MDS=0 ./vstart.sh -d -n -x
......
root@cci-test3:~/blog/ceph-0.94.3/src/#：./ceph -s
*** DEVELOPER MODE: setting PATH, PYTHONPATH and LD_LIBRARY_PATH ***
    cluster 05f0dc34-4a98-498a-9e89-06decaba704c
     health HEALTH_OK
     monmap e1: 1 mons at {a=192.168.138.95:6789/0}
            election epoch 2, quorum 0 a
     osdmap e9: 3 osds: 3 up, 3 in
      pgmap v2689: 8 pgs, 1 pools, 0 bytes data, 0 objects
            54178 MB used, 48713 MB / 105 GB avail
                   8 active+clean
root@cci-test3:~/blog/ceph-0.94.3/src/#
5. 调试集群已经启动成功，可以使用命令停止集群
root@cci-test3:~/blog/ceph-0.94.3/src/# ./stop.sh

问题1，什么是块设备？
块设备将信息存储在固定大小的块中，每个块都能进行编址。
块设备的基本特征是每个块都能区别于其它块而读写。块设备也是底层设备的抽象，块设备上未建立文件系统时，也称之为裸设备。


ceph相关命令的源码在哪？比如ceph osd map
一般在mon里面，osdmonitor ，pgmonitor等
mon记录了所有的map信息，查看基本上都是这边，也能比较快的了解整个交互过程

rbd命名规则
 18 /* New-style rbd image 'foo' consists of objects
 19  *   rbd_id.foo              - id of image
 20  *   rbd_header.<id>         - image metadata
 21  *   rbd_object_map.<id>     - optional image object map
 22  *   rbd_data.<id>.00000000
 23  *   rbd_data.<id>.00000001
 24  *   ...                     - data
 25  */
 26    
 27 #define RBD_HEADER_PREFIX      "rbd_header."
 28 #define RBD_OBJECT_MAP_PREFIX  "rbd_object_map."
 29 #define RBD_DATA_PREFIX        "rbd_data."
 30 #define RBD_ID_PREFIX          "rbd_id."
 
http://www.zphj1987.com 磨磨博客

NAS和SAN的区别：
SAN是一个网络上的磁盘；NAS是一个网络上的文件系统。
可以这样来比作：SAN是一个网络上的磁盘；NAS是一个网络上的文件系统。其实根据SAN的定义，可知SAN其实是
指一个网络，但是这个网络里包含着各种各样的元素，主机、适配器、网络交换机、磁盘阵列前端、盘阵后端、
磁盘等。长时间以来，人们都习惯性的用SAN来特指FC，特指远端的磁盘。那么，一旦设计出了一种基于FC网络
的NAS,而此时的SAN应该怎样称呼？所以，在说两者的区别时，用了一个比方，即把FC网络上的磁盘叫做SAN,把
以太网络上的文件系统称为NAS，我们可以这样简单来理解。



rgw有一个lru算法的缓存 测试的时候都要关掉的


英特尔intel ssd sc2c T060A3k5-CBOX 60G 
顺序读/写（最高） 500M/秒 400M/秒
随机 4kb 读/写   42000 iops  52000 iops



discard|nodiscard
              是否发出要求块设备回收无用空间的命令。默认值是"nodiscard"。
              此选项对于SSD(固态硬盘)、thin-provision设备(CONFIG_DM_THIN_PROVISIONING)、虚拟机镜像比较有意义。
              [注意]由于开启此项后对性能有明显的不利影响，所以推荐使用 Util-Linux 包中的 fstrim 程序进行回收。
http://www.sysnote.org/?author=2  ceph源码博客
https://www.hastexo.com/blogs     ceph源码博客

前端web的log 对接到ELK 做这个筛选 然后直接后台删除对应的object 

阻塞socket和非阻塞socket的区别：
　　1、读操作
　　对于阻塞的socket,当socket的接收缓冲区中没有数据时，read调用会一直阻塞住，直到有数据到来才返回。当socket缓冲区中的数据量小于期望读取的数据量时，返回实际读取的字节数。当sockt的接收缓冲区中的数据大于期望读取的字节数时，读取期望读取的字节数，返回实际读取的长度。
　　对于非阻塞socket而言，socket的接收缓冲区中有没有数据，read调用都会立刻返回。接收缓冲区中有数据时，与阻塞socket有数据的情况是一样的，如果接收缓冲区中没有数据，则返回错误号为EWOULDBLOCK,表示该操作本来应该阻塞的，但是由于本socket为非阻塞的socket，因此立刻返回，遇到这样的情况，可以在下次接着去尝试读取。如果返回值是其它负值，则表明读取错误。
　　因此，非阻塞的rea调用一般这样写:
　　if ((nread = read(sock_fd, buffer, len)) < 0)
{
if (errno == EWOULDBLOCK)
{
return 0; //表示没有读到数据
}else return -1; //表示读取失败
}else return nread;读到数据长度
　　2、写操作
　　对于写操作write,原理是类似的，非阻塞socket在发送缓冲区没有空间时会直接返回错误号EWOULDBLOCK,表示没有空间可写数据，如果错误号是别的值，则表明发送失败。如果发送缓冲区中有足够空间或者是不足以拷贝所有待发送数据的空间的话，则拷贝前面N个能够容纳的数据，返回实际拷贝的字节数。
　　而对于阻塞Socket而言，如果发送缓冲区没有空间或者空间不足的话，write操作会直接阻塞住，如果有足够空间，则拷贝所有数据到发送缓冲区，然后返回.
　　非阻塞的write操作一般写法是:
　　int write_pos = 0;
int nLeft = nLen;
　　while (nLeft > 0)
{
int nWrite = 0;
if ((nWrite = write(sock_fd, data + write_pos, nLeft)) <= 0)
{
if (errno == EWOULDBLOCK)
{
nWrite = 0;
}else return -1; //表示写失败
}
nLeft -= nWrite;
write_pos += nWrite;
}
return nLen;
　　3、建立连接
　　阻塞方式下，connect首先发送SYN请求道服务器，当客户端收到服务器返回的SYN的确认时，则connect返回.否则的话一直阻塞.
　　非阻塞方式，connect将启用TCP协议的三次握手，但是connect函数并不等待连接建立好才返回，而是立即返回。返回的错误码为EINPROGRESS,表示正在进行某种过程.
　　4、接收连接
　　对于阻塞方式的倾听socket,accept在连接队列中没有建立好的连接时将阻塞，直到有可用的连接，才返回。
　　非阻塞倾听socket,在有没有连接时都立即返回，没有连接时，返回的错误码为EWOULDBLOCK,表示本来应该阻塞。
　　无阻塞的设置方法
　　方法一:fcntl
int flag;
if (flag = fcntl(fd, F_GETFL, 0) <0) perror("get flag");
flag |= O_NONBLOCK;
if (fcntl(fd, F_SETFL, flag) < 0)
perror("set flag");
　　方法二:ioctl
　　int b_on = 1;
ioctl (fd, FIONBIO, &b_on);

http://www.sysnote.org/?p=265 ceph源码

CriticalSection临界区
Event  事件
Mutex 互斥量
Semaphore 信号量

linux虚拟文件系统四大对象：
	1、超级块（super block） ----每个超级块保存了对于文件系统（非vfs）的类型、大小、状态信息等等）
	2、索引节点（inode）----index node-----数据的元数据（文件大小、设备标识符、用户描述符、文件模式、扩展信息，
								指向存储该内容的磁盘区块的指针、文件分类等等；
	3、目录项（dentry） ---- 存在内存中的关于文件的目录树
	4、文件对象         ---- 对于正打开文件的描述的实例

[osd.2]
host = u91
public addr  = 61.164.252.91
cluster addr = 61.164.252.85

[client.radosgw.u89]
host = u89
keyring = /etc/ceph/ceph.client.radosgw.u89.keyring
rgw_socket_path = /tmp/radosgw.sock
log_file = /var/log/ceph/radosgw.u89.log
rgw_dns_name = s-api.yunvm.com

在非阻塞io的socket上调用read/write函数时，返回EAGAIN或者EWOULDBLOCK;
	字面上意思：EAGAIN：再试一次，
	EWOULDBLOCK：如果这是一个阻塞socket，操作将被block，perror输出：Resource temporarily unavailable；
总结：
	这些错误表示资源暂时不够，能read时，读缓冲区没有数据，或者write时，写缓冲区满；
	遇到这类情况时，如果是阻塞io，read/write就会阻塞掉，而非阻塞io，read/write立即
	返回-1，同时errno设置为EAGAIN.
	所以，对于阻塞io，read/write返回-1表示网络出错了；
		  对于非阻塞io，read/write返回-1不一定网络真的出错了，可能是Resource temporarily unavailable。此时应该再试，直到Resource available;
    综合上述，对于non-blocking的io，正确的读写操作为：
    	读：忽略掉errno=EAGAIN的错误，下次继续读
    	写：忽略掉errno=EAGAIN的错误，下次继续写
    epoll的level-trigger（水平触发）只要某个socket处于readable/writable状态，
    无论什么时候进行epoll_wait都会返回该socket，
    而edge-trigger（边沿触发）只有socket从unreadable变成readable或者unwriteable变成
    writeable时，epoll_wait才会返回该socket；

    所以，在epoll的ET模式下，正确的读写方式是：
    	读：只要可读，就一直读，直到返回0，或者errno=EAGAIN
    	写：只要可写，就一直写，直到数据发送完成，或者errno=EAGAIN
虚拟文件系统：
	linux成功的关键因素之一是：它具有兼容其他操作系统的能力
	


mongodb:
查找：
	1、 var cursor(光标) = db.things.find()
		while(cursor.hasNext()) printjson(cursor.next())

	2、db.things.find().forEach(printjson)

	3、var cursor = db.things.find()
		printjson(cursor[4])
	4、var arr = db.things.find().toArray();
		arr[5]

条件查询：
	1、db.things.find({name:"mongodb"}).forEach(printjson)
	(select  * from things where name="mongo")

	查询条件：{a:A, b:B, ... } 类似 "where a == A and b==B and ... "

	select j from things where x=5;
	db.things.find({x:4}, {j:true}).forEach(printjson)

	获取第一个元素：
	printjson(db.things.findOne({name:"mongo"}))

	通过limit限制结果集数量：
	db.things.find().limit(3)






{ "_id" : ObjectId("56ffb27bdde38b0c2362a40f"), "title" : "my Blog Post", "content" : "here is my blog post", "date" : ISODate("2016-04-02T11:52:04.663Z") }
{ "_id" : ObjectId("57086988cc54a5a6f525df68"), "name" : "mongo" }


{ "_id" : ObjectId("57086988cc54a5a6f525df68"), "name" : "mongo" }
{ "_id" : ObjectId("56ffb27bdde38b0c2362a40f"), "content" : "here is my blog post", "date" : ISODate("2016-04-02T11:52:04.663Z"), "name" : "my Blog", "title" : "my Blog Post" }




















